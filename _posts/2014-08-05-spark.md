---
title: Apache Spark Analysis
author: anthony
layout: tutorial
---

{% include tutorial-header.html %}

<!-- add some style to fix the xml formatting color -->
<style>
code.xml { color:#93a1a1 }
</style>

### Background

#### Apache Spark

[Apache Spark](http://spark.apache.org) is a "fast and general engine for large-scale data processing".
Spark presents an abstraction called a Resilient Distributed Dataset (RDD) that facilitates expressing
transformations, filters, and aggregations, and efficiently executes the computation across a distributed
set of resources.  Spark manages the lineage of a block of transformed data so that if a node goes down,
Spark can restart the computation for just the missing blocks.

GeoMesa has support for executing Spark jobs over data stored in GeoMesa.  You can initialize a Spark
RDD using standard CQL queries and by passing standard CQL functions to transform the data.  In the spirit
of the obligatory Word Count map-reduce example, we demonstrate two geospatial spins on word count. First,
counting features by time resolution to compute a time series of spatial data, and second, aggregating by grid 
cell to rapidly generate density plots.  Apache Spark enables us to express these transformations easily and
succinctly.
<!--more-->
## Count by Day of Year

We will use the GDELT data as described [here](http://geomesa.github.io/2014/04/17/geomesa-gdelt-analysis/).
First, we get a handle to a GeoMesa data store and construct a CQL query for our bounding box.

{% highlight scala %}
val params = Map(
  "instanceId" -> "instance",
  "zookeepers" -> "zoo1,zoo2,zoo3",
  "user"       -> "user",
  "password"   -> "*****",
  "auths"      -> "USER,ADMIN",
  "tableName"  -> "geomesa_catalog")

val ds = DataStoreFinder.getDataStore(params)

val ff = CommonFactoryFinder.getFilterFactory2
val f = ff.bbox("geom", -80, 35, -70, 40, "EPSG:4326")
val q = new Query("GDELT", f)
{% endhighlight %}

Next, initialize an ```RDD[SimpleFeature]``` using ```GeoMesaSpark```.

{% highlight scala %}
val conf = new Configuration
val sconf = init(new SparkConf(true), ds)
val sc = new SparkContext(sconf)

val queryRDD = geomesa.compute.spark.GeoMesaSpark.rdd(conf, sconf, ds, query)
{% endhighlight %}

Finally, we construct our computation which consists of extracting the ```SQLDATE```
from each ```SimpleFeature``` and truncating it to the day resolution.  

{% highlight scala %}
val dayAndFeature = queryRDD.mapPartitions { iter =>
  val df = new SimpleDateFormat("yyyyMMdd")
  val ff = CommonFactoryFinder.getFilterFactory2
  val exp = ff.property("SQLDATE")
  iter.map { f => (df.format(exp.evaluate(f).asInstanceOf[java.util.Date]), f) }
}
{% endhighlight %}

Then, we group by the day and count up the number of events in each group.

{% highlight scala %}
val groupedByDay = dayAndFeature.groupBy { case (date, _) => date }
val countByDay = groupedByDay.map { case (date, iter) => (date, iter.size) }
countByDay.collect.foreach(println)
{% endhighlight %}

## Parallel Density Computations

In the second demonstration, we compute densities of our feature by discretizing
the spatial domain and counting occurrences of the feature in each grid cell.  We use
[GeoHashes](http://geohash.org) as our discretization of the world so that we can 
configure the resolution of our density by setting the number of bits in the GeoHash.

First, start with a similar ```RDD[SimpleFeature]``` as before but expand the bounding
box.

{% highlight scala %}
val f = ff.bbox("geom", -180, -90, 180, 90, "EPSG:4326")
val q = new Query("GDELT", f)

val queryRDD = geomesa.compute.spark.GeoMesaSpark.rdd(conf, sconf, ds, query)
{% endhighlight %}

Project (in the relational sense) the SimpleFeature to a 2-tuple of (GeoHash, 1).

{% highlight scala %}
val discretized = queryRDD.map { f => 
   (geomesa.utils.geohash.GeoHash(f.getDefaultGeometry.asInstanceOf[Point], 25), 1) 
}
{% endhighlight %}

Then, group by grid cell and count the number of features per cell.

{% highlight scala %}
val density = discretized
   .groupBy { case (gh, _)    => gh }
   .map     { case (gh, iter) => (gh.bbox.envelope, iter.size) }

density.collect.foreach(println)
{% endhighlight %}

The resulting density plot is visualized below.

!["Registering new Data Store"](/img/tutorials/2014-08-05-spark/gdelt-global-density.png)
