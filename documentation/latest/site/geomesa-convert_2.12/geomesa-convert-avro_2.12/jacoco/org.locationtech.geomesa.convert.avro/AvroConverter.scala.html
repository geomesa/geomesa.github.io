<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml" lang=""><head><meta http-equiv="Content-Type" content="text/html;charset=UTF-8"/><link rel="stylesheet" href="../jacoco-resources/report.css" type="text/css"/><link rel="shortcut icon" href="../jacoco-resources/report.gif" type="image/gif"/><title>AvroConverter.scala</title><link rel="stylesheet" href="../jacoco-resources/prettify.css" type="text/css"/><script type="text/javascript" src="../jacoco-resources/prettify.js"></script></head><body onload="window['PR_TAB_WIDTH']=4;prettyPrint()"><div class="breadcrumb" id="breadcrumb"><span class="info"><a href="../jacoco-sessions.html" class="el_session">Sessions</a></span><a href="../index.html" class="el_report">GeoMesa Convert Avro</a> &gt; <a href="index.source.html" class="el_package">org.locationtech.geomesa.convert.avro</a> &gt; <span class="el_source">AvroConverter.scala</span></div><h1>AvroConverter.scala</h1><pre class="source lang-java linenums">/***********************************************************************
 * Copyright (c) 2013-2025 General Atomics Integrated Intelligence, Inc.
 * All rights reserved. This program and the accompanying materials
 * are made available under the terms of the Apache License, Version 2.0
 * which accompanies this distribution and is available at
 * https://www.apache.org/licenses/LICENSE-2.0
 ***********************************************************************/

package org.locationtech.geomesa.convert.avro

import com.typesafe.config.Config
import org.apache.avro.Schema
import org.apache.avro.Schema.Parser
import org.apache.avro.file.DataFileStream
import org.apache.avro.generic.{GenericDatumReader, GenericDatumWriter, GenericRecord}
import org.apache.avro.io.{DecoderFactory, EncoderFactory}
import org.geotools.api.feature.simple.SimpleFeatureType
import org.locationtech.geomesa.convert.EvaluationContext
import org.locationtech.geomesa.convert.avro.AvroConverter._
import org.locationtech.geomesa.convert2.AbstractConverter.{BasicField, BasicOptions}
import org.locationtech.geomesa.convert2.transforms.Expression
import org.locationtech.geomesa.convert2.transforms.Expression.Column
import org.locationtech.geomesa.convert2.{AbstractConverter, ConverterConfig, ConverterName}
import org.locationtech.geomesa.utils.collection.CloseableIterator
import org.locationtech.geomesa.utils.io.CopyingInputStream

import java.io.{ByteArrayOutputStream, InputStream}

<span class="nc" id="L29">class AvroConverter(sft: SimpleFeatureType, config: AvroConfig, fields: Seq[BasicField], options: BasicOptions)</span>
<span class="nc" id="L30">    extends AbstractConverter[GenericRecord, AvroConfig, BasicField, BasicOptions](sft, config, fields, options) {</span>

<span class="nc" id="L32">  private val schema = config.schema match {</span>
<span class="nc bnc" id="L33" title="All 2 branches missed.">    case SchemaEmbedded =&gt; None</span>
<span class="nc bnc" id="L34" title="All 2 branches missed.">    case SchemaString(s) =&gt; Some(new Parser().parse(s))</span>
<span class="nc bnc" id="L35" title="All 2 branches missed.">    case SchemaFile(s) =&gt;</span>
<span class="nc" id="L36">      val loader = Option(Thread.currentThread.getContextClassLoader).getOrElse(getClass.getClassLoader)</span>
<span class="nc" id="L37">      val res = Option(loader.getResourceAsStream(s)).orElse(Option(getClass.getResourceAsStream(s))).getOrElse {</span>
<span class="nc" id="L38">        throw new IllegalArgumentException(s&quot;Could not load schema resource at $s&quot;)</span>
      }
<span class="nc" id="L40">      Some(new Parser().parse(res))</span>
  }

  // if required, set the raw bytes in the result array
<span class="nc" id="L44">  private val requiresBytes = {</span>
<span class="nc" id="L45">    val expressions = config.idField.toSeq ++ fields.flatMap(_.transforms) ++ config.userData.values</span>
<span class="nc" id="L46">    Expression.flatten(expressions).contains(Column(0))</span>
  }

  override protected def parse(is: InputStream, ec: EvaluationContext): CloseableIterator[GenericRecord] = {
<span class="nc" id="L50">    schema match {</span>
<span class="nc bnc" id="L51" title="All 4 branches missed.">      case Some(s) if requiresBytes =&gt; new GenericRecordBytesIterator(new CopyingInputStream(is), s, ec)</span>
<span class="nc bnc" id="L52" title="All 2 branches missed.">      case Some(s)                  =&gt; new GenericRecordIterator(is, s, ec)</span>
<span class="nc bnc" id="L53" title="All 4 branches missed.">      case None    if requiresBytes =&gt; new FileStreamBytesIterator(is, ec)</span>
<span class="nc bnc" id="L54" title="All 2 branches missed.">      case None                     =&gt; new FileStreamIterator(is, ec)</span>
    }
  }

  override protected def values(parsed: CloseableIterator[GenericRecord],
                                ec: EvaluationContext): CloseableIterator[Array[Any]] = {
<span class="nc" id="L60">    val array = Array.ofDim[Any](2)</span>
<span class="nc bnc" id="L61" title="All 2 branches missed.">    if (requiresBytes) {</span>
<span class="nc" id="L62">      parsed.map { record =&gt; array(0) = record.get(BytesField); array(1) = record; array }</span>
    } else {
<span class="nc" id="L64">      parsed.map { record =&gt; array(1) = record; array }</span>
    }
  }
}

<span class="nc" id="L69">object AvroConverter {</span>

  import scala.collection.JavaConverters._

<span class="nc" id="L73">  val BytesField = &quot;__bytes__&quot;</span>

  /**
    * Add a `__bytes__` field to the schema, for storing the raw bytes
    *
    * @param schema schema
    * @return
    */
  def addBytes(schema: Schema): Schema = {
<span class="nc" id="L82">    schema.getType match {</span>
<span class="nc bnc" id="L83" title="All 2 branches missed.">      case Schema.Type.RECORD =&gt;</span>
<span class="nc" id="L84">        val fields = new java.util.ArrayList[Schema.Field](schema.getFields.size() + 1)</span>
<span class="nc" id="L85">        schema.getFields.asScala.foreach { field =&gt;</span>
<span class="nc" id="L86">          fields.add(new Schema.Field(field.name, field.schema, field.doc, field.defaultVal()))</span>
        }
<span class="nc" id="L88">        fields.add(new Schema.Field(BytesField, Schema.create(Schema.Type.BYTES), &quot;raw bytes&quot;, &quot;&quot;))</span>

<span class="nc" id="L90">        val updated = Schema.createRecord(schema.getName, schema.getDoc, schema.getNamespace, schema.isError)</span>
<span class="nc" id="L91">        updated.setFields(fields)</span>
<span class="nc" id="L92">        updated</span>

<span class="nc bnc" id="L94" title="All 2 branches missed.">      case Schema.Type.UNION =&gt;</span>
<span class="nc" id="L95">        Schema.createUnion(schema.getTypes.asScala.map(s =&gt; addBytes(s)).toSeq: _*)</span>

      case _ =&gt;
<span class="nc" id="L98">        throw new UnsupportedOperationException(</span>
<span class="nc" id="L99">          s&quot;Raw Avro bytes (i.e. $$0) is not implemented for schema type ${schema.getType}&quot;)</span>
    }
  }

<span class="nc bnc" id="L103" title="All 53 branches missed.">  case class AvroConfig(</span>
<span class="nc" id="L104">      `type`: String,</span>
<span class="nc" id="L105">      converterName: Option[String],</span>
<span class="nc" id="L106">      schema: SchemaConfig,</span>
<span class="nc" id="L107">      idField: Option[Expression],</span>
<span class="nc" id="L108">      caches: Map[String, Config],</span>
<span class="nc" id="L109">      userData: Map[String, Expression]</span>
<span class="nc" id="L110">    ) extends ConverterConfig with ConverterName</span>

  sealed trait SchemaConfig

<span class="nc bnc" id="L114" title="All 18 branches missed.">  case class SchemaString(schema: String) extends SchemaConfig</span>
<span class="nc bnc" id="L115" title="All 18 branches missed.">  case class SchemaFile(file: String) extends SchemaConfig</span>
<span class="nc" id="L116">  case object SchemaEmbedded extends SchemaConfig {</span>
<span class="nc" id="L117">    val name: String = &quot;embedded&quot;</span>
  }

  /**
    * Reads avro records using a pre-defined schema
    *
    * @param is input stream
    * @param schema schema
    * @param ec evaluation context
    */
<span class="nc" id="L127">  class GenericRecordIterator private [AvroConverter] (is: InputStream, schema: Schema, ec: EvaluationContext)</span>
<span class="nc" id="L128">      extends CloseableIterator[GenericRecord] {</span>

<span class="nc" id="L130">    private val reader = new GenericDatumReader[GenericRecord](schema)</span>
<span class="nc" id="L131">    private val decoder = DecoderFactory.get.binaryDecoder(is, null)</span>
<span class="nc" id="L132">    private var record: GenericRecord = _</span>

<span class="nc bnc" id="L134" title="All 2 branches missed.">    override def hasNext: Boolean = !decoder.isEnd</span>

    override def next(): GenericRecord = {
<span class="nc" id="L137">      ec.line += 1</span>
<span class="nc" id="L138">      record = reader.read(record, decoder)</span>
<span class="nc" id="L139">      record</span>
    }

<span class="nc" id="L142">    override def close(): Unit = is.close()</span>
  }

  /**
    * Reads avro records using a pre-defined schema, setting the bytes for each record in a
    * special `__bytes__` field
    *
    * @param is input stream
    * @param schema schema
    * @param ec evaluation context
    */
<span class="nc" id="L153">  class GenericRecordBytesIterator private [AvroConverter] (is: CopyingInputStream, schema: Schema, ec: EvaluationContext)</span>
<span class="nc" id="L154">      extends CloseableIterator[GenericRecord] {</span>

<span class="nc" id="L156">    private val reader = new GenericDatumReader[GenericRecord](schema, addBytes(schema))</span>
<span class="nc" id="L157">    private val decoder = DecoderFactory.get.binaryDecoder(is, null)</span>
<span class="nc" id="L158">    private var record: GenericRecord = _</span>

<span class="nc bnc" id="L160" title="All 2 branches missed.">    override def hasNext: Boolean = !decoder.isEnd</span>

    override def next(): GenericRecord = {
<span class="nc" id="L163">      ec.line += 1</span>
<span class="nc" id="L164">      record = reader.read(record, decoder)</span>
      // parse out the bytes read and set them in the record
      // check to see if the decoder buffered some bytes that weren't actually used
<span class="nc" id="L167">      val buffered = decoder.inputStream().available()</span>
<span class="nc" id="L168">      record.put(BytesField, is.replay(is.copied - buffered))</span>
<span class="nc" id="L169">      record</span>
    }

<span class="nc" id="L172">    override def close(): Unit = is.close()</span>
  }

  /**
    * Reads avro records from an avro file, with the schema embedded
    *
    * @param is input
    * @param ec evaluation context
    */
<span class="nc" id="L181">  class FileStreamIterator private [AvroConverter] (is: InputStream, ec: EvaluationContext)</span>
<span class="nc" id="L182">      extends CloseableIterator[GenericRecord] {</span>

<span class="nc" id="L184">    private val stream = new DataFileStream(is, new GenericDatumReader[GenericRecord]())</span>
<span class="nc" id="L185">    private var record: GenericRecord = _</span>

<span class="nc" id="L187">    override def hasNext: Boolean = stream.hasNext</span>

    override def next(): GenericRecord = {
<span class="nc" id="L190">      ec.line += 1</span>
<span class="nc" id="L191">      record = stream.next(record)</span>
<span class="nc" id="L192">      record</span>
    }

<span class="nc" id="L195">    override def close(): Unit = stream.close()</span>
  }

  /**
    * Reads avro records from an avro file, with the schema embedded, setting the bytes for
    * each record in a special `__bytes__` field
    *
    * @param is input
    * @param ec evaluation context
    */
<span class="nc" id="L205">  class FileStreamBytesIterator private [AvroConverter] (is: InputStream, ec: EvaluationContext)</span>
<span class="nc" id="L206">      extends CloseableIterator[GenericRecord] {</span>

<span class="nc" id="L208">    private val reader = new GenericDatumReader[GenericRecord]()</span>
<span class="nc" id="L209">    private val stream = new DataFileStream(is, reader)</span>
<span class="nc" id="L210">    private var record: GenericRecord = _</span>

<span class="nc" id="L212">    reader.setExpected(addBytes(reader.getSchema))</span>

    // we can't tell which bytes correspond to which feature (due to buffering). if we could access the
    // underlying avro encoder we could figure it out, but it is not exposed through DataFileStream. instead,
    // re-serialize each record to get the raw bytes
<span class="nc" id="L217">    private val out = new ByteArrayOutputStream()</span>
<span class="nc" id="L218">    private val writer = new GenericDatumWriter[GenericRecord](stream.getSchema)</span>
<span class="nc" id="L219">    private val encoder = EncoderFactory.get.binaryEncoder(out, null)</span>

<span class="nc" id="L221">    override def hasNext: Boolean = stream.hasNext</span>

    override def next(): GenericRecord = {
<span class="nc" id="L224">      ec.line += 1</span>
<span class="nc" id="L225">      record = stream.next(record)</span>
      // regenerate the bytes read and set them in the record
<span class="nc" id="L227">      out.reset()</span>
<span class="nc" id="L228">      writer.write(record, encoder)</span>
<span class="nc" id="L229">      encoder.flush()</span>
<span class="nc" id="L230">      record.put(BytesField, out.toByteArray)</span>
<span class="nc" id="L231">      record</span>
    }

<span class="nc" id="L234">    override def close(): Unit = stream.close()</span>
  }
<span class="nc" id="L236">}</span>
</pre><div class="footer"><span class="right">Created with <a href="http://www.jacoco.org/jacoco">JaCoCo</a> 0.8.14.202510111229</span></div></body></html>