<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml" lang=""><head><meta http-equiv="Content-Type" content="text/html;charset=UTF-8"/><link rel="stylesheet" href="../jacoco-resources/report.css" type="text/css"/><link rel="shortcut icon" href="../jacoco-resources/report.gif" type="image/gif"/><title>FileStreamInputFormat.scala</title><link rel="stylesheet" href="../jacoco-resources/prettify.css" type="text/css"/><script type="text/javascript" src="../jacoco-resources/prettify.js"></script></head><body onload="window['PR_TAB_WIDTH']=4;prettyPrint()"><div class="breadcrumb" id="breadcrumb"><span class="info"><a href="../jacoco-sessions.html" class="el_session">Sessions</a></span><a href="../index.html" class="el_report">GeoMesa Jobs</a> &gt; <a href="index.source.html" class="el_package">org.locationtech.geomesa.jobs.mapreduce</a> &gt; <span class="el_source">FileStreamInputFormat.scala</span></div><h1>FileStreamInputFormat.scala</h1><pre class="source lang-java linenums">/***********************************************************************
 * Copyright (c) 2013-2025 General Atomics Integrated Intelligence, Inc.
 * All rights reserved. This program and the accompanying materials
 * are made available under the terms of the Apache License, Version 2.0
 * which accompanies this distribution and is available at
 * https://www.apache.org/licenses/LICENSE-2.0
 ***********************************************************************/

package org.locationtech.geomesa.jobs.mapreduce

import com.typesafe.scalalogging.LazyLogging
import org.apache.commons.io.IOUtils
import org.apache.hadoop.conf.Configuration
import org.apache.hadoop.fs.{Path, Seekable}
import org.apache.hadoop.io.LongWritable
import org.apache.hadoop.io.compress.{CodecPool, CompressionCodecFactory, Decompressor}
import org.apache.hadoop.mapreduce._
import org.apache.hadoop.mapreduce.lib.input.{FileInputFormat, FileSplit}
import org.geotools.api.feature.simple.{SimpleFeature, SimpleFeatureType}
import org.locationtech.geomesa.utils.geotools.SimpleFeatureTypes

import java.io.{Closeable, InputStream}
import scala.util.control.NonFatal

/**
 * Input format that gives us access to the entire file as a byte stream via the record reader.
 */
<span class="nc" id="L28">abstract class FileStreamInputFormat extends FileInputFormat[LongWritable, SimpleFeature] {</span>

  type SFRR = RecordReader[LongWritable, SimpleFeature]

<span class="nc" id="L32">  override protected def isSplitable(context: JobContext, filename: Path): Boolean = false</span>

  override def createRecordReader(split: InputSplit, context: TaskAttemptContext): SFRR =
<span class="nc" id="L35">    createRecordReader()</span>

  /**
   * Abstract method to create a subclass of record reader
   *
   * @return file stream record reader implementation
   */
  def createRecordReader(): FileStreamRecordReader
}

<span class="nc" id="L45">object FileStreamInputFormat {</span>

<span class="nc" id="L47">  val SftKey        = &quot;org.locationtech.geomesa.jobs.ingest.sft&quot;</span>
<span class="nc" id="L48">  val TypeNameKey   = &quot;org.locationtech.geomesa.jobs.ingest.sft.name&quot;</span>

  /**
   * Set the simple feature type in the job configuration for distributed access.
   *
   * @param job job
   * @param sft simple feature type
   */
<span class="nc" id="L56">  def setSft(job: Job, sft: SimpleFeatureType): Unit = setSft(job.getConfiguration, sft)</span>

  /**
    * Set the simple feature type in the job configuration for distributed access.
    *
    * @param conf job conf
    * @param sft simple feature type
    */
  def setSft(conf: Configuration, sft: SimpleFeatureType): Unit = {
<span class="nc" id="L65">    conf.set(SftKey, SimpleFeatureTypes.encodeType(sft))</span>
<span class="nc" id="L66">    conf.set(TypeNameKey, sft.getTypeName)</span>
  }

  /**
   * Gets the simple feature type previously set with setSft
   *
   * @param conf job configuration
   * @return simple feature type
   */
  def getSft(conf: Configuration): SimpleFeatureType = {
<span class="nc" id="L76">    val typeName = conf.get(FileStreamInputFormat.TypeNameKey)</span>
<span class="nc" id="L77">    SimpleFeatureTypes.createType(typeName, conf.get(FileStreamInputFormat.SftKey))</span>
  }
}

/**
 * Base class for operating on file input streams. Abstracts away most of the m/r framework.
 */
<span class="nc bnc" id="L84" title="All 4 branches missed.">abstract class FileStreamRecordReader() extends RecordReader[LongWritable, SimpleFeature] with LazyLogging {</span>

<span class="nc" id="L86">  private var dec: Decompressor = _</span>
<span class="nc" id="L87">  private var stream: InputStream with Seekable = _</span>
<span class="nc" id="L88">  private var iter: Iterator[SimpleFeature] with Closeable = _</span>
<span class="nc" id="L89">  private var length: Float = 0</span>

<span class="nc" id="L91">  private val curKey = new LongWritable(0)</span>
<span class="nc" id="L92">  private var curValue: SimpleFeature = _</span>

  def createIterator(stream: InputStream with Seekable,
                     filePath: Path,
                     context: TaskAttemptContext): Iterator[SimpleFeature] with Closeable

  override def getProgress: Float = {
<span class="nc bnc" id="L99" title="All 2 branches missed.">    if (length == 0) { 0f } else {</span>
<span class="nc" id="L100">      try { math.min(1f, stream.getPos / length) } catch {</span>
<span class="nc bnc" id="L101" title="All 4 branches missed.">        case NonFatal(e) =&gt; logger.warn(s&quot;Error checking stream position - it may be closed? $e&quot;); 1f</span>
      }
    }
  }

  override def nextKeyValue(): Boolean = {
<span class="nc bnc" id="L107" title="All 2 branches missed.">    if (iter.hasNext) {</span>
<span class="nc" id="L108">      curKey.set(curKey.get() + 1)</span>
<span class="nc" id="L109">      curValue = iter.next()</span>
<span class="nc" id="L110">      true</span>
    } else {
<span class="nc" id="L112">      false</span>
    }
  }

<span class="nc" id="L116">  override def getCurrentValue: SimpleFeature = curValue</span>

  override def initialize(split: InputSplit, context: TaskAttemptContext): Unit = {
<span class="nc" id="L119">    val job   = context.getConfiguration</span>
<span class="nc" id="L120">    val path  = split.asInstanceOf[FileSplit].getPath</span>
<span class="nc" id="L121">    val codec = new CompressionCodecFactory(job).getCodec(path)</span>
<span class="nc" id="L122">    val fs    = path.getFileSystem(job)</span>

<span class="nc" id="L124">    length = split.getLength.toFloat</span>
<span class="nc" id="L125">    stream =</span>
<span class="nc bnc" id="L126" title="All 2 branches missed.">      if (codec != null) {</span>
<span class="nc" id="L127">        dec = CodecPool.getDecompressor(codec)</span>
<span class="nc" id="L128">        codec.createInputStream(fs.open(path), dec)</span>
      } else {
<span class="nc" id="L130">        fs.open(path)</span>
      }
<span class="nc" id="L132">    iter = createIterator(stream, path, context)</span>
<span class="nc bnc" id="L133" title="All 2 branches missed.">    logger.debug(s&quot;Initialized record reader on split ${path.toString}&quot;)</span>
  }

<span class="nc" id="L136">  override def getCurrentKey: LongWritable = curKey</span>

  override def close(): Unit = {
<span class="nc" id="L139">    IOUtils.closeQuietly(iter)</span>
<span class="nc" id="L140">    IOUtils.closeQuietly(stream)</span>
<span class="nc bnc" id="L141" title="All 2 branches missed.">    if (dec != null) {</span>
<span class="nc" id="L142">      CodecPool.returnDecompressor(dec)</span>
    }
  }
}
</pre><div class="footer"><span class="right">Created with <a href="http://www.jacoco.org/jacoco">JaCoCo</a> 0.8.14.202510111229</span></div></body></html>