<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml" lang=""><head><meta http-equiv="Content-Type" content="text/html;charset=UTF-8"/><link rel="stylesheet" href="../jacoco-resources/report.css" type="text/css"/><link rel="shortcut icon" href="../jacoco-resources/report.gif" type="image/gif"/><title>ConverterInputFormat.scala</title><link rel="stylesheet" href="../jacoco-resources/prettify.css" type="text/css"/><script type="text/javascript" src="../jacoco-resources/prettify.js"></script></head><body onload="window['PR_TAB_WIDTH']=4;prettyPrint()"><div class="breadcrumb" id="breadcrumb"><span class="info"><a href="../jacoco-sessions.html" class="el_session">Sessions</a></span><a href="../index.html" class="el_report">GeoMesa Jobs</a> &gt; <a href="index.source.html" class="el_package">org.locationtech.geomesa.jobs.mapreduce</a> &gt; <span class="el_source">ConverterInputFormat.scala</span></div><h1>ConverterInputFormat.scala</h1><pre class="source lang-java linenums">/***********************************************************************
 * Copyright (c) 2013-2025 General Atomics Integrated Intelligence, Inc.
 * All rights reserved. This program and the accompanying materials
 * are made available under the terms of the Apache License, Version 2.0
 * which accompanies this distribution and is available at
 * https://www.apache.org/licenses/LICENSE-2.0
 ***********************************************************************/

package org.locationtech.geomesa.jobs.mapreduce

import com.typesafe.config.ConfigFactory
import com.typesafe.scalalogging.LazyLogging
import org.apache.commons.compress.archivers.zip.ZipFile
import org.apache.commons.compress.archivers.{ArchiveEntry, ArchiveInputStream, ArchiveStreamFactory}
import org.apache.commons.compress.utils.SeekableInMemoryByteChannel
import org.apache.commons.io.IOUtils
import org.apache.hadoop.conf.Configuration
import org.apache.hadoop.fs.{Path, Seekable}
import org.apache.hadoop.io.LongWritable
import org.apache.hadoop.mapreduce._
import org.apache.hadoop.mapreduce.lib.input.{CombineFileInputFormat, CombineFileRecordReader, CombineFileRecordReaderWrapper, CombineFileSplit}
import org.geotools.api.feature.simple.{SimpleFeature, SimpleFeatureType}
import org.geotools.data.ReTypeFeatureReader
import org.geotools.data.simple.DelegateSimpleFeatureReader
import org.geotools.feature.collection.DelegateSimpleFeatureIterator
import org.geotools.filter.text.ecql.ECQL
import org.locationtech.geomesa.convert.EvaluationContext
import org.locationtech.geomesa.convert.EvaluationContext.ContextListener
import org.locationtech.geomesa.convert2.SimpleFeatureConverter
import org.locationtech.geomesa.jobs.GeoMesaConfigurator
import org.locationtech.geomesa.jobs.mapreduce.ConverterInputFormat.{ConverterCounters, ConverterKey, RetypeKey}
import org.locationtech.geomesa.utils.collection.CloseableIterator
import org.locationtech.geomesa.utils.geotools.SimpleFeatureTypes
import org.locationtech.geomesa.utils.io.fs.{ArchiveFileIterator, ZipFileIterator}
import org.locationtech.geomesa.utils.io.{CloseWithLogging, PathUtils}

import java.io.{Closeable, InputStream}
import java.util.Locale

/**
 * Input format for Converters gives us access to the entire file as a byte stream
 * via the record reader.
 */
<span class="nc" id="L44">class ConverterInputFormat extends FileStreamInputFormat {</span>
<span class="nc" id="L45">  override def createRecordReader(): FileStreamRecordReader = new ConverterRecordReader</span>
}

<span class="nc" id="L48">object ConverterInputFormat {</span>

  // note: we can get away with a single instance b/c m/r doesn't end up sharing it
<span class="nc bnc" id="L51" title="All 4 branches missed.">  lazy private [mapreduce] val instance = new ConverterInputFormat</span>

<span class="nc" id="L53">  object ConverterCounters {</span>
<span class="nc" id="L54">    val Group     = &quot;org.locationtech.geomesa.jobs.convert&quot;</span>
<span class="nc" id="L55">    val Converted = &quot;converted&quot;</span>
<span class="nc" id="L56">    val Failed    = &quot;failed&quot;</span>
  }

<span class="nc" id="L59">  val ConverterKey  = &quot;org.locationtech.geomesa.jobs.ingest.converter&quot;</span>
<span class="nc" id="L60">  val RetypeKey = &quot;org.locationtech.geomesa.jobs.ingest.retype&quot;</span>

<span class="nc" id="L62">  def setConverterConfig(job: Job, config: String): Unit = setConverterConfig(job.getConfiguration, config)</span>
<span class="nc" id="L63">  def setConverterConfig(conf: Configuration, config: String): Unit = conf.set(ConverterKey, config)</span>

<span class="nc" id="L65">  def setSft(job: Job, sft: SimpleFeatureType): Unit = FileStreamInputFormat.setSft(job, sft)</span>
<span class="nc" id="L66">  def setSft(conf: Configuration, sft: SimpleFeatureType): Unit = FileStreamInputFormat.setSft(conf, sft)</span>

<span class="nc" id="L68">  def setRetypeSft(job: Job, sft: SimpleFeatureType): Unit = setRetypeSft(job.getConfiguration, sft)</span>
<span class="nc" id="L69">  def setRetypeSft(conf: Configuration, sft: SimpleFeatureType): Unit = conf.set(RetypeKey, SimpleFeatureTypes.encodeType(sft))</span>

<span class="nc" id="L71">  def setFilter(job: Job, ecql: String): Unit = setFilter(job.getConfiguration, ecql)</span>
<span class="nc" id="L72">  def setFilter(conf: Configuration, ecql: String): Unit = GeoMesaConfigurator.setFilter(conf, ecql)</span>
}

<span class="nc" id="L75">class ConverterCombineInputFormat extends CombineFileInputFormat[LongWritable, SimpleFeature] {</span>

<span class="nc" id="L77">  override protected def isSplitable(context: JobContext, filename: Path): Boolean = false</span>

  override def createRecordReader(split: InputSplit, context: TaskAttemptContext) =
<span class="nc" id="L80">    new CombineFileRecordReader(split.asInstanceOf[CombineFileSplit], context, classOf[CombineFileStreamRecordReaderWrapper])</span>
}

<span class="nc" id="L83">class CombineFileStreamRecordReaderWrapper(split: CombineFileSplit,</span>
                                           ctx: TaskAttemptContext,
                                           idx: java.lang.Integer)
<span class="nc" id="L86">  extends CombineFileRecordReaderWrapper[LongWritable, SimpleFeature](ConverterInputFormat.instance, split, ctx, idx)</span>

<span class="nc" id="L88">class ConverterRecordReader extends FileStreamRecordReader with LazyLogging {</span>

  import scala.collection.JavaConverters._

  override def createIterator(
      stream: InputStream with Seekable,
      filePath: Path,
      context: TaskAttemptContext): Iterator[SimpleFeature] with Closeable = {

<span class="nc" id="L97">    val confStr   = context.getConfiguration.get(ConverterKey)</span>
<span class="nc" id="L98">    val conf      = ConfigFactory.parseString(confStr)</span>
<span class="nc" id="L99">    val sft       = FileStreamInputFormat.getSft(context.getConfiguration)</span>
<span class="nc" id="L100">    val converter = SimpleFeatureConverter(sft, conf)</span>
<span class="nc" id="L101">    val filter    = GeoMesaConfigurator.getFilter(context.getConfiguration).map(ECQL.toFilter)</span>
<span class="nc" id="L102">    val retypedSpec = context.getConfiguration.get(RetypeKey)</span>

    def ec(path: String): EvaluationContext = {
      // global success/failure counters for the entire job
<span class="nc" id="L106">      val success = context.getCounter(ConverterCounters.Group, ConverterCounters.Converted)</span>
<span class="nc" id="L107">      val failure = context.getCounter(ConverterCounters.Group, ConverterCounters.Failed)</span>
<span class="nc" id="L108">      converter.createEvaluationContext(EvaluationContext.inputFileParam(path)).withListener(new MapReduceListener(success, failure))</span>
    }

<span class="nc bnc" id="L111" title="All 2 branches missed.">    lazy val defaultEc = ec(filePath.toString)</span>

    val streams: CloseableIterator[(Option[String], InputStream)] =
<span class="nc" id="L114">      PathUtils.getUncompressedExtension(filePath.getName).toLowerCase(Locale.US) match {</span>
<span class="nc bnc" id="L115" title="All 2 branches missed.">        case ArchiveStreamFactory.TAR =&gt;</span>
          val archive: ArchiveInputStream[_ &lt;: ArchiveEntry] =
<span class="nc" id="L117">            new ArchiveStreamFactory().createArchiveInputStream(ArchiveStreamFactory.TAR, stream)</span>
<span class="nc" id="L118">          new ArchiveFileIterator(archive, filePath.toString)</span>

<span class="nc bnc" id="L120" title="All 6 branches missed.">        case ArchiveStreamFactory.ZIP | ArchiveStreamFactory.JAR =&gt;</span>
          // we have to read the bytes into memory to get random access reads
          // note: stream is closed in super class
<span class="nc" id="L123">          val bytes = new SeekableInMemoryByteChannel(IOUtils.toByteArray(stream))</span>
<span class="nc" id="L124">          new ZipFileIterator(ZipFile.builder.setSeekableByteChannel(bytes).get(), filePath.toString)</span>

        case _ =&gt;
<span class="nc" id="L127">          CloseableIterator.single(None -&gt; stream, stream.close())</span>
      }

<span class="nc bnc" id="L130" title="All 2 branches missed.">    val all = streams.flatMap { case (name, is) =&gt;</span>
<span class="nc" id="L131">      converter.process(is, name.map(ec).getOrElse(defaultEc))</span>
    }
<span class="nc" id="L133">    val iter = filter match {</span>
<span class="nc bnc" id="L134" title="All 2 branches missed.">      case Some(f) =&gt; all.filter(f.evaluate)</span>
<span class="nc bnc" id="L135" title="All 2 branches missed.">      case None    =&gt; all</span>
    }

<span class="nc bnc" id="L138" title="All 2 branches missed.">    val featureReader = if (retypedSpec != null) {</span>
<span class="nc" id="L139">      val retypedSft = SimpleFeatureTypes.createType(sft.getTypeName, retypedSpec)</span>
<span class="nc" id="L140">      val reader = new DelegateSimpleFeatureReader(sft, new DelegateSimpleFeatureIterator(iter.asJava))</span>
<span class="nc" id="L141">      new ReTypeFeatureReader(reader, retypedSft)</span>
    } else {
<span class="nc" id="L143">      new DelegateSimpleFeatureReader(sft, new DelegateSimpleFeatureIterator(iter.asJava))</span>
    }

<span class="nc bnc" id="L146" title="All 2 branches missed.">    logger.debug(s&quot;Initialized record reader on split ${filePath.toString} with &quot; +</span>
<span class="nc" id="L147">      s&quot;type name ${sft.getTypeName} and convert conf $confStr&quot;)</span>

<span class="nc" id="L149">    new Iterator[SimpleFeature] with Closeable {</span>
<span class="nc" id="L150">      override def hasNext: Boolean = featureReader.hasNext</span>
<span class="nc" id="L151">      override def next(): SimpleFeature = featureReader.next</span>
      override def close(): Unit = {
<span class="nc" id="L153">        CloseWithLogging(featureReader)</span>
<span class="nc" id="L154">        CloseWithLogging(iter)</span>
<span class="nc" id="L155">        CloseWithLogging(converter)</span>
      }
    }
  }

<span class="nc bnc" id="L160" title="All 2 branches missed.">  private class MapReduceListener(success: Counter, failure: Counter) extends ContextListener {</span>
<span class="nc" id="L161">    override def onSuccess(i: Int): Unit = success.increment(i)</span>
<span class="nc" id="L162">    override def onFailure(i: Int): Unit = failure.increment(i)</span>
  }
}
</pre><div class="footer"><span class="right">Created with <a href="http://www.jacoco.org/jacoco">JaCoCo</a> 0.8.14.202510111229</span></div></body></html>