<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml" lang=""><head><meta http-equiv="Content-Type" content="text/html;charset=UTF-8"/><link rel="stylesheet" href="../jacoco-resources/report.css" type="text/css"/><link rel="shortcut icon" href="../jacoco-resources/report.gif" type="image/gif"/><title>HadoopDelegate.scala</title><link rel="stylesheet" href="../jacoco-resources/prettify.css" type="text/css"/><script type="text/javascript" src="../jacoco-resources/prettify.js"></script></head><body onload="window['PR_TAB_WIDTH']=4;prettyPrint()"><div class="breadcrumb" id="breadcrumb"><span class="info"><a href="../jacoco-sessions.html" class="el_session">Sessions</a></span><a href="../index.html" class="el_report">GeoMesa Hadoop Utils</a> &gt; <a href="index.source.html" class="el_package">org.locationtech.geomesa.utils.hadoop</a> &gt; <span class="el_source">HadoopDelegate.scala</span></div><h1>HadoopDelegate.scala</h1><pre class="source lang-java linenums">/***********************************************************************
 * Copyright (c) 2013-2025 General Atomics Integrated Intelligence, Inc.
 * All rights reserved. This program and the accompanying materials
 * are made available under the terms of the Apache License, Version 2.0
 * which accompanies this distribution and is available at
 * https://www.apache.org/licenses/LICENSE-2.0
 ***********************************************************************/

package org.locationtech.geomesa.utils.hadoop

import com.typesafe.scalalogging.LazyLogging
import org.apache.commons.compress.archivers.zip.ZipFile
import org.apache.commons.compress.archivers.{ArchiveEntry, ArchiveInputStream, ArchiveStreamFactory}
import org.apache.commons.compress.utils.SeekableInMemoryByteChannel
import org.apache.commons.io.IOUtils
import org.apache.hadoop.conf.Configuration
import org.apache.hadoop.fs._
import org.locationtech.geomesa.utils.collection.CloseableIterator
import org.locationtech.geomesa.utils.hadoop.HadoopDelegate.{HadoopFileHandle, HadoopTarHandle, HadoopZipHandle}
import org.locationtech.geomesa.utils.io.fs.FileSystemDelegate.{CreateMode, FileHandle}
import org.locationtech.geomesa.utils.io.fs.{ArchiveFileIterator, FileSystemDelegate, ZipFileIterator}
import org.locationtech.geomesa.utils.io.{PathUtils, WithClose}

import java.io.{IOException, InputStream, OutputStream}
import java.net.{MalformedURLException, URL}
import java.util.Locale
import scala.collection.mutable.ListBuffer

/**
  * Delegate allows us to avoid a runtime dependency on hadoop
  */
<span class="nc bnc" id="L32" title="All 4 branches missed.">class HadoopDelegate(conf: Configuration) extends FileSystemDelegate {</span>

  import ArchiveStreamFactory.{JAR, TAR, ZIP}
  import HadoopDelegate.HiddenFileFilter
  import org.apache.hadoop.fs.{LocatedFileStatus, Path}

<span class="nc" id="L38">  def this() = this(new Configuration())</span>

  // use the same property as FileInputFormat
<span class="nc" id="L41">  private val recursive = conf.getBoolean(&quot;mapreduce.input.fileinputformat.input.dir.recursive&quot;, false)</span>

  // we need to add the hadoop url factories to the JVM to support hdfs, S3, or wasb
  // we only want to call this once per jvm or it will throw an error
<span class="nc" id="L45">  HadoopDelegate.configureURLFactory()</span>

  override def getHandle(path: String): FileHandle = {
<span class="nc" id="L48">    val p = new Path(path)</span>
<span class="nc" id="L49">    val fs = getSharedFs(p)</span>
<span class="nc" id="L50">    PathUtils.getUncompressedExtension(p.getName).toLowerCase(Locale.US) match {</span>
<span class="nc bnc" id="L51" title="All 2 branches missed.">      case TAR       =&gt; new HadoopTarHandle(fs, p)</span>
<span class="nc bnc" id="L52" title="All 6 branches missed.">      case ZIP | JAR =&gt; new HadoopZipHandle(fs, p)</span>
<span class="nc" id="L53">      case _         =&gt; new HadoopFileHandle(fs, p)</span>
    }
  }

  // based on logic from hadoop FileInputFormat
  override def interpretPath(path: String): Seq[FileHandle] = {
<span class="nc" id="L59">    val p = new Path(path)</span>
<span class="nc" id="L60">    val fs = getSharedFs(p)</span>
<span class="nc" id="L61">    val files = fs.globStatus(p, HiddenFileFilter)</span>

<span class="nc bnc" id="L63" title="All 2 branches missed.">    if (files == null) {</span>
<span class="nc" id="L64">      throw new IllegalArgumentException(s&quot;Input path does not exist: $path&quot;)</span>
<span class="nc bnc" id="L65" title="All 2 branches missed.">    } else if (files.isEmpty) {</span>
<span class="nc" id="L66">      throw new IllegalArgumentException(s&quot;Input path does not match any files: $path&quot;)</span>
    }

<span class="nc" id="L69">    val remaining = scala.collection.mutable.Queue(files: _*)</span>
<span class="nc" id="L70">    val result = ListBuffer.empty[FileHandle]</span>

<span class="nc bnc" id="L72" title="All 2 branches missed.">    while (remaining.nonEmpty) {</span>
<span class="nc" id="L73">      val file = remaining.dequeue()</span>
<span class="nc bnc" id="L74" title="All 2 branches missed.">      if (file.isDirectory) {</span>
<span class="nc bnc" id="L75" title="All 2 branches missed.">        if (recursive) {</span>
<span class="nc" id="L76">          val children = fs.listLocatedStatus(file.getPath)</span>
<span class="nc" id="L77">          val iter = new Iterator[LocatedFileStatus] {</span>
<span class="nc" id="L78">            override def hasNext: Boolean = children.hasNext</span>
<span class="nc" id="L79">            override def next(): LocatedFileStatus = children.next</span>
          }
<span class="nc" id="L81">          remaining ++= iter.filter(f =&gt; HiddenFileFilter.accept(f.getPath))</span>
        }
      } else {
<span class="nc" id="L84">        PathUtils.getUncompressedExtension(file.getPath.getName).toLowerCase(Locale.US) match {</span>
<span class="nc bnc" id="L85" title="All 2 branches missed.">          case TAR       =&gt; result += new HadoopTarHandle(fs, file.getPath)</span>
<span class="nc bnc" id="L86" title="All 6 branches missed.">          case ZIP | JAR =&gt; result += new HadoopZipHandle(fs, file.getPath)</span>
<span class="nc" id="L87">          case _         =&gt; result += new HadoopFileHandle(fs, file.getPath)</span>
        }
      }
    }

<span class="nc" id="L92">    result.result</span>
  }

  override def getUrl(path: String): URL = {
<span class="nc" id="L96">    try { new URL(path) } catch {</span>
<span class="nc" id="L97">      case e: MalformedURLException =&gt; throw new IllegalArgumentException(s&quot;Invalid URL $path: &quot;, e)</span>
    }
  }

  private def getSharedFs(path: Path): FileSystem = {
    // standardize the uri so we get the same cached instance
<span class="nc" id="L103">    val uri = path.toUri</span>
<span class="nc bnc" id="L104" title="All 4 branches missed.">    val root = if (uri.getScheme == null || uri.getAuthority == null) { uri } else { uri.resolve(&quot;/&quot;) }</span>
<span class="nc" id="L105">    FileSystem.get(root, conf)</span>
  }
}

<span class="nc bnc" id="L109" title="All 4 branches missed.">object HadoopDelegate extends LazyLogging {</span>

<span class="nc" id="L111">  private val factory = new ArchiveStreamFactory()</span>

<span class="nc" id="L113">  private var setUrlFactory = true</span>

<span class="nc" id="L115">  val HiddenFileFilter: PathFilter = new PathFilter() {</span>
    override def accept(path: Path): Boolean = {
<span class="nc" id="L117">      val name = path.getName</span>
<span class="nc bnc" id="L118" title="All 4 branches missed.">      !name.startsWith(&quot;_&quot;) &amp;&amp; !name.startsWith(&quot;.&quot;)</span>
    }
  }

  /**
   * Ensure that the Hadoop URL Factory is configured, so that urls staring with hdfs:// can be parsed
   */
  private def configureURLFactory(): Unit = synchronized {
<span class="nc bnc" id="L126" title="All 2 branches missed.">    if (setUrlFactory) {</span>
<span class="nc" id="L127">      setUrlFactory = false</span>
      try { // Calling this method twice in the same JVM causes a java.lang.Error
<span class="nc" id="L129">        URL.setURLStreamHandlerFactory(new FsUrlStreamHandlerFactory)</span>
<span class="nc bnc" id="L130" title="All 2 branches missed.">        logger.trace(&quot;Configured Hadoop URL Factory&quot;)</span>
      } catch {
        case _: Throwable =&gt;
<span class="nc bnc" id="L133" title="All 2 branches missed.">          logger.warn(&quot;Could not register Hadoop URL Factory. Some filesystems may not be available.&quot;)</span>
      }
    }
  }

<span class="nc bnc" id="L138" title="All 4 branches missed.">  class HadoopFileHandle(fs: FileSystem, file: Path) extends FileHandle {</span>

<span class="nc" id="L140">    override def path: String = file.toString</span>

<span class="nc" id="L142">    override def exists: Boolean = fs.exists(file)</span>

<span class="nc bnc" id="L144" title="All 2 branches missed.">    override def length: Long = if (exists) { fs.getFileStatus(file).getLen } else { 0L }</span>

    override def open: CloseableIterator[(Option[String], InputStream)] = {
<span class="nc" id="L147">      val is = PathUtils.handleCompression(fs.open(file), file.getName)</span>
<span class="nc" id="L148">      CloseableIterator.single(None -&gt; is, is.close())</span>
    }

<span class="nc" id="L151">    override def write(mode: CreateMode): OutputStream = {</span>
<span class="nc" id="L152">      mode.validate()</span>
<span class="nc bnc" id="L153" title="All 2 branches missed.">      if (mode.append) {</span>
<span class="nc" id="L154">        fs.append(file)</span>
      } else {
<span class="nc" id="L156">        fs.create(file, mode.overwrite) // TODO do we need to hsync/hflush?</span>
      }
    }

    override def delete(recursive: Boolean): Unit = {
<span class="nc bnc" id="L161" title="All 2 branches missed.">      if (!fs.delete(file, recursive)) {</span>
<span class="nc" id="L162">        throw new IOException(s&quot;Could not delete file: $path&quot;)</span>
      }
    }
  }

<span class="nc" id="L167">  class HadoopZipHandle(fs: FileSystem, file: Path) extends HadoopFileHandle(fs, file) {</span>
    override def open: CloseableIterator[(Option[String], InputStream)] = {
      // we have to read the bytes into memory to get random access reads
<span class="nc" id="L170">      val bytes = WithClose(PathUtils.handleCompression(fs.open(file), file.getName)) { is =&gt;</span>
<span class="nc" id="L171">        IOUtils.toByteArray(is)</span>
      }
<span class="nc" id="L173">      new ZipFileIterator(new ZipFile(new SeekableInMemoryByteChannel(bytes)), file.toString)</span>
    }

    override def write(mode: CreateMode): OutputStream =
<span class="nc" id="L177">      factory.createArchiveOutputStream(ArchiveStreamFactory.ZIP, super.write(mode))</span>
  }

<span class="nc" id="L180">  class HadoopTarHandle(fs: FileSystem, file: Path) extends HadoopFileHandle(fs, file) {</span>
    override def open: CloseableIterator[(Option[String], InputStream)] = {
<span class="nc" id="L182">      val uncompressed = PathUtils.handleCompression(fs.open(file), file.getName)</span>
      val archive: ArchiveInputStream[_ &lt;: ArchiveEntry] =
<span class="nc" id="L184">        factory.createArchiveInputStream(ArchiveStreamFactory.TAR, uncompressed)</span>
<span class="nc" id="L185">      new ArchiveFileIterator(archive, file.toString)</span>
    }

    override def write(mode: CreateMode): OutputStream =
<span class="nc" id="L189">      factory.createArchiveOutputStream(ArchiveStreamFactory.TAR, super.write(mode))</span>
  }
}
</pre><div class="footer"><span class="right">Created with <a href="http://www.jacoco.org/jacoco">JaCoCo</a> 0.8.14.202510111229</span></div></body></html>