<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml" lang=""><head><meta http-equiv="Content-Type" content="text/html;charset=UTF-8"/><link rel="stylesheet" href="../jacoco-resources/report.css" type="text/css"/><link rel="shortcut icon" href="../jacoco-resources/report.gif" type="image/gif"/><title>SQLRules.scala</title><link rel="stylesheet" href="../jacoco-resources/prettify.css" type="text/css"/><script type="text/javascript" src="../jacoco-resources/prettify.js"></script></head><body onload="window['PR_TAB_WIDTH']=4;prettyPrint()"><div class="breadcrumb" id="breadcrumb"><span class="info"><a href="../jacoco-sessions.html" class="el_session">Sessions</a></span><a href="../index.html" class="el_report">GeoMesa Spark SQL</a> &gt; <a href="index.source.html" class="el_package">org.locationtech.geomesa.spark.sql</a> &gt; <span class="el_source">SQLRules.scala</span></div><h1>SQLRules.scala</h1><pre class="source lang-java linenums">/***********************************************************************
 * Copyright (c) 2013-2025 General Atomics Integrated Intelligence, Inc.
 * All rights reserved. This program and the accompanying materials
 * are made available under the terms of the Apache License, Version 2.0
 * which accompanies this distribution and is available at
 * https://www.apache.org/licenses/LICENSE-2.0
 ***********************************************************************/

package org.locationtech.geomesa.spark.sql

import com.typesafe.scalalogging.LazyLogging
import org.apache.spark.sql.catalyst.expressions._
import org.apache.spark.sql.catalyst.plans.logical._
import org.apache.spark.sql.catalyst.rules.Rule
import org.apache.spark.sql.execution.datasources.LogicalRelation
import org.apache.spark.sql.execution.{ProjectExec, SparkPlan}
import org.apache.spark.sql.sedona_sql.UDT.{GeometryUDT =&gt; Sedona_GeometryUDT}
import org.apache.spark.sql.sedona_sql.expressions.{ST_Predicate =&gt; Sedona_ST_Predicate}
import org.apache.spark.sql.types.DataTypes
import org.apache.spark.sql.{SQLContext, Strategy}
import org.geotools.api.filter.expression.{Expression =&gt; GTExpression, Literal =&gt; GTLiteral}
import org.geotools.api.filter.{FilterFactory, Filter =&gt; GTFilter}
import org.geotools.factory.CommonFactoryFinder
import org.locationtech.geomesa.filter.FilterHelper
import org.locationtech.geomesa.spark.haveSedona
import org.locationtech.geomesa.spark.jts.rules.GeometryLiteral
import org.locationtech.geomesa.spark.jts.udf.SpatialRelationFunctions._
import org.locationtech.geomesa.spark.sql.GeoMesaRelation.PartitionedIndexedRDD
import org.locationtech.jts.geom.{Envelope, Geometry}

import java.time.{LocalDateTime, ZoneId, ZoneOffset}
import java.util.Date

<span class="nc bnc" id="L34" title="All 4 branches missed.">object SQLRules extends LazyLogging {</span>
  @transient
<span class="nc" id="L36">  private val ff: FilterFactory = CommonFactoryFinder.getFilterFactory</span>

  def scalaUDFtoGTFilter(udf: Expression): Option[GTFilter] = {
<span class="nc" id="L39">    udf match {</span>
<span class="nc bnc" id="L40" title="All 4 branches missed.">      case u: ScalaUDF if u.children.length == 2 =&gt; buildGTFilter(u.function, u.children.head, u.children.last)</span>
<span class="nc" id="L41">      case _ =&gt; None</span>
    }
  }

  private def buildGTFilter(func: AnyRef, exprA: Expression, exprB: Expression): Option[GTFilter] =
    for {
<span class="nc" id="L47">      builder &lt;- funcToFF(func)</span>
<span class="nc" id="L48">      gtExprA &lt;- sparkExprToGTExpr(exprA)</span>
<span class="nc" id="L49">      gtExprB &lt;- sparkExprToGTExpr(exprB)</span>
    } yield {
<span class="nc" id="L51">      builder(gtExprA, gtExprB)</span>
    }

  def funcToFF(func: AnyRef): Option[(GTExpression, GTExpression) =&gt; GTFilter] = {
<span class="nc" id="L55">    func match {</span>
<span class="nc bnc" id="L56" title="All 6 branches missed.">      case ST_Contains =&gt; Some((expr1: GTExpression, expr2: GTExpression) =&gt;</span>
<span class="nc" id="L57">        ff.contains(expr1, expr2))</span>
<span class="nc bnc" id="L58" title="All 6 branches missed.">      case ST_Crosses =&gt; Some((expr1: GTExpression, expr2: GTExpression) =&gt;</span>
<span class="nc" id="L59">        ff.crosses(expr1, expr2))</span>
<span class="nc bnc" id="L60" title="All 6 branches missed.">      case ST_Disjoint =&gt; Some((expr1: GTExpression, expr2: GTExpression) =&gt;</span>
<span class="nc" id="L61">        ff.disjoint(expr1, expr2))</span>
<span class="nc bnc" id="L62" title="All 6 branches missed.">      case ST_Equals =&gt; Some((expr1: GTExpression, expr2: GTExpression) =&gt;</span>
<span class="nc" id="L63">        ff.equal(expr1, expr2))</span>
<span class="nc bnc" id="L64" title="All 6 branches missed.">      case ST_Intersects =&gt; Some((expr1: GTExpression, expr2: GTExpression) =&gt;</span>
<span class="nc" id="L65">        ff.intersects(expr1, expr2))</span>
<span class="nc bnc" id="L66" title="All 6 branches missed.">      case ST_Overlaps =&gt; Some((expr1: GTExpression, expr2: GTExpression) =&gt;</span>
<span class="nc" id="L67">        ff.overlaps(expr1, expr2))</span>
<span class="nc bnc" id="L68" title="All 6 branches missed.">      case ST_Touches =&gt; Some((expr1: GTExpression, expr2: GTExpression) =&gt;</span>
<span class="nc" id="L69">        ff.touches(expr1, expr2))</span>
<span class="nc bnc" id="L70" title="All 6 branches missed.">      case ST_Within =&gt; Some((expr1: GTExpression, expr2: GTExpression) =&gt;</span>
<span class="nc" id="L71">        ff.within(expr1, expr2))</span>
<span class="nc" id="L72">      case _ =&gt; None</span>
    }
  }

  def sedonaExprToGTFilter(pred: Sedona_ST_Predicate): Option[GTFilter] = {
<span class="nc" id="L77">    sparkExprToGTExpr(pred.children.head).flatMap { expr1 =&gt;</span>
<span class="nc" id="L78">      sparkExprToGTExpr(pred.children.last).flatMap { expr2 =&gt;</span>
        // sedona classes are private, so we have to match on the class name instead of the class itself
<span class="nc" id="L80">        pred.getClass.getSimpleName match {</span>
<span class="nc bnc" id="L81" title="All 2 branches missed.">          case &quot;ST_Contains&quot;   =&gt; Some(ff.contains(expr1, expr2))</span>
<span class="nc bnc" id="L82" title="All 2 branches missed.">          case &quot;ST_Crosses&quot;    =&gt; Some(ff.crosses(expr1, expr2))</span>
<span class="nc bnc" id="L83" title="All 2 branches missed.">          case &quot;ST_Overlaps&quot;   =&gt; Some(ff.overlaps(expr1, expr2))</span>
<span class="nc bnc" id="L84" title="All 2 branches missed.">          case &quot;ST_Intersects&quot; =&gt; Some(ff.intersects(expr1, expr2))</span>
<span class="nc bnc" id="L85" title="All 2 branches missed.">          case &quot;ST_Within&quot;     =&gt; Some(ff.within(expr1, expr2))</span>
<span class="nc bnc" id="L86" title="All 2 branches missed.">          case &quot;ST_Touches&quot;    =&gt; Some(ff.touches(expr1, expr2))</span>
<span class="nc bnc" id="L87" title="All 2 branches missed.">          case &quot;ST_Equals&quot;     =&gt; Some(ff.equal(expr1, expr2))</span>
<span class="nc bnc" id="L88" title="All 2 branches missed.">          case &quot;ST_Disjoint&quot;   =&gt; Some(ff.disjoint(expr1, expr2))</span>
<span class="nc" id="L89">          case _ =&gt; None</span>
        }
      }
    }
  }

  def sparkFilterToGTFilter(expr: Expression): Option[GTFilter] = {
<span class="nc" id="L96">    expr match {</span>
<span class="nc bnc" id="L97" title="All 2 branches missed.">      case udf: ScalaUDF =&gt; scalaUDFtoGTFilter(udf)</span>
<span class="nc bnc" id="L98" title="All 4 branches missed.">      case binaryComp@BinaryComparison(left, right) =&gt;</span>
<span class="nc" id="L99">        val leftExpr = sparkExprToGTExpr(left)</span>
<span class="nc" id="L100">        val rightExpr = sparkExprToGTExpr(right)</span>
<span class="nc bnc" id="L101" title="All 4 branches missed.">        if (leftExpr.isEmpty || rightExpr.isEmpty) {</span>
<span class="nc" id="L102">          None</span>
        } else {
<span class="nc" id="L104">          binaryComp match {</span>
<span class="nc bnc" id="L105" title="All 2 branches missed.">            case _: EqualTo            =&gt; Some(ff.equals(leftExpr.get, rightExpr.get))</span>
<span class="nc bnc" id="L106" title="All 2 branches missed.">            case _: LessThan           =&gt; Some(ff.less(leftExpr.get, rightExpr.get))</span>
<span class="nc bnc" id="L107" title="All 2 branches missed.">            case _: LessThanOrEqual    =&gt; Some(ff.lessOrEqual(leftExpr.get, rightExpr.get))</span>
<span class="nc bnc" id="L108" title="All 2 branches missed.">            case _: GreaterThan        =&gt; Some(ff.greater(leftExpr.get, rightExpr.get))</span>
<span class="nc bnc" id="L109" title="All 2 branches missed.">            case _: GreaterThanOrEqual =&gt; Some(ff.greaterOrEqual(leftExpr.get, rightExpr.get))</span>
<span class="nc" id="L110">            case _ =&gt; None</span>
          }
        }
<span class="nc bnc" id="L113" title="All 2 branches missed.">      case unary: UnaryExpression =&gt;</span>
<span class="nc" id="L114">        val sparkExpr = unary.child</span>
<span class="nc" id="L115">        val gtExpr = sparkExprToGTExpr(sparkExpr)</span>
<span class="nc bnc" id="L116" title="All 2 branches missed.">        if (gtExpr.isEmpty)</span>
<span class="nc" id="L117">          None</span>
        else {
<span class="nc" id="L119">          unary match {</span>
<span class="nc bnc" id="L120" title="All 2 branches missed.">            case _: IsNotNull =&gt; Some(ff.not(ff.isNull(gtExpr.get)))</span>
<span class="nc bnc" id="L121" title="All 2 branches missed.">            case _: IsNull =&gt; Some(ff.isNull(gtExpr.get))</span>
<span class="nc" id="L122">            case _ =&gt; None</span>
          }
        }
      case _ =&gt;
<span class="nc bnc" id="L126" title="All 4 branches missed.">        if (haveSedona &amp;&amp; expr.isInstanceOf[Sedona_ST_Predicate]) {</span>
<span class="nc" id="L127">          sedonaExprToGTFilter(expr.asInstanceOf[Sedona_ST_Predicate])</span>
        } else {
<span class="nc bnc" id="L129" title="All 2 branches missed.">          logger.debug(s&quot;Got expr: $expr.  Don't know how to turn this into a GeoTools Expression.&quot;)</span>
<span class="nc" id="L130">          None</span>
        }
    }
  }

<span class="nc" id="L135">  def sparkExprToGTExpr(expression: Expression): Option[GTExpression] = expression match {</span>
<span class="nc bnc" id="L136" title="All 2 branches missed.">    case g: GeometryLiteral =&gt;</span>
<span class="nc" id="L137">      Some(ff.literal(g.geom))</span>

<span class="nc bnc" id="L139" title="All 8 branches missed.">    case a: AttributeReference if a.name != &quot;__fid__&quot; =&gt;</span>
<span class="nc" id="L140">      Some(ff.property(a.name))</span>

<span class="nc bnc" id="L142" title="All 2 branches missed.">    case c: Cast =&gt;</span>
<span class="nc bnc" id="L143" title="All 2 branches missed.">      lazy val zone = c.timeZoneId.map(ZoneId.of).orNull</span>
<span class="nc" id="L144">      sparkExprToGTExpr(c.child).map {</span>
<span class="nc bnc" id="L145" title="All 6 branches missed.">        case lit: GTLiteral if lit.getValue.isInstanceOf[Date] &amp;&amp; zone != null =&gt;</span>
<span class="nc" id="L146">          val date = LocalDateTime.ofInstant(lit.getValue.asInstanceOf[Date].toInstant, zone)</span>
<span class="nc" id="L147">          ff.literal(new Date(date.atZone(ZoneOffset.UTC).toInstant.toEpochMilli))</span>
<span class="nc" id="L148">        case e =&gt; e</span>
      }

<span class="nc bnc" id="L151" title="All 8 branches missed.">    case lit: Literal if lit.dataType == DataTypes.StringType =&gt;</span>
      // the actual class is org.apache.spark.unsafe.types.UTF8String, we need to make it
      // a normal string so that geotools can handle it
<span class="nc" id="L154">      Some(ff.literal(Option(lit.value).map(_.toString).orNull))</span>

<span class="nc bnc" id="L156" title="All 8 branches missed.">    case lit: Literal if lit.dataType == DataTypes.TimestampType =&gt;</span>
      // timestamps are defined as microseconds
<span class="nc" id="L158">      Some(ff.literal(new Date(lit.value.asInstanceOf[Long] / 1000)))</span>

<span class="nc bnc" id="L160" title="All 6 branches missed.">    case lit: Literal if haveSedona &amp;&amp; lit.dataType.isInstanceOf[Sedona_GeometryUDT] =&gt;</span>
<span class="nc" id="L161">      Some(ff.literal(lit.dataType.asInstanceOf[Sedona_GeometryUDT].deserialize(lit.value)))</span>

<span class="nc bnc" id="L163" title="All 2 branches missed.">    case lit: Literal =&gt;</span>
<span class="nc" id="L164">      Some(ff.literal(lit.value))</span>

    case _ =&gt;
<span class="nc bnc" id="L167" title="All 2 branches missed.">      logger.debug(s&quot;Can't turn expression into geotools: $expression&quot;)</span>
<span class="nc" id="L168">      None</span>
  }

  // new optimizations rules
<span class="nc" id="L172">  object SpatialOptimizationsRule extends Rule[LogicalPlan] with PredicateHelper {</span>

<span class="nc" id="L174">    def extractGeometry(e: org.apache.spark.sql.catalyst.expressions.Expression): Option[Geometry] = e match {</span>
<span class="nc bnc" id="L175" title="All 2 branches missed.">      case GeometryLiteral(_, geom) =&gt; Some(geom)</span>
<span class="nc bnc" id="L176" title="All 2 branches missed.">      case And(l, r) =&gt; extractGeometry(l).orElse(extractGeometry(r))</span>
<span class="nc bnc" id="L177" title="All 6 branches missed.">      case u: ScalaUDF =&gt; u.children.collectFirst { case GeometryLiteral(_, geom) =&gt; geom }</span>
<span class="nc" id="L178">      case _ =&gt; None</span>
    }

    private def extractGridId(envelopes: List[Envelope],
                              e: org.apache.spark.sql.catalyst.expressions.Expression): Option[List[Int]] =
<span class="nc" id="L183">      extractGeometry(e).map(RelationUtils.gridIdMapper(_, envelopes))</span>

    // Replace the relation in a join with a GeoMesaJoin Relation
    private def alterJoin(join: Join): LogicalPlan = {
<span class="nc" id="L187">      val isSpatialUDF = join.condition.exists {</span>
<span class="nc bnc" id="L188" title="All 4 branches missed.">        case u: ScalaUDF if u.function.isInstanceOf[(Geometry, Geometry) =&gt; java.lang.Boolean] =&gt;</span>
<span class="nc bnc" id="L189" title="All 4 branches missed.">          u.children.head.isInstanceOf[AttributeReference] &amp;&amp; u.children(1).isInstanceOf[AttributeReference]</span>
<span class="nc" id="L190">        case _ =&gt; false</span>
      }

<span class="nc bnc" id="L193" title="All 4 branches missed.">      (join.left, join.right) match {</span>
<span class="nc bnc" id="L194" title="All 6 branches missed.">        case (left: LogicalRelation, right: LogicalRelation) if isSpatialUDF =&gt;</span>
<span class="nc bnc" id="L195" title="All 2 branches missed.">          (left.relation, right.relation) match {</span>
<span class="nc bnc" id="L196" title="All 4 branches missed.">            case (leftRel: GeoMesaRelation, rightRel: GeoMesaRelation) =&gt;</span>
<span class="nc" id="L197">              leftRel.join(rightRel, join.condition.get) match {</span>
<span class="nc bnc" id="L198" title="All 2 branches missed.">                case None =&gt; join</span>
<span class="nc bnc" id="L199" title="All 2 branches missed.">                case Some(joinRelation) =&gt;</span>
<span class="nc" id="L200">                  val newLogicalRelLeft = SparkVersions.copy(left)(output = left.output ++ right.output, relation = joinRelation)</span>
<span class="nc" id="L201">                  SparkVersions.copy(join)(left = newLogicalRelLeft)</span>
              }

<span class="nc" id="L204">            case _ =&gt; join</span>
          }

<span class="nc bnc" id="L207" title="All 4 branches missed.">        case (leftProject @ Project(leftProjectList, left: LogicalRelation),</span>
<span class="nc bnc" id="L208" title="All 6 branches missed.">            Project(rightProjectList, right: LogicalRelation)) if isSpatialUDF =&gt;</span>
<span class="nc bnc" id="L209" title="All 2 branches missed.">          (left.relation, right.relation) match {</span>
<span class="nc bnc" id="L210" title="All 4 branches missed.">            case (leftRel: GeoMesaRelation, rightRel: GeoMesaRelation) =&gt;</span>
<span class="nc" id="L211">              leftRel.join(rightRel, join.condition.get) match {</span>
<span class="nc bnc" id="L212" title="All 2 branches missed.">                case None =&gt; join</span>
<span class="nc bnc" id="L213" title="All 2 branches missed.">                case Some(joinRelation) =&gt;</span>
<span class="nc" id="L214">                  val newLogicalRelLeft = SparkVersions.copy(left)(output = left.output ++ right.output, relation = joinRelation)</span>
<span class="nc" id="L215">                  val newProjectLeft = leftProject.copy(projectList = leftProjectList ++ rightProjectList, child = newLogicalRelLeft)</span>
<span class="nc" id="L216">                  SparkVersions.copy(join)(left = newProjectLeft)</span>
              }

<span class="nc" id="L219">            case _ =&gt; join</span>
          }

<span class="nc" id="L222">        case _ =&gt; join</span>
      }
    }

    override def apply(plan: LogicalPlan): LogicalPlan = {
<span class="nc bnc" id="L227" title="All 2 branches missed.">      logger.debug(s&quot;Optimizer sees $plan&quot;)</span>

      // NOTE: The number of arguments in Aggregate constructor is 3 on community spark and 4 on DataBricks
      // so we cannot use pattern matching to unapply the constructor also
      // need to use reflection to safely create new Aggregate instance
      val optimizeAggregate: PartialFunction[LogicalPlan, LogicalPlan] =
<span class="nc" id="L233">        Function.unlift { plan: LogicalPlan =&gt;</span>
<span class="nc" id="L234">          plan match {</span>
<span class="nc bnc" id="L235" title="All 2 branches missed.">            case agg: Aggregate =&gt;</span>
<span class="nc" id="L236">              agg.child match {</span>
<span class="nc bnc" id="L237" title="All 4 branches missed.">                case Project(projectList, join: Join) =&gt;</span>
<span class="nc" id="L238">                  val alteredJoin = SpatialOptimizationsRule.alterJoin(join)</span>
<span class="nc" id="L239">                  Some(Aggregates.instance(agg.groupingExpressions, agg.aggregateExpressions, Project(projectList, alteredJoin), None))</span>
<span class="nc bnc" id="L240" title="All 2 branches missed.">                case join: Join =&gt;</span>
<span class="nc" id="L241">                  val alteredJoin = SpatialOptimizationsRule.alterJoin(join)</span>
<span class="nc" id="L242">                  Some(Aggregates.instance(agg.groupingExpressions, agg.aggregateExpressions, alteredJoin, None))</span>
<span class="nc" id="L243">                case _ =&gt; None</span>
              }
<span class="nc" id="L245">            case _ =&gt; None</span>
          }
        }

<span class="nc" id="L249">      val optimizeRest: PartialFunction[LogicalPlan, LogicalPlan] = {</span>
<span class="nc bnc" id="L250" title="All 4 branches missed.">        case join: Join =&gt;</span>
<span class="nc" id="L251">          alterJoin(join)</span>
<span class="nc bnc" id="L252" title="All 4 branches missed.">        case sort @ Sort(_, _, _) =&gt; sort    // No-op.  Just realizing what we can do:)</span>
<span class="nc bnc" id="L253" title="All 12 branches missed.">        case filt @ Filter(f, lr: LogicalRelation) if lr.relation.isInstanceOf[GeoMesaRelation] =&gt;</span>
          // TODO: deal with `or`

<span class="nc" id="L256">          val gmRel = lr.relation.asInstanceOf[GeoMesaRelation]</span>
          // split up conjunctive predicates and extract the st_contains variable
<span class="nc" id="L258">          val sparkFilters: Seq[Expression] =  splitConjunctivePredicates(f)</span>

<span class="nc bnc" id="L260" title="All 10 branches missed.">          val (gtFilters: Seq[GTFilter], sFilters: Seq[Expression]) = sparkFilters.foldLeft((Seq[GTFilter](), Seq[Expression]())) {</span>
<span class="nc bnc" id="L261" title="All 4 branches missed.">            case ((gts: Seq[GTFilter], sfilters), expression: Expression) =&gt;</span>
<span class="nc" id="L262">              sparkFilterToGTFilter(expression) match {</span>
<span class="nc bnc" id="L263" title="All 2 branches missed.">                case Some(gtf) =&gt; (gts.+:(gtf), sfilters)</span>
<span class="nc bnc" id="L264" title="All 2 branches missed.">                case None      =&gt; (gts,         sfilters.+:(expression))</span>
              }
          }

<span class="nc bnc" id="L268" title="All 2 branches missed.">          if (gtFilters.nonEmpty) {</span>
            // if we have a partitioned cache, exclude partitions that don't match the query filter
<span class="nc" id="L270">            val partitioned = gmRel.cached.map {</span>
<span class="nc bnc" id="L271" title="All 2 branches missed.">              case c: PartitionedIndexedRDD =&gt;</span>
<span class="nc" id="L272">                val hints = sparkFilters.flatMap(extractGridId(c.envelopes, _)).flatten</span>
<span class="nc bnc" id="L273" title="All 2 branches missed.">                if (hints.isEmpty) { c } else {</span>
<span class="nc bnc" id="L274" title="All 2 branches missed.">                  c.copy(rdd = c.rdd.filter { case (key, _) =&gt; hints.contains(key) })</span>
                }

<span class="nc" id="L277">              case c =&gt; c</span>
            }
<span class="nc" id="L279">            val filt = FilterHelper.filterListAsAnd(gmRel.filter.toSeq ++ gtFilters)</span>
<span class="nc" id="L280">            val relation = gmRel.copy(filter = filt, cached = partitioned)</span>
<span class="nc" id="L281">            val newrel = SparkVersions.copy(lr)(output = lr.output, relation = relation)</span>
<span class="nc bnc" id="L282" title="All 2 branches missed.">            if (sFilters.nonEmpty) {</span>
              // Keep filters that couldn't be transformed at the top level
<span class="nc" id="L284">              Filter(sFilters.reduce(And), newrel)</span>
            } else {
              // if all filters could be transformed to GeoTools filters, just return the new relation
<span class="nc" id="L287">              newrel</span>
            }
          } else {
<span class="nc" id="L290">            filt</span>
          }
      }
<span class="nc" id="L293">      plan.transform(optimizeAggregate orElse optimizeRest)</span>
    }

  }

  // A catch for when we are able to precompute the join using the sweepline algorithm.
  // Skips doing a full cartesian product with catalyst.
<span class="nc" id="L300">  object SpatialJoinStrategy extends Strategy {</span>

    import org.apache.spark.sql.catalyst.plans.logical._

    def alterJoin(logicalPlan: Join): Seq[SparkPlan] = {
<span class="nc" id="L305">      logicalPlan.left match {</span>
<span class="nc bnc" id="L306" title="All 6 branches missed.">        case Project(projectList, lr: LogicalRelation) if lr.relation.isInstanceOf[GeoMesaJoinRelation] =&gt;</span>
<span class="nc" id="L307">           ProjectExec(projectList, planLater(lr)) :: Nil</span>

<span class="nc bnc" id="L309" title="All 4 branches missed.">        case lr: LogicalRelation if lr.relation.isInstanceOf[GeoMesaJoinRelation] =&gt; planLater(lr) :: Nil</span>

<span class="nc" id="L311">        case _ =&gt; Nil</span>
      }
    }

<span class="nc" id="L315">    override def apply(plan: LogicalPlan): Seq[SparkPlan] = plan match {</span>
      //TODO: handle other kinds of joins
<span class="nc bnc" id="L317" title="All 4 branches missed.">      case Project(_, logicalPlan: Join) =&gt; alterJoin(logicalPlan)</span>
<span class="nc bnc" id="L318" title="All 2 branches missed.">      case join: Join =&gt; alterJoin(join)</span>
<span class="nc" id="L319">      case _ =&gt; Nil</span>
    }
  }

  def registerOptimizations(sqlContext: SQLContext): Unit = {

<span class="nc" id="L325">    Seq(SpatialOptimizationsRule).foreach { r =&gt;</span>
<span class="nc bnc" id="L326" title="All 2 branches missed.">      if(!sqlContext.experimental.extraOptimizations.contains(r))</span>
<span class="nc" id="L327">        sqlContext.experimental.extraOptimizations ++= Seq(r)</span>
    }

<span class="nc" id="L330">    Seq(SpatialJoinStrategy).foreach { s =&gt;</span>
<span class="nc bnc" id="L331" title="All 2 branches missed.">      if(!sqlContext.experimental.extraStrategies.contains(s))</span>
<span class="nc" id="L332">        sqlContext.experimental.extraStrategies ++= Seq(s)</span>
    }
  }
}
</pre><div class="footer"><span class="right">Created with <a href="http://www.jacoco.org/jacoco">JaCoCo</a> 0.8.14.202510111229</span></div></body></html>