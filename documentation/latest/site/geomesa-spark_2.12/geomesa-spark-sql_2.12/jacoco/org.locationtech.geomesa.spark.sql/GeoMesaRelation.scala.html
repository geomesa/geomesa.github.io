<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml" lang=""><head><meta http-equiv="Content-Type" content="text/html;charset=UTF-8"/><link rel="stylesheet" href="../jacoco-resources/report.css" type="text/css"/><link rel="shortcut icon" href="../jacoco-resources/report.gif" type="image/gif"/><title>GeoMesaRelation.scala</title><link rel="stylesheet" href="../jacoco-resources/prettify.css" type="text/css"/><script type="text/javascript" src="../jacoco-resources/prettify.js"></script></head><body onload="window['PR_TAB_WIDTH']=4;prettyPrint()"><div class="breadcrumb" id="breadcrumb"><span class="info"><a href="../jacoco-sessions.html" class="el_session">Sessions</a></span><a href="../index.html" class="el_report">GeoMesa Spark SQL</a> &gt; <a href="index.source.html" class="el_package">org.locationtech.geomesa.spark.sql</a> &gt; <span class="el_source">GeoMesaRelation.scala</span></div><h1>GeoMesaRelation.scala</h1><pre class="source lang-java linenums">/***********************************************************************
 * Copyright (c) 2013-2025 General Atomics Integrated Intelligence, Inc.
 * All rights reserved. This program and the accompanying materials
 * are made available under the terms of the Apache License, Version 2.0
 * which accompanies this distribution and is available at
 * https://www.apache.org/licenses/LICENSE-2.0
 ***********************************************************************/

package org.locationtech.geomesa.spark.sql

import com.typesafe.scalalogging.LazyLogging
import org.apache.hadoop.conf.Configuration
import org.apache.spark.rdd.RDD
import org.apache.spark.sql.catalyst.expressions.Expression
import org.apache.spark.sql.sources._
import org.apache.spark.sql.types.StructType
import org.apache.spark.sql.{Row, SQLContext}
import org.apache.spark.storage.StorageLevel
import org.geotools.api.data.{DataStoreFinder, Query, Transaction}
import org.geotools.api.feature.simple.{SimpleFeature, SimpleFeatureType}
import org.geotools.filter.text.ecql.ECQL
import org.locationtech.geomesa.filter.FilterHelper
import org.locationtech.geomesa.memory.cqengine.datastore.GeoCQEngineDataStore
import org.locationtech.geomesa.spark.jts.util.WKTUtils
import org.locationtech.geomesa.spark.sql.GeoMesaRelation.{CachedRDD, IndexedRDD, PartitionedIndexedRDD, PartitionedRDD}
import org.locationtech.geomesa.spark.sql.GeoMesaSparkSQL.GEOMESA_SQL_FEATURE
import org.locationtech.geomesa.spark.{GeoMesaSpark, SpatialRDD}
import org.locationtech.geomesa.utils.collection.CloseableIterator
import org.locationtech.geomesa.utils.geotools.{FeatureUtils, SimpleFeatureTypes}
import org.locationtech.geomesa.utils.io.WithClose
import org.locationtech.jts.geom.Envelope

import java.util.{Collections, Locale}
import scala.util.control.NonFatal

/**
  * The Spark Relation that builds the scan over the GeoMesa table
  *
  * @param sqlContext spark sql context
  * @param sft simple feature type associated with the rows in the relation
  * @param schema spark sql schema (must correspond to the sft)
  * @param params user parameters, generally for configured the underlying data store and/or caching/partitioning
  * @param filter a push-down geotools filter applied to the relation
  * @param cached an optional cached RDD, used to speed up queries when enabled
  * @param partitioned an optional spatially partitioned RDD, used to speed up spatial joins when enabled
  */
@SerialVersionUID(5903335864840136924L)
<span class="nc bnc" id="L48" title="All 64 branches missed.">case class GeoMesaRelation(</span>
<span class="nc" id="L49">    sqlContext: SQLContext,</span>
<span class="nc" id="L50">    sft: SimpleFeatureType,</span>
<span class="nc" id="L51">    schema: StructType,</span>
<span class="nc" id="L52">    params: Map[String, String],</span>
<span class="nc" id="L53">    filter: Option[org.geotools.api.filter.Filter],</span>
<span class="nc" id="L54">    cached: Option[CachedRDD],</span>
<span class="nc" id="L55">    partitioned: Option[PartitionedRDD]</span>
<span class="nc" id="L56">  ) extends BaseRelation with PrunedFilteredScan with LazyLogging {</span>

  import scala.collection.JavaConverters._

  /**
    * Attempts to do an optimized join between two relations.
    *
    * Currently this method uses grid partitioning on both relations so that the join comparisons
    * only need to be applied on each pair of partitions, instead of globally. This only works
    * if both relations have already been grid partitioned.
    *
    * @param other relation to join
    * @param condition join condition
    * @return an optimized join, if possible to do so
    */
  def join(other: GeoMesaRelation, condition: Expression): Option[GeoMesaJoinRelation] = {
<span class="nc" id="L72">    val opt = for { p &lt;- partitioned; o &lt;- other.partitioned } yield {</span>
<span class="nc bnc" id="L73" title="All 6 branches missed.">      val toJoin = if (p.envelopes == o.envelopes) {</span>
<span class="nc" id="L74">        Some(other)</span>
<span class="nc bnc" id="L75" title="All 2 branches missed.">      } else if (p.cover) {</span>
<span class="nc" id="L76">        val repartitioned: SpatialRDD = p.partitions match {</span>
<span class="nc bnc" id="L77" title="All 2 branches missed.">          case None =&gt; p.raw</span>
<span class="nc bnc" id="L78" title="All 2 branches missed.">          case Some(partitions) =&gt; SpatialRDD(p.raw.repartition(partitions), p.raw.schema)</span>
        }
<span class="nc" id="L80">        val parallelism = p.partitions.getOrElse(sqlContext.sparkContext.defaultParallelism)</span>
<span class="nc" id="L81">        val rdd = RelationUtils.grid(repartitioned, p.envelopes, parallelism)</span>
<span class="nc" id="L82">        val partitioned = Some(PartitionedRDD(rdd, p.raw, p.envelopes, p.partitions, p.cover))</span>
<span class="nc" id="L83">        Some(other.copy(partitioned = partitioned))</span>
      } else {
<span class="nc bnc" id="L85" title="All 2 branches missed.">        logger.warn(&quot;Joining across two relations that are not partitioned by the same scheme - unable to optimize&quot;)</span>
<span class="nc" id="L86">        None</span>
      }
<span class="nc" id="L88">      toJoin.map { rel =&gt;</span>
<span class="nc" id="L89">        GeoMesaJoinRelation(sqlContext, this, rel, StructType(schema.fields ++ rel.schema.fields), condition)</span>
      }
    }
<span class="nc" id="L92">    opt.flatten</span>
  }

  override def buildScan(
      requiredColumns: Array[String],
      filters: Array[org.apache.spark.sql.sources.Filter]): RDD[Row] = {
<span class="nc bnc" id="L98" title="All 2 branches missed.">    lazy val debug =</span>
      s&quot;filt = $filter, filters = ${filters.mkString(&quot;,&quot;)}, requiredColumns = ${requiredColumns.mkString(&quot;,&quot;)}&quot;

    val filt = {
<span class="nc" id="L102">      val sum = Seq.newBuilder[org.geotools.api.filter.Filter]</span>
<span class="nc" id="L103">      filter.foreach(sum += _)</span>
<span class="nc" id="L104">      filters.foreach(f =&gt; SparkUtils.sparkFilterToCQLFilter(f).foreach(sum += _))</span>
<span class="nc" id="L105">      FilterHelper.filterListAsAnd(sum.result).getOrElse(org.geotools.api.filter.Filter.INCLUDE)</span>
    }
<span class="nc bnc" id="L107" title="All 6 branches missed.">    val requiredAttributes = requiredColumns.filterNot(_ == &quot;__fid__&quot;)</span>

    // avoid closures on complex objects
<span class="nc" id="L110">    val schema = this.schema // note: referencing case class members evidently serializes the whole class??</span>
<span class="nc" id="L111">    val typeName = sft.getTypeName</span>

<span class="nc" id="L113">    val result: RDD[SimpleFeature] = cached match {</span>
<span class="nc bnc" id="L114" title="All 2 branches missed.">      case None =&gt;</span>
<span class="nc bnc" id="L115" title="All 2 branches missed.">        logger.debug(s&quot;Building scan, $debug&quot;)</span>
<span class="nc" id="L116">        val conf = new Configuration(sqlContext.sparkContext.hadoopConfiguration)</span>
<span class="nc" id="L117">        val query = new Query(typeName, filt, requiredAttributes: _*)</span>
<span class="nc" id="L118">        GeoMesaSpark(params.asJava).rdd(conf, sqlContext.sparkContext, params, query)</span>

<span class="nc bnc" id="L120" title="All 4 branches missed.">      case Some(IndexedRDD(rdd)) =&gt;</span>
<span class="nc bnc" id="L121" title="All 2 branches missed.">        logger.debug(s&quot;Building in-memory scan, $debug&quot;)</span>
<span class="nc" id="L122">        val cql = ECQL.toCQL(filt)</span>
<span class="nc" id="L123">        rdd.flatMap { engine =&gt;</span>
<span class="nc" id="L124">          val query = new Query(typeName, ECQL.toFilter(cql), requiredAttributes: _*)</span>
          // note: this isn't getting closed, but since it's cq-engine it doesn't need to be
<span class="nc" id="L126">          CloseableIterator(engine.getFeatureReader(query, Transaction.AUTO_COMMIT))</span>
        }

<span class="nc bnc" id="L129" title="All 4 branches missed.">      case Some(PartitionedIndexedRDD(rdd, _)) =&gt;</span>
<span class="nc bnc" id="L130" title="All 2 branches missed.">        logger.debug(s&quot;Building partitioned in-memory scan, $debug&quot;)</span>
<span class="nc" id="L131">        val cql = ECQL.toCQL(filt)</span>
<span class="nc bnc" id="L132" title="All 2 branches missed.">        rdd.flatMap { case (_, engine) =&gt;</span>
<span class="nc" id="L133">          val query = new Query(typeName, ECQL.toFilter(cql), requiredAttributes: _*)</span>
          // note: this isn't getting closed, but since it's cq-engine it doesn't need to be
<span class="nc" id="L135">          CloseableIterator(engine.getFeatureReader(query, Transaction.AUTO_COMMIT))</span>
        }
    }

<span class="nc" id="L139">    val extractors = SparkUtils.getExtractors(requiredColumns, schema)</span>

<span class="nc" id="L141">    result.map(SparkUtils.sf2row(schema, _, extractors))</span>
  }

  override def unhandledFilters(filters: Array[Filter]): Array[Filter] = {
<span class="nc" id="L145">    filters.filter {</span>
<span class="nc bnc" id="L146" title="All 6 branches missed.">      case _ @ (_:IsNotNull | _:IsNull) =&gt; true</span>
<span class="nc" id="L147">      case _ =&gt; false</span>
    }
  }
}

<span class="nc bnc" id="L152" title="All 4 branches missed.">object GeoMesaRelation extends LazyLogging {</span>

  import scala.collection.JavaConverters._

  /**
    * Create a new relation based on the input parameters
    *
    * @param sqlContext sql context
    * @param params parameters
    * @return
    */
  def apply(sqlContext: SQLContext, params: Map[String, String]): GeoMesaRelation = {
<span class="nc" id="L164">    val name = params.getOrElse(GEOMESA_SQL_FEATURE,</span>
<span class="nc" id="L165">      throw new IllegalArgumentException(s&quot;Feature type must be specified with '$GEOMESA_SQL_FEATURE'&quot;))</span>
<span class="nc" id="L166">    val sft = GeoMesaSpark(params.asJava).sft(params, name).getOrElse {</span>
<span class="nc" id="L167">      throw new IllegalArgumentException(s&quot;Could not load feature type with name '$name'&quot;)</span>
    }
<span class="nc" id="L169">    apply(sqlContext, params, SparkUtils.createStructType(sft), sft)</span>
  }

  /**
    * Create a new relation based on the input parameters, with the given schema
    *
    * @param sqlContext sql context
    * @param params parameters
    * @param schema schema
    * @return
    */
  def apply(sqlContext: SQLContext, params: Map[String, String], schema: StructType): GeoMesaRelation = {
<span class="nc" id="L181">    val name = params.getOrElse(GEOMESA_SQL_FEATURE,</span>
<span class="nc" id="L182">      throw new IllegalArgumentException(s&quot;Feature type must be specified with '$GEOMESA_SQL_FEATURE'&quot;))</span>
<span class="nc" id="L183">    val sft = GeoMesaSpark(params.asJava).sft(params, name).getOrElse {</span>
<span class="nc" id="L184">      throw new IllegalArgumentException(s&quot;Could not load feature type with name '$name'&quot;)</span>
    }
<span class="nc" id="L186">    apply(sqlContext, params, schema, sft)</span>
  }

  /**
    * Create a new relation based on the input parameters, with the given schema and underlying feature type
    *
    * @param sqlContext sql context
    * @param params parameters
    * @param schema schema
    * @param sft simple feature type
    * @return
    */
  def apply(
      sqlContext: SQLContext,
      params: Map[String, String],
      schema: StructType,
      sft: SimpleFeatureType): GeoMesaRelation = {

<span class="nc bnc" id="L204" title="All 2 branches missed.">    logger.trace(s&quot;Creating GeoMesaRelation with sft: $sft&quot;)</span>

    def get[T](key: String, transform: String =&gt; T, default: =&gt; T): T = {
<span class="nc" id="L207">      params.get(key) match {</span>
<span class="nc bnc" id="L208" title="All 2 branches missed.">        case None =&gt; default</span>
<span class="nc bnc" id="L209" title="All 2 branches missed.">        case Some(v) =&gt;</span>
<span class="nc" id="L210">          try { transform(v) } catch {</span>
<span class="nc bnc" id="L211" title="All 4 branches missed.">            case NonFatal(e) =&gt; logger.error(s&quot;Error evaluating param '$key' with value '$v':&quot;, e); default</span>
          }
      }
    }

    def rawRDD: SpatialRDD = {
<span class="nc" id="L217">      val query = new Query(sft.getTypeName, ECQL.toFilter(params.getOrElse(&quot;query&quot;, &quot;INCLUDE&quot;)))</span>
<span class="nc" id="L218">      GeoMesaSpark(params.asJava).rdd(new Configuration(), sqlContext.sparkContext, params, query)</span>
    }

<span class="nc bnc" id="L221" title="All 2 branches missed.">    val partitioned = if (!get[Boolean](&quot;spatial&quot;, _.toBoolean, false)) { None } else {</span>
<span class="nc" id="L222">      val raw = rawRDD</span>
<span class="nc" id="L223">      val bounds: Envelope = params.get(&quot;bounds&quot;) match {</span>
<span class="nc bnc" id="L224" title="All 2 branches missed.">        case None =&gt; RelationUtils.getBound(raw)</span>
<span class="nc bnc" id="L225" title="All 2 branches missed.">        case Some(b) =&gt;</span>
<span class="nc" id="L226">          try { WKTUtils.read(b).getEnvelopeInternal } catch {</span>
            case NonFatal(e) =&gt; throw new IllegalArgumentException(s&quot;Error reading provided bounds '$b':&quot;, e)
          }
      }

<span class="nc bnc" id="L231" title="All 2 branches missed.">      val partitions = Option(get[Int](&quot;partitions&quot;, _.toInt, -1)).filter(_ &gt; 0)</span>
<span class="nc" id="L232">      val parallelism = partitions.getOrElse(sqlContext.sparkContext.defaultParallelism)</span>
      // control partitioning strategies that require a sample of the data
<span class="nc bnc" id="L234" title="All 2 branches missed.">      lazy val sampleSize = get[Int](&quot;sampleSize&quot;, _.toInt, 100)</span>
<span class="nc bnc" id="L235" title="All 2 branches missed.">      lazy val threshold = get[Double](&quot;threshold&quot;, _.toDouble, 0.3)</span>

<span class="nc" id="L237">      val envelopes = params.getOrElse(&quot;strategy&quot;, &quot;equal&quot;).toLowerCase(Locale.US) match {</span>
<span class="nc bnc" id="L238" title="All 2 branches missed.">        case &quot;equal&quot;    =&gt; RelationUtils.equalPartitioning(bounds, parallelism)</span>
<span class="nc bnc" id="L239" title="All 2 branches missed.">        case &quot;earth&quot;    =&gt; RelationUtils.wholeEarthPartitioning(parallelism)</span>
<span class="nc bnc" id="L240" title="All 2 branches missed.">        case &quot;weighted&quot; =&gt; RelationUtils.weightedPartitioning(raw, bounds, parallelism, sampleSize)</span>
<span class="nc bnc" id="L241" title="All 2 branches missed.">        case &quot;rtree&quot;    =&gt; RelationUtils.rtreePartitioning(raw, parallelism, sampleSize, threshold)</span>
<span class="nc" id="L242">        case s =&gt; throw new IllegalArgumentException(s&quot;Invalid partitioning strategy: $s&quot;)</span>
      }

<span class="nc" id="L245">      val rdd = RelationUtils.grid(raw, envelopes, parallelism)</span>
<span class="nc" id="L246">      rdd.persist(StorageLevel.MEMORY_ONLY)</span>
<span class="nc" id="L247">      Some(PartitionedRDD(rdd, raw, envelopes, partitions, get[Boolean](&quot;cover&quot;, _.toBoolean, false)))</span>
    }

<span class="nc bnc" id="L250" title="All 2 branches missed.">    val cached = if (!get[Boolean](&quot;cache&quot;, _.toBoolean, false)) { None } else {</span>
<span class="nc" id="L251">      val check = Collections.singletonMap[String, java.io.Serializable](&quot;cqengine&quot;, &quot;true&quot;)</span>
<span class="nc bnc" id="L252" title="All 2 branches missed.">      if (!DataStoreFinder.getAvailableDataStores.asScala.exists(_.canProcess(check))) {</span>
<span class="nc" id="L253">        throw new IllegalArgumentException(&quot;Caching requires the GeoCQEngineDataStore to be available on the classpath&quot;)</span>
      }

      // avoid closure on full sft
<span class="nc" id="L257">      val typeName = sft.getTypeName</span>
<span class="nc" id="L258">      val encodedSft = SimpleFeatureTypes.encodeType(sft, includeUserData = true)</span>

<span class="nc" id="L260">      val indexGeom = get[Boolean](&quot;indexGeom&quot;, _.toBoolean, false)</span>

<span class="nc" id="L262">      partitioned match {</span>
<span class="nc bnc" id="L263" title="All 2 branches missed.">        case Some(p) =&gt;</span>
<span class="nc" id="L264">          val rdd = p.rdd.mapValues { iter =&gt;</span>
<span class="nc" id="L265">            val engine = new GeoCQEngineDataStore(indexGeom)</span>
<span class="nc" id="L266">            engine.createSchema(SimpleFeatureTypes.createType(typeName, encodedSft))</span>
<span class="nc" id="L267">            WithClose(engine.getFeatureWriterAppend(typeName, Transaction.AUTO_COMMIT)) { writer =&gt;</span>
<span class="nc" id="L268">              iter.foreach(FeatureUtils.write(writer, _, useProvidedFid = true))</span>
            }
<span class="nc" id="L270">            engine</span>
          }
<span class="nc" id="L272">          p.rdd.unpersist() // make this call blocking?</span>
<span class="nc" id="L273">          rdd.persist(StorageLevel.MEMORY_ONLY)</span>
<span class="nc" id="L274">          Some(PartitionedIndexedRDD(rdd, p.envelopes))</span>

<span class="nc bnc" id="L276" title="All 2 branches missed.">        case None =&gt;</span>
<span class="nc" id="L277">          val rdd = rawRDD.mapPartitions { iter =&gt;</span>
<span class="nc" id="L278">            val engine = new GeoCQEngineDataStore(indexGeom)</span>
<span class="nc" id="L279">            engine.createSchema(SimpleFeatureTypes.createType(typeName, encodedSft))</span>
<span class="nc" id="L280">            WithClose(engine.getFeatureWriterAppend(typeName, Transaction.AUTO_COMMIT)) { writer =&gt;</span>
<span class="nc" id="L281">              iter.foreach(FeatureUtils.write(writer, _, useProvidedFid = true))</span>
            }
<span class="nc" id="L283">            Iterator.single(engine)</span>
          }
<span class="nc" id="L285">          rdd.persist(StorageLevel.MEMORY_ONLY)</span>
<span class="nc" id="L286">          Some(IndexedRDD(rdd))</span>
      }
    }
   
<span class="nc" id="L290">   val filter = Option(ECQL.toFilter(params.getOrElse(&quot;query&quot;, &quot;INCLUDE&quot;)))</span>

<span class="nc" id="L292">    GeoMesaRelation(sqlContext, sft, schema, params, filter, cached, partitioned)</span>
  }

  /**
    * Holder for a partitioning scheme
    *
    * @param rdd partitioned rdd
    * @param raw underlying unpartitioned rdd
    * @param envelopes envelopes used in partitioning
    * @param partitions hint for number of partitions
    * @param cover cover partitions or not when joining
    */
<span class="nc bnc" id="L304" title="All 44 branches missed.">  case class PartitionedRDD(</span>
<span class="nc" id="L305">      rdd: RDD[(Int, Iterable[SimpleFeature])],</span>
<span class="nc" id="L306">      raw: SpatialRDD,</span>
<span class="nc" id="L307">      envelopes: List[Envelope],</span>
<span class="nc" id="L308">      partitions: Option[Int],</span>
<span class="nc" id="L309">      cover: Boolean</span>
    )

  /**
    * Trait for cached RDDs used to accelerate scans
    */
  sealed trait CachedRDD

  /**
    * An RDD where each element is a spatial index containing multiple features
    *
    * @param rdd indexed features
    */
<span class="nc bnc" id="L322" title="All 18 branches missed.">  case class IndexedRDD(rdd: RDD[GeoCQEngineDataStore]) extends CachedRDD</span>

  /**
    * An RDD where each element is a spatial index containing multiple features, partitioned by
    * a spatial grid
    *
    * @param rdd grid cell -&gt; indexed features
    * @param envelopes envelopes corresponding the each grid cell
    */
<span class="nc bnc" id="L331" title="All 25 branches missed.">  case class PartitionedIndexedRDD(rdd: RDD[(Int, GeoCQEngineDataStore)], envelopes: List[Envelope])</span>
<span class="nc" id="L332">      extends CachedRDD</span>
}
</pre><div class="footer"><span class="right">Created with <a href="http://www.jacoco.org/jacoco">JaCoCo</a> 0.8.14.202510111229</span></div></body></html>