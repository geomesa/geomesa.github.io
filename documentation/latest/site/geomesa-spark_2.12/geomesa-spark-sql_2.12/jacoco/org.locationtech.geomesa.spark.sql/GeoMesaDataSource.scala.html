<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml" lang=""><head><meta http-equiv="Content-Type" content="text/html;charset=UTF-8"/><link rel="stylesheet" href="../jacoco-resources/report.css" type="text/css"/><link rel="shortcut icon" href="../jacoco-resources/report.gif" type="image/gif"/><title>GeoMesaDataSource.scala</title><link rel="stylesheet" href="../jacoco-resources/prettify.css" type="text/css"/><script type="text/javascript" src="../jacoco-resources/prettify.js"></script></head><body onload="window['PR_TAB_WIDTH']=4;prettyPrint()"><div class="breadcrumb" id="breadcrumb"><span class="info"><a href="../jacoco-sessions.html" class="el_session">Sessions</a></span><a href="../index.html" class="el_report">GeoMesa Spark SQL</a> &gt; <a href="index.source.html" class="el_package">org.locationtech.geomesa.spark.sql</a> &gt; <span class="el_source">GeoMesaDataSource.scala</span></div><h1>GeoMesaDataSource.scala</h1><pre class="source lang-java linenums">/***********************************************************************
 * Copyright (c) 2013-2025 General Atomics Integrated Intelligence, Inc.
 * All rights reserved. This program and the accompanying materials
 * are made available under the terms of the Apache License, Version 2.0
 * which accompanies this distribution and is available at
 * https://www.apache.org/licenses/LICENSE-2.0
 ***********************************************************************/

package org.locationtech.geomesa.spark.sql

import com.typesafe.scalalogging.LazyLogging
import org.apache.spark.rdd.RDD
import org.apache.spark.sql._
import org.apache.spark.sql.sources._
import org.apache.spark.sql.types.StructType
import org.geotools.api.data.DataStore
import org.geotools.api.feature.simple.{SimpleFeature, SimpleFeatureType}
import org.geotools.util.factory.Hints
import org.locationtech.geomesa.spark.GeoMesaSpark
import org.locationtech.geomesa.spark.sql.GeoMesaSparkSQL._
import org.locationtech.geomesa.utils.geotools.SimpleFeatureTypes
import org.locationtech.geomesa.utils.io.WithStore

// Spark DataSource for GeoMesa
// enables loading a GeoMesa DataFrame as
// {{
// val df = spark.read
//   .format(&quot;geomesa&quot;)
//   .option(GM.instanceIdParam.getName, &quot;mycloud&quot;)
//   .option(GM.userParam.getName, &quot;user&quot;)
//   .option(GM.passwordParam.getName, &quot;password&quot;)
//   .option(GM.tableNameParam.getName, &quot;sparksql&quot;)
//   .option(&quot;geomesa.feature&quot;, &quot;chicago&quot;)
//   .load()
// }}
<span class="nc bnc" id="L36" title="All 4 branches missed.">class GeoMesaDataSource extends DataSourceRegister</span>
    with RelationProvider with SchemaRelationProvider with CreatableRelationProvider with LazyLogging {

  import scala.collection.JavaConverters._

<span class="nc" id="L41">  override def shortName(): String = &quot;geomesa&quot;</span>

  override def createRelation(sqlContext: SQLContext, parameters: Map[String, String]): BaseRelation = {
<span class="nc" id="L44">    SQLTypes.init(sqlContext)</span>
<span class="nc" id="L45">    GeoMesaRelation(sqlContext, parameters)</span>
  }

  // JNH: Q: Why doesn't this method have the call to SQLTypes.init(sqlContext)?
  override def createRelation(
      sqlContext: SQLContext,
      parameters: Map[String, String],
      schema: StructType): BaseRelation = {
<span class="nc" id="L53">    GeoMesaRelation(sqlContext, parameters, schema)</span>
  }

  override def createRelation(
      sqlContext: SQLContext,
      mode: SaveMode,
      parameters: Map[String, String],
      data: DataFrame): BaseRelation = {

<span class="nc" id="L62">    val newFeatureName = parameters(GEOMESA_SQL_FEATURE)</span>
<span class="nc" id="L63">    val rddSft = SparkUtils.createFeatureType(newFeatureName, data.schema)</span>

<span class="nc" id="L65">    val storeSft = WithStore[DataStore](parameters) { ds =&gt;</span>
<span class="nc bnc" id="L66" title="All 2 branches missed.">      if (ds.getTypeNames.contains(newFeatureName)) {</span>
<span class="nc" id="L67">        val existing = ds.getSchema(newFeatureName)</span>
<span class="nc bnc" id="L68" title="All 2 branches missed.">        if (!compatible(existing, rddSft)) {</span>
<span class="nc" id="L69">          throw new IllegalStateException(</span>
<span class="nc" id="L70">            &quot;The dataframe is not compatible with the existing schema in the datastore:&quot; +</span>
<span class="nc" id="L71">              s&quot;\n  Dataframe schema: ${SimpleFeatureTypes.encodeType(rddSft)}&quot; +</span>
<span class="nc" id="L72">              s&quot;\n  Datastore schema: ${SimpleFeatureTypes.encodeType(existing)}&quot;)</span>
        }
<span class="nc" id="L74">        existing</span>
      } else {
<span class="nc" id="L76">        rddSft.getUserData.put(&quot;override.reserved.words&quot;, java.lang.Boolean.TRUE)</span>
<span class="nc" id="L77">        ds.createSchema(rddSft)</span>
<span class="nc" id="L78">        ds.getSchema(rddSft.getTypeName)</span>
      }
    }

<span class="nc bnc" id="L82" title="All 2 branches missed.">    val structType = if (data.queryExecution == null) {</span>
<span class="nc" id="L83">      SparkUtils.createStructType(rddSft)</span>
    } else {
<span class="nc" id="L85">      data.schema</span>
    }

    // we need to pass schema to every worker in a serializable way
<span class="nc" id="L89">    val sftString = SimpleFeatureTypes.encodeType(storeSft, includeUserData = true)</span>
<span class="nc" id="L90">    val typeName = storeSft.getTypeName</span>
<span class="nc" id="L91">    val rddToSave: RDD[SimpleFeature] = data.rdd.mapPartitions { partition =&gt;</span>
<span class="nc" id="L92">      val sft = SimpleFeatureTypes.createType(typeName, sftString)</span>
<span class="nc" id="L93">      val mappings = SparkUtils.rowsToFeatures(sft, structType)</span>
<span class="nc" id="L94">      partition.map { row =&gt;</span>
<span class="nc" id="L95">        val sf = mappings.apply(row)</span>
<span class="nc" id="L96">        sf.getUserData.put(Hints.USE_PROVIDED_FID, java.lang.Boolean.TRUE)</span>
<span class="nc" id="L97">        sf</span>
      }
    }

<span class="nc" id="L101">    GeoMesaSpark(parameters.asJava).save(rddToSave, parameters, newFeatureName)</span>

<span class="nc" id="L103">    GeoMesaRelation(sqlContext, parameters, data.schema, rddSft)</span>
  }

  // are schemas compatible? we're flexible with order, but require the same number, names and types
  private def compatible(sft: SimpleFeatureType, dataframe: SimpleFeatureType): Boolean = {
<span class="nc bnc" id="L108" title="All 4 branches missed.">    sft.getAttributeCount == dataframe.getAttributeCount &amp;&amp; sft.getAttributeDescriptors.asScala.forall { ad =&gt;</span>
<span class="nc" id="L109">      val df = dataframe.getDescriptor(ad.getLocalName)</span>
<span class="nc bnc" id="L110" title="All 4 branches missed.">      df != null &amp;&amp; ad.getType.getBinding.isAssignableFrom(df.getType.getBinding)</span>
    }
  }
}
</pre><div class="footer"><span class="right">Created with <a href="http://www.jacoco.org/jacoco">JaCoCo</a> 0.8.14.202510111229</span></div></body></html>