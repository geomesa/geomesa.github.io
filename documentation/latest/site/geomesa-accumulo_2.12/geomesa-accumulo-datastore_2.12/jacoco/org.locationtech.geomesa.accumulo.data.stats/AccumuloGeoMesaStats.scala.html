<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml" lang=""><head><meta http-equiv="Content-Type" content="text/html;charset=UTF-8"/><link rel="stylesheet" href="../jacoco-resources/report.css" type="text/css"/><link rel="shortcut icon" href="../jacoco-resources/report.gif" type="image/gif"/><title>AccumuloGeoMesaStats.scala</title><link rel="stylesheet" href="../jacoco-resources/prettify.css" type="text/css"/><script type="text/javascript" src="../jacoco-resources/prettify.js"></script></head><body onload="window['PR_TAB_WIDTH']=4;prettyPrint()"><div class="breadcrumb" id="breadcrumb"><span class="info"><a href="../jacoco-sessions.html" class="el_session">Sessions</a></span><a href="../index.html" class="el_report">GeoMesa Accumulo DataStore</a> &gt; <a href="index.source.html" class="el_package">org.locationtech.geomesa.accumulo.data.stats</a> &gt; <span class="el_source">AccumuloGeoMesaStats.scala</span></div><h1>AccumuloGeoMesaStats.scala</h1><pre class="source lang-java linenums">/***********************************************************************
 * Copyright (c) 2013-2025 General Atomics Integrated Intelligence, Inc.
 * All rights reserved. This program and the accompanying materials
 * are made available under the terms of the Apache License, Version 2.0
 * which accompanies this distribution and is available at
 * https://www.apache.org/licenses/LICENSE-2.0
 ***********************************************************************/

package org.locationtech.geomesa.accumulo.data.stats

import org.apache.accumulo.core.client.AccumuloClient
import org.apache.hadoop.io.Text
import org.geotools.api.feature.simple.SimpleFeatureType
import org.locationtech.geomesa.accumulo.combiners.StatsCombiner
import org.locationtech.geomesa.accumulo.data._
import org.locationtech.geomesa.index.stats.GeoMesaStats.{GeoMesaStatWriter, StatUpdater}
import org.locationtech.geomesa.index.stats.MetadataBackedStats.{StatsMetadataSerializer, WritableStat}
import org.locationtech.geomesa.index.stats._
import org.locationtech.geomesa.utils.concurrent.ExitingExecutor

import java.util.concurrent.atomic.{AtomicBoolean, AtomicLong}
import java.util.concurrent.{ScheduledFuture, ScheduledThreadPoolExecutor, TimeUnit}

/**
  * Accumulo stats implementation handling table compactions
  *
  * @param ds ds
  */
<span class="nc" id="L29">class AccumuloGeoMesaStats(ds: AccumuloDataStore, val metadata: AccumuloBackedMetadata[Stat])</span>
<span class="nc" id="L30">    extends MetadataBackedStats(ds, metadata) {</span>

  import AccumuloGeoMesaStats._
  import org.locationtech.geomesa.utils.geotools.RichSimpleFeatureType.RichSimpleFeatureType

  import scala.collection.JavaConverters._

<span class="nc" id="L37">  private val compactionScheduled = new AtomicBoolean(false)</span>
<span class="nc" id="L38">  private val lastCompaction = new AtomicLong(0L)</span>

<span class="nc" id="L40">  private val running = new AtomicBoolean(true)</span>
<span class="nc" id="L41">  private var scheduledCompaction: ScheduledFuture[_] = _</span>

<span class="nc bnc" id="L43" title="All 2 branches missed.">  private val compactor = new Runnable() {</span>
    override def run(): Unit = {
      import org.locationtech.geomesa.accumulo.AccumuloProperties.StatsProperties.STAT_COMPACTION_INTERVAL
<span class="nc" id="L46">      val compactInterval = STAT_COMPACTION_INTERVAL.toDuration.get.toMillis</span>
<span class="nc bnc" id="L47" title="All 2 branches missed.">      if (lastCompaction.get &lt; System.currentTimeMillis() - compactInterval &amp;&amp;</span>
<span class="nc bnc" id="L48" title="All 2 branches missed.">          compactionScheduled.compareAndSet(true, false) ) {</span>
<span class="nc" id="L49">        compact()</span>
      }
<span class="nc bnc" id="L51" title="All 2 branches missed.">      if (running.get) {</span>
<span class="nc" id="L52">        synchronized{scheduledCompaction = executor.schedule(this, compactInterval, TimeUnit.MILLISECONDS)}</span>
      }
    }
  }

<span class="nc" id="L57">  compactor.run() // schedule initial compaction</span>

<span class="nc" id="L59">  override val writer: GeoMesaStatWriter = new AccumuloMetadataStatWriter()</span>

  override def close(): Unit = {
<span class="nc" id="L62">    super.close()</span>
<span class="nc" id="L63">    running.set(false)</span>
<span class="nc" id="L64">    synchronized{scheduledCompaction.cancel(false)}</span>
  }

  /**
    * Configures the stat combiner to sum stats dynamically.
    *
    * Note: should be called with a distributed lock on the stats table
    *
    * @param connector accumulo connector
    * @param sft simple feature type
    */
  def configureStatCombiner(connector: AccumuloClient, sft: SimpleFeatureType): Unit = {
    import MetadataBackedStats._

<span class="nc" id="L78">    StatsCombiner.configure(sft, connector, metadata.table, metadata.typeNameSeparator.toString)</span>

<span class="nc" id="L80">    val keys = Seq(CountKey, BoundsKeyPrefix, TopKKeyPrefix, FrequencyKeyPrefix, HistogramKeyPrefix)</span>
<span class="nc" id="L81">    val splits = new java.util.TreeSet[Text](keys.map(k =&gt; new Text(metadata.encodeRow(sft.getTypeName, k))).asJava)</span>
<span class="nc" id="L82">    connector.tableOperations().addSplits(metadata.table, splits)</span>
  }

  /**
    * Remove the stats combiner for a simple feature type
    *
    * Note: should be called with a distributed lock on the stats table
    *
    * @param connector accumulo connector
    * @param sft simple feature type
    */
  def removeStatCombiner(connector: AccumuloClient, sft: SimpleFeatureType): Unit =
<span class="nc" id="L94">    StatsCombiner.remove(sft, connector, metadata.table, metadata.typeNameSeparator.toString)</span>

  override protected def write(typeName: String, stats: Seq[WritableStat]): Unit = {
<span class="nc bnc" id="L97" title="All 2 branches missed.">    val (merge, overwrite) = stats.partition(_.merge)</span>
<span class="nc" id="L98">    metadata.insert(typeName, merge.map(s =&gt; s.key -&gt; s.stat).toMap)</span>
    // invalidate the cache as we would need to reload from accumulo for the combiner to take effect
<span class="nc" id="L100">    merge.foreach(s =&gt; metadata.invalidateCache(typeName, s.key))</span>
<span class="nc bnc" id="L101" title="All 2 branches missed.">    if (overwrite.nonEmpty) {</span>
      // due to accumulo issues with combiners, deletes and compactions, we have to:
      // 1) delete the existing data; 2) compact the table; 3) insert the new value
      // see: https://issues.apache.org/jira/browse/ACCUMULO-2232
<span class="nc" id="L105">      metadata.remove(typeName, overwrite.map(_.key))</span>
<span class="nc" id="L106">      compact()</span>
<span class="nc" id="L107">      metadata.insert(typeName, overwrite.map(s =&gt; s.key -&gt; s.stat).toMap)</span>
    }
  }

  /**
    * Performs a synchronous compaction of the stats table
    */
<span class="nc" id="L114">  private [accumulo] def compact(wait: Boolean = true): Unit = {</span>
<span class="nc" id="L115">    compactionScheduled.set(false)</span>
<span class="nc" id="L116">    ds.client.tableOperations().compact(metadata.table, null, null, true, wait)</span>
<span class="nc" id="L117">    lastCompaction.set(System.currentTimeMillis())</span>
  }

<span class="nc" id="L120">  class AccumuloMetadataStatWriter extends MetadataStatWriter {</span>

    override def rename(sft: SimpleFeatureType, previous: SimpleFeatureType): Unit = {
      // the stat combiner should still be configured for the old sft
      // the call to super() will read the old rows and write new rows, but it doesn't read any
      // new rows after writing them, so the combiner does not need to be correct yet
<span class="nc" id="L126">      super.rename(sft, previous)</span>
      // now remove the old sft and configure the new one
<span class="nc" id="L128">      removeStatCombiner(ds.client, previous)</span>
<span class="nc" id="L129">      configureStatCombiner(ds.client, sft)</span>
    }

    override def updater(sft: SimpleFeatureType): StatUpdater =
<span class="nc bnc" id="L133" title="All 2 branches missed.">      if (sft.statsEnabled) { new AccumuloStatUpdater(sft) } else { NoopStatUpdater }</span>
  }

  /**
    * Stores stats as metadata entries
    *
    * @param sft SimpleFeatureType
    */
<span class="nc" id="L141">  class AccumuloStatUpdater(sft: SimpleFeatureType) extends MetadataStatUpdater(sft) {</span>
    override def close(): Unit = {
<span class="nc" id="L143">      super.close()</span>
      // schedule a compaction so our metadata doesn't stack up too much
<span class="nc" id="L145">      compactionScheduled.set(true)</span>
    }
  }
}

<span class="nc" id="L150">object AccumuloGeoMesaStats {</span>

  def apply(ds: AccumuloDataStore): AccumuloGeoMesaStats = {
<span class="nc" id="L153">    val table = s&quot;${ds.config.catalog}_stats&quot;</span>
<span class="nc" id="L154">    val persistence = new AccumuloBackedMetadata(ds.client, table, new StatsMetadataSerializer(ds), ds.config.queries.consistency)</span>
<span class="nc" id="L155">    new AccumuloGeoMesaStats(ds, persistence)</span>
  }

<span class="nc" id="L158">  private[stats] val executor = ExitingExecutor(new ScheduledThreadPoolExecutor(3), force = true)</span>
<span class="nc" id="L159">}</span>
</pre><div class="footer"><span class="right">Created with <a href="http://www.jacoco.org/jacoco">JaCoCo</a> 0.8.14.202510111229</span></div></body></html>