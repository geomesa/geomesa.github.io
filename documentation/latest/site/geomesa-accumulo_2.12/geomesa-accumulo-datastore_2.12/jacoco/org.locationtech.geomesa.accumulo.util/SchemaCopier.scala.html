<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml" lang=""><head><meta http-equiv="Content-Type" content="text/html;charset=UTF-8"/><link rel="stylesheet" href="../jacoco-resources/report.css" type="text/css"/><link rel="shortcut icon" href="../jacoco-resources/report.gif" type="image/gif"/><title>SchemaCopier.scala</title><link rel="stylesheet" href="../jacoco-resources/prettify.css" type="text/css"/><script type="text/javascript" src="../jacoco-resources/prettify.js"></script></head><body onload="window['PR_TAB_WIDTH']=4;prettyPrint()"><div class="breadcrumb" id="breadcrumb"><span class="info"><a href="../jacoco-sessions.html" class="el_session">Sessions</a></span><a href="../index.html" class="el_report">GeoMesa Accumulo DataStore</a> &gt; <a href="index.source.html" class="el_package">org.locationtech.geomesa.accumulo.util</a> &gt; <span class="el_source">SchemaCopier.scala</span></div><h1>SchemaCopier.scala</h1><pre class="source lang-java linenums">/***********************************************************************
 * Copyright (c) 2013-2025 General Atomics Integrated Intelligence, Inc.
 * All rights reserved. This program and the accompanying materials
 * are made available under the terms of the Apache License, Version 2.0
 * which accompanies this distribution and is available at
 * https://www.apache.org/licenses/LICENSE-2.0
 ***********************************************************************/

package org.locationtech.geomesa.accumulo.util

import com.typesafe.scalalogging.StrictLogging
import org.apache.accumulo.core.client.admin.TableOperations
import org.apache.hadoop.conf.Configuration
import org.apache.hadoop.fs.{FileSystem, FileUtil, Path}
import org.apache.hadoop.tools.DistCp
import org.geotools.api.data.DataStoreFinder
import org.geotools.api.feature.simple.SimpleFeatureType
import org.locationtech.geomesa.accumulo.data.{AccumuloDataStore, AccumuloDataStoreParams}
import org.locationtech.geomesa.accumulo.util.SchemaCopier._
import org.locationtech.geomesa.features.ScalaSimpleFeature
import org.locationtech.geomesa.index.api.GeoMesaFeatureIndex
import org.locationtech.geomesa.index.conf.partition.TablePartition
import org.locationtech.geomesa.utils.concurrent.CachedThreadPool
import org.locationtech.geomesa.utils.geotools.SimpleFeatureTypes
import org.locationtech.geomesa.utils.hadoop.DistributedCopyOptions
import org.locationtech.geomesa.utils.io.{CloseWithLogging, WithClose}

import java.io.{Closeable, File, IOException}
import java.nio.charset.StandardCharsets
import java.util.Collections
import java.util.concurrent.{Callable, ConcurrentHashMap}
import scala.collection.mutable.{ArrayBuffer, ListBuffer}
import scala.io.Source
import scala.util.Try
import scala.util.control.NonFatal

/**
 * Copies a schema from one cluster (or catalog) to another, using bulk file operations
 *
 * @param fromCluster source cluster
 * @param toCluster destination cluster
 * @param typeName name of the feature type to copy
 * @param exportDir hdfs or s3a path to use for source table export - the scheme and authority (e.g. bucket name) must match
 *                   the destination table filesystem
 * @param indices specific indices to copy, or empty to copy all indices
 * @param partitions if schema is partitioned - specific partitions to copy, or empty to copy all partitions
 * @param options other copy options
 */
<span class="nc" id="L49">class SchemaCopier(</span>
<span class="nc" id="L50">    fromCluster: ClusterConfig,</span>
<span class="nc" id="L51">    toCluster: ClusterConfig,</span>
<span class="nc" id="L52">    typeName: String,</span>
<span class="nc" id="L53">    exportDir: String,</span>
<span class="nc" id="L54">    indices: Seq[String],</span>
<span class="nc" id="L55">    partitions: Seq[PartitionId],</span>
<span class="nc" id="L56">    options: CopyOptions,</span>
<span class="nc" id="L57">  ) extends Callable[Set[CopyResult]] with Closeable with StrictLogging {</span>

  import org.locationtech.geomesa.utils.geotools.RichSimpleFeatureType.RichSimpleFeatureType

  import scala.collection.JavaConverters._

<span class="nc" id="L63">  private val tryFrom = Try(Cluster(fromCluster))</span>
<span class="nc" id="L64">  private val tryTo = Try(Cluster(toCluster))</span>
<span class="nc" id="L65">  private var closed = false</span>

  // note: all other class variables are lazy, so that we can instantiate an instance and then clean up connections on close()

  // the cluster we are exporting from
<span class="nc bnc" id="L70" title="All 4 branches missed.">  lazy private val from: Cluster = tryFrom.get</span>
  // the cluster we are exporting to
<span class="nc bnc" id="L72" title="All 4 branches missed.">  lazy private val to: Cluster = tryTo.get</span>

<span class="nc bnc" id="L74" title="All 4 branches missed.">  lazy private val exportPath: Path = {</span>
<span class="nc" id="L75">    val defaultFs = FileSystem.get(to.conf)</span>
    // makeQualified is a no-op if the path was already qualified (has a defined scheme and is not a relative path)
<span class="nc" id="L77">    val path = new Path(exportDir).makeQualified(defaultFs.getUri, defaultFs.getWorkingDirectory)</span>
<span class="nc bnc" id="L78" title="All 6 branches missed.">    if (path.toUri.getScheme == &quot;file&quot;) {</span>
<span class="nc" id="L79">      throw new RuntimeException(&quot;Could not read defaultFS - this may be caused by a missing Hadoop core-site.xml file&quot;)</span>
    }
<span class="nc" id="L81">    path</span>
  }

<span class="nc bnc" id="L84" title="All 4 branches missed.">  lazy private val exportFs: FileSystem = exportPath.getFileSystem(to.conf)</span>

  // sft should be shareable/the same from both datastores
<span class="nc bnc" id="L87" title="All 4 branches missed.">  lazy private val sft: SimpleFeatureType = {</span>
<span class="nc" id="L88">    val sft = from.ds.getSchema(typeName)</span>
<span class="nc bnc" id="L89" title="All 2 branches missed.">    if (sft == null) {</span>
<span class="nc" id="L90">      throw new IllegalArgumentException(s&quot;Schema '$typeName' does not exist in the source store&quot;)</span>
    } else {
<span class="nc" id="L92">      val toSft = to.ds.getSchema(typeName)</span>
<span class="nc bnc" id="L93" title="All 2 branches missed.">      if (toSft == null) {</span>
<span class="nc" id="L94">        throw new IllegalArgumentException(s&quot;Schema '$typeName' does not exist in the destination store&quot;)</span>
<span class="nc bnc" id="L95" title="All 2 branches missed.">      } else if (SimpleFeatureTypes.compare(sft, toSft) != 0) {</span>
<span class="nc" id="L96">        throw new IllegalArgumentException(s&quot;Schema '$typeName' is not the same in the source and destination store&quot;)</span>
<span class="nc bnc" id="L97" title="All 2 branches missed.">      } else if (SimpleFeatureTypes.compareIndexConfigs(sft, toSft) != 0) {</span>
<span class="nc" id="L98">        throw new IllegalArgumentException(s&quot;Schema '$typeName' does not have compatible indices in the source and destination store&quot;)</span>
      }
    }
<span class="nc" id="L101">    sft</span>
  }

<span class="nc bnc" id="L104" title="All 4 branches missed.">  lazy private val indexPairs: Seq[(GeoMesaFeatureIndex[_, _], GeoMesaFeatureIndex[_, _])] = {</span>
<span class="nc" id="L105">    val all = from.ds.manager.indices(sft)</span>
<span class="nc bnc" id="L106" title="All 2 branches missed.">    val fromIndices = if (indices.isEmpty) { all } else {</span>
<span class="nc" id="L107">      val builder = Seq.newBuilder[GeoMesaFeatureIndex[_, _]]</span>
<span class="nc" id="L108">      indices.foreach { ident =&gt;</span>
<span class="nc" id="L109">        val filtered = all.filter(_.identifier.contains(ident))</span>
<span class="nc bnc" id="L110" title="All 2 branches missed.">        if (filtered.isEmpty) {</span>
<span class="nc" id="L111">          throw new IllegalArgumentException(</span>
<span class="nc" id="L112">            s&quot;Index '$ident' does not exist in the schema. Available indices: ${all.map(_.identifier).mkString(&quot;, &quot;)}&quot;)</span>
        }
<span class="nc bnc" id="L114" title="All 2 branches missed.">        logger.debug(s&quot;Mapped identifier $ident to ${filtered.map(_.identifier).mkString(&quot;, &quot;)}&quot;)</span>
<span class="nc" id="L115">        builder ++= filtered</span>
      }
<span class="nc" id="L117">      builder.result.distinct</span>
    }
<span class="nc" id="L119">    fromIndices.map(from =&gt; from -&gt; to.ds.manager.index(sft, from.identifier))</span>
  }

  // these get passed into our index method calls - for partitioned schemas, it must be a Seq[Some[_]],
  // while for non-partitioned schemas it must always be Seq(None)
<span class="nc bnc" id="L124" title="All 4 branches missed.">  lazy private val fromPartitions: Seq[Option[String]] = {</span>
<span class="nc bnc" id="L125" title="All 2 branches missed.">    if (sft.isPartitioned) {</span>
<span class="nc" id="L126">      val builder = ListBuffer.empty[String]</span>
<span class="nc bnc" id="L127" title="All 2 branches missed.">      lazy val partitioning = TablePartition(from.ds, sft).get</span>
<span class="nc bnc" id="L128" title="All 2 branches missed.">      lazy val sf = new ScalaSimpleFeature(sft, &quot;&quot;)</span>
<span class="nc" id="L129">      builder ++= partitions.map {</span>
<span class="nc bnc" id="L130" title="All 2 branches missed.">        case PartitionName(name) =&gt; name</span>
<span class="nc bnc" id="L131" title="All 2 branches missed.">        case PartitionValue(value) =&gt;</span>
<span class="nc" id="L132">          sf.setAttribute(sft.getDtgIndex.get, value)</span>
<span class="nc" id="L133">          val partition = partitioning.partition(sf)</span>
<span class="nc bnc" id="L134" title="All 2 branches missed.">          logger.debug(s&quot;Generated partition $partition from value $value&quot;)</span>
<span class="nc" id="L135">          partition</span>
      }
<span class="nc bnc" id="L137" title="All 2 branches missed.">      if (builder.isEmpty) {</span>
<span class="nc bnc" id="L138" title="All 2 branches missed.">        logger.debug(&quot;No partitions specified - loading all partitions from store&quot;)</span>
<span class="nc" id="L139">        builder ++= indexPairs.flatMap(_._1.getPartitions)</span>
      }
<span class="nc" id="L141">      builder.result.distinct.sorted.map(Option.apply)</span>
    } else {
<span class="nc bnc" id="L143" title="All 2 branches missed.">      if (partitions.nonEmpty) {</span>
<span class="nc" id="L144">        throw new IllegalArgumentException(&quot;partitions are not applicable for a non-partitioned schema&quot;)</span>
      }
<span class="nc" id="L146">      Seq(None)</span>
    }
  }

  // planned copies
<span class="nc bnc" id="L151" title="All 4 branches missed.">  lazy val plans: Set[CopyPlan] =</span>
<span class="nc" id="L152">    fromPartitions.flatMap { partition =&gt;</span>
<span class="nc bnc" id="L153" title="All 2 branches missed.">      indexPairs.map { case (fromIndex, _) =&gt;</span>
<span class="nc" id="L154">        CopyPlan(fromIndex.identifier, partition)</span>
      }
    }.toSet

  /**
   * Execute the copy
   *
   * @return results
   */
<span class="nc" id="L163">  override def call(): Set[CopyResult] = call(false)</span>

  /**
   * Execute the copy
   *
   * @param resume resume from a previously interrupted run, vs overwrite any existing output
   * @return results
   */
  def call(resume: Boolean): Set[CopyResult] = {
<span class="nc" id="L172">    val results = Collections.newSetFromMap(new ConcurrentHashMap[CopyResult, java.lang.Boolean]())</span>
<span class="nc" id="L173">    CachedThreadPool.executor(options.tableConcurrency) { executor =&gt;</span>
<span class="nc" id="L174">      fromPartitions.foreach { partition =&gt;</span>
<span class="nc bnc" id="L175" title="All 2 branches missed.">        indexPairs.foreach { case (fromIndex, toIndex) =&gt;</span>
<span class="nc" id="L176">          val partitionLogId = s&quot;${partition.fold(s&quot;index&quot;)(p =&gt; s&quot;partition $p&quot;)} ${fromIndex.identifier}&quot;</span>
<span class="nc" id="L177">          val runnable: Runnable = () =&gt; {</span>
<span class="nc bnc" id="L178" title="All 2 branches missed.">            logger.info(s&quot;Copying $partitionLogId&quot;)</span>
<span class="nc" id="L179">            val result = copy(fromIndex, toIndex, partition, resume, partitionLogId)</span>
<span class="nc" id="L180">            result.error match {</span>
<span class="nc bnc" id="L181" title="All 2 branches missed.">              case None =&gt;</span>
<span class="nc bnc" id="L182" title="All 2 branches missed.">                logger.info(s&quot;Bulk copy complete for $partitionLogId&quot;)</span>
<span class="nc bnc" id="L183" title="All 2 branches missed.">              case Some(e) =&gt;</span>
<span class="nc bnc" id="L184" title="All 2 branches missed.">                logger.error(s&quot;Error copying $partitionLogId: ${e.getMessage}&quot;)</span>
<span class="nc bnc" id="L185" title="All 2 branches missed.">                logger.debug(s&quot;Error copying $partitionLogId&quot;, e)</span>
            }
<span class="nc" id="L187">            results.add(result)</span>
          }
<span class="nc" id="L189">          executor.submit(runnable)</span>
        }
      }
    }
<span class="nc" id="L193">    results.asScala.toSet</span>
  }

  /**
   * Copy a single index + partition
   *
   * @param fromIndex from index
   * @param toIndex to index
   * @param partition partition name - must be Some if schema is partitioned
   * @param resume use any partial results from a previous run, if present
   * @param partitionLogId identifier for log messages
   * @return result
   */
  private def copy(
      fromIndex: GeoMesaFeatureIndex[_, _],
      toIndex: GeoMesaFeatureIndex[_, _],
      partition: Option[String],
      resume: Boolean,
      partitionLogId: String): CopyResult = {
<span class="nc" id="L212">    val start = System.currentTimeMillis()</span>
<span class="nc" id="L213">    val files = Collections.newSetFromMap(new ConcurrentHashMap[String, java.lang.Boolean]())</span>
<span class="nc" id="L214">    var fromTable: String = &quot;&quot;</span>
    // lazy so that table, files and finish time are filled in appropriately
<span class="nc bnc" id="L216" title="All 2 branches missed.">    lazy val result =</span>
      CopyResult(fromIndex.identifier, partition, fromTable, files.asScala.toSeq, None, start, System.currentTimeMillis())
    try {
<span class="nc" id="L219">      fromTable = fromIndex.getTableName(partition)</span>
<span class="nc" id="L220">      copy(fromTable, toIndex, partition, resume, partitionLogId, files)</span>
<span class="nc" id="L221">      result</span>
    } catch {
      // catch Throwable so NoClassDefFound still gets logged
<span class="nc" id="L224">      case e: Throwable =&gt; result.withError(e)</span>
    }
  }

  /**
   * Copy a single index + partition
   *
   * @param fromTable from table
   * @param toIndex to index
   * @param partition partition name - must be Some if schema is partitioned
   * @param resume use any partial results from a previous run, if present
   * @param partitionLogId identifier for log messages
   * @param fileResults set to hold files that we've copied successfully
   * @return result
   */
  private def copy(
      fromTable: String,
      toIndex: GeoMesaFeatureIndex[_, _],
      partition: Option[String],
      resume: Boolean,
      partitionLogId: String,
      fileResults: java.util.Set[String]): Unit = {

<span class="nc bnc" id="L247" title="All 2 branches missed.">    require(sft.isPartitioned == partition.isDefined) // sanity check - this should always be true due to our setup</span>

<span class="nc" id="L249">    val completeMarker = new Path(exportPath, s&quot;$fromTable.complete&quot;)</span>
<span class="nc bnc" id="L250" title="All 2 branches missed.">    if (exportFs.exists(completeMarker)) {</span>
<span class="nc bnc" id="L251" title="All 2 branches missed.">      if (resume) {</span>
<span class="nc bnc" id="L252" title="All 2 branches missed.">        logger.debug(&quot;Skipping already completed copy&quot;)</span>
<span class="nc" id="L253">        return</span>
      } else {
<span class="nc" id="L255">        exportFs.delete(completeMarker, false)</span>
      }
    }

<span class="nc" id="L259">    val tableExportPath = new Path(exportPath, fromTable)</span>
<span class="nc" id="L260">    val distcpPath = new Path(tableExportPath, &quot;distcp.txt&quot;)</span>
<span class="nc" id="L261">    val copyToDir = new Path(tableExportPath, &quot;files&quot;)</span>
<span class="nc" id="L262">    val cloneTable = s&quot;${fromTable}_bc_tmp&quot;</span>

<span class="nc bnc" id="L264" title="All 2 branches missed.">    logger.debug(s&quot;Source table $fromTable (${from.tableOps.tableIdMap().get(fromTable)})&quot;)</span>
<span class="nc bnc" id="L265" title="All 2 branches missed.">    logger.debug(s&quot;Export path $tableExportPath&quot;)</span>

<span class="nc bnc" id="L267" title="All 4 branches missed.">    if (resume &amp;&amp; from.tableOps.exists(cloneTable)) {</span>
<span class="nc bnc" id="L268" title="All 2 branches missed.">      logger.debug(s&quot;Using existing cloned table $cloneTable - ensuring table is offline&quot;)</span>
<span class="nc" id="L269">      from.tableOps.offline(cloneTable, true)</span>
    } else {
      // clone the table as we have to take it offline in order to export it
      // note that cloning is just a metadata op as it shares the underlying data files (until they change)
<span class="nc bnc" id="L273" title="All 2 branches missed.">      logger.debug(s&quot;Checking for existence and deleting any existing cloned table $cloneTable&quot;)</span>
<span class="nc" id="L274">      from.ds.adapter.deleteTable(cloneTable) // no-op if table doesn't exist</span>
<span class="nc bnc" id="L275" title="All 2 branches missed.">      logger.debug(s&quot;Cloning $fromTable to $cloneTable&quot;)</span>
<span class="nc" id="L276">      from.tableOps.clone(fromTable, cloneTable, true, Collections.emptyMap(), Collections.emptySet()) // use 2.0 method for compatibility</span>
<span class="nc bnc" id="L277" title="All 2 branches missed.">      logger.debug(s&quot;Taking $cloneTable offline&quot;)</span>
<span class="nc" id="L278">      from.tableOps.offline(cloneTable, true)</span>
    }

<span class="nc bnc" id="L281" title="All 6 branches missed.">    if (resume &amp;&amp; exportFs.exists(distcpPath) &amp;&amp; exportFs.getFileStatus(distcpPath).getLen &gt; 0) {</span>
<span class="nc bnc" id="L282" title="All 2 branches missed.">      logger.debug(s&quot;Using existing export results $distcpPath&quot;)</span>
<span class="nc" id="L283">    } else {</span>
<span class="nc bnc" id="L284" title="All 2 branches missed.">      if (exportFs.exists(tableExportPath)) {</span>
<span class="nc bnc" id="L285" title="All 2 branches missed.">        logger.debug(s&quot;Deleting existing export directory $tableExportPath&quot;)</span>
<span class="nc" id="L286">        exportFs.delete(tableExportPath, true)</span>
      }
<span class="nc bnc" id="L288" title="All 2 branches missed.">      logger.debug(s&quot;Exporting table to $tableExportPath&quot;)</span>
<span class="nc" id="L289">      from.tableOps.exportTable(cloneTable, tableExportPath.toString)</span>

<span class="nc bnc" id="L291" title="All 4 branches missed.">      if (!exportFs.exists(distcpPath) || exportFs.getFileStatus(distcpPath).getLen == 0) {</span>
<span class="nc" id="L292">        throw new RuntimeException(s&quot;Could not read table export results at $distcpPath&quot;)</span>
      }
    }

    // ensures the destination table exists
<span class="nc bnc" id="L297" title="All 2 branches missed.">    logger.debug(s&quot;Checking destination for table $fromTable&quot;)</span>
<span class="nc" id="L298">    to.ds.adapter.createTable(toIndex, partition, Seq.empty)</span>
<span class="nc" id="L299">    val toTable = try { toIndex.getTableName(partition) } catch {</span>
<span class="nc bnc" id="L300" title="All 2 branches missed.">      case NonFatal(e) =&gt; throw new RuntimeException(&quot;Could not get destination table&quot;, e)</span>
    }
<span class="nc bnc" id="L302" title="All 2 branches missed.">    logger.debug(s&quot;Destination table $toTable (${to.tableOps.tableIdMap().get(toTable)})&quot;)</span>

    // create splits, do this separately in case the table already exists
<span class="nc" id="L305">    val splits = new java.util.TreeSet(from.tableOps.listSplits(cloneTable))</span>
<span class="nc" id="L306">    val existingSplits = to.tableOps.listSplits(toTable)</span>
<span class="nc" id="L307">    splits.removeAll(existingSplits)</span>
<span class="nc bnc" id="L308" title="All 2 branches missed.">    if (!splits.isEmpty) {</span>
<span class="nc bnc" id="L309" title="All 2 branches missed.">      if (!existingSplits.isEmpty) {</span>
<span class="nc bnc" id="L310" title="All 2 branches missed.">        logger.warn(s&quot;Detected split mismatch between source ($fromTable) and destination ($toTable) for $partitionLogId&quot;)</span>
      }
<span class="nc bnc" id="L312" title="All 2 branches missed.">      logger.debug(s&quot;Adding splits to destination table $toTable&quot;)</span>
<span class="nc" id="L313">      to.tableOps.addSplits(toTable, splits)</span>
    }

<span class="nc" id="L316">    val copyErrors = Collections.newSetFromMap(new ConcurrentHashMap[Throwable, java.lang.Boolean]())</span>
    // read the distcp.txt file produced by the table export
    // consumer: (src, dest) =&gt; Unit
    def distCpConsumer(threads: Int)(consumer: (Path, Path) =&gt; Unit): Unit = {
<span class="nc bnc" id="L320" title="All 2 branches missed.">      logger.debug(s&quot;Reading $distcpPath&quot;)</span>
<span class="nc" id="L321">      WithClose(Source.fromInputStream(exportFs.open(distcpPath))(StandardCharsets.UTF_8)) { files =&gt;</span>
<span class="nc" id="L322">        CachedThreadPool.executor(threads) { executor =&gt;</span>
<span class="nc" id="L323">          files.getLines().foreach { file =&gt;</span>
<span class="nc" id="L324">            val runnable: Runnable = () =&gt; {</span>
<span class="nc" id="L325">              val path = new Path(file)</span>
<span class="nc" id="L326">              val copy = new Path(copyToDir, path.getName)</span>
              try {
<span class="nc bnc" id="L328" title="All 4 branches missed.">                if (resume &amp;&amp; exportFs.exists(copy) &amp;&amp;</span>
<span class="nc bnc" id="L329" title="All 2 branches missed.">                  path.getFileSystem(from.conf).getFileStatus(path).getLen == exportFs.getFileStatus(copy).getLen) {</span>
<span class="nc bnc" id="L330" title="All 2 branches missed.">                  logger.debug(s&quot;Using existing copy of $path at $copy&quot;)</span>
                } else {
<span class="nc" id="L332">                  consumer(path, copy)</span>
                }
              } catch {
                // catch Throwable so NoClassDefFound still gets logged
                case e: Throwable =&gt;
<span class="nc" id="L337">                  copyErrors.add(e)</span>
<span class="nc bnc" id="L338" title="All 2 branches missed.">                  logger.error(s&quot;Failed to copy $path to $copy: ${e.getMessage}&quot;)</span>
<span class="nc bnc" id="L339" title="All 2 branches missed.">                  logger.debug(s&quot;Failed to copy $path to $copy&quot;, e)</span>
              }
            }
<span class="nc" id="L342">            executor.submit(runnable)</span>
          }
        }
      }
    }

<span class="nc bnc" id="L348" title="All 2 branches missed.">    if (options.distCp) {</span>
<span class="nc" id="L349">      var inputPath = distcpPath</span>
<span class="nc" id="L350">      val distCpFiles = ArrayBuffer.empty[String]</span>
<span class="nc bnc" id="L351" title="All 2 branches missed.">      if (resume) {</span>
<span class="nc bnc" id="L352" title="All 2 branches missed.">        logger.debug(s&quot;Checking copy status of files in $distcpPath&quot;)</span>
<span class="nc" id="L353">        inputPath = new Path(tableExportPath, &quot;distcp-remaining.txt&quot;)</span>
<span class="nc" id="L354">        WithClose(exportFs.create(inputPath, true)) { out =&gt;</span>
<span class="nc" id="L355">          distCpConsumer(1) { (path, _) =&gt;</span>
<span class="nc bnc" id="L356" title="All 2 branches missed.">            logger.debug(s&quot;Adding $path to distcp&quot;)</span>
<span class="nc" id="L357">            out.writeUTF(s&quot;$path\n&quot;)</span>
<span class="nc" id="L358">            distCpFiles += path.getName</span>
          }
        }
<span class="nc" id="L361">      } else {</span>
<span class="nc bnc" id="L362" title="All 2 branches missed.">        logger.debug(s&quot;Checking file list at $distcpPath&quot;)</span>
<span class="nc" id="L363">        distCpConsumer(1) { (path, _) =&gt;</span>
<span class="nc" id="L364">          distCpFiles += path.getName</span>
        }
      }
<span class="nc" id="L367">      val job = new DistCp(from.conf, DistributedCopyOptions(inputPath, copyToDir)).execute()</span>
<span class="nc bnc" id="L368" title="All 2 branches missed.">      logger.info(s&quot;Tracking available at ${job.getStatus.getTrackingUrl}&quot;)</span>
<span class="nc bnc" id="L369" title="All 2 branches missed.">      while (!job.isComplete) {</span>
<span class="nc" id="L370">        Thread.sleep(500)</span>
      }
<span class="nc bnc" id="L372" title="All 2 branches missed.">      if (job.isSuccessful) {</span>
<span class="nc bnc" id="L373" title="All 2 branches missed.">        logger.info(s&quot;Successfully copied data to $copyToDir&quot;)</span>
<span class="nc" id="L374">        fileResults.addAll(distCpFiles.asJava)</span>
      } else {
<span class="nc" id="L376">        val msg = s&quot;DistCp job failed with state ${job.getStatus.getState} due to: ${job.getStatus.getFailureInfo}&quot;</span>
<span class="nc" id="L377">        copyErrors.add(new RuntimeException(msg))</span>
<span class="nc bnc" id="L378" title="All 2 branches missed.">        logger.error(msg)</span>
      }
    } else {
<span class="nc" id="L381">      distCpConsumer(options.fileConcurrency) { (path, copy) =&gt;</span>
<span class="nc bnc" id="L382" title="All 2 branches missed.">        logger.debug(s&quot;Copying $path to $copy&quot;)</span>
<span class="nc" id="L383">        val fs = path.getFileSystem(from.conf)</span>
<span class="nc bnc" id="L384" title="All 2 branches missed.">        if (FileUtil.copy(fs, path, exportFs, copy, false, true, to.conf)) {</span>
<span class="nc" id="L385">          fileResults.add(path.getName)</span>
        } else {
          // consolidate error handling in the catch block
<span class="nc" id="L388">          throw new IOException(s&quot;Failed to copy $path to $copy, copy returned false&quot;)</span>
        }
      }
    }
<span class="nc bnc" id="L392" title="All 2 branches missed.">    if (!copyErrors.isEmpty) {</span>
<span class="nc" id="L393">      val e = new RuntimeException(&quot;Error copying data files&quot;)</span>
<span class="nc" id="L394">      copyErrors.asScala.foreach(e.addSuppressed)</span>
<span class="nc" id="L395">      throw e</span>
    }

<span class="nc bnc" id="L398" title="All 2 branches missed.">    logger.debug(s&quot;Loading rfiles from $copyToDir to $toTable&quot;)</span>
<span class="nc" id="L399">    val importDir = to.tableOps.importDirectory(copyToDir.toString).to(toTable)</span>
<span class="nc" id="L400">    try { importDir.ignoreEmptyDir(true) } catch {</span>
<span class="nc" id="L401">      case _: NoSuchMethodError =&gt; // accumulo 2.0, ignore</span>
    }
<span class="nc" id="L403">    try { importDir.load() } catch {</span>
<span class="nc bnc" id="L404" title="All 2 branches missed.">      case e: IllegalArgumentException =&gt; logger.trace(&quot;Error importing directory:&quot;, e) // should mean empty dir</span>
    }

    // create marker indicating this copy was successful
<span class="nc bnc" id="L408" title="All 2 branches missed.">    logger.debug(s&quot;Creating completion marker $completeMarker&quot;)</span>
<span class="nc" id="L409">    exportFs.create(completeMarker).close()</span>

    // cleanup
<span class="nc bnc" id="L412" title="All 2 branches missed.">    logger.debug(s&quot;Deleting export path $tableExportPath&quot;)</span>
<span class="nc" id="L413">    exportFs.delete(tableExportPath, true)</span>
<span class="nc bnc" id="L414" title="All 2 branches missed.">    logger.debug(s&quot;Deleting clone table $cloneTable&quot;)</span>
<span class="nc" id="L415">    from.tableOps.delete(cloneTable)</span>
  }

<span class="nc" id="L418">  override def close(): Unit = synchronized {</span>
<span class="nc bnc" id="L419" title="All 2 branches missed.">    if (!closed) {</span>
<span class="nc" id="L420">      closed = true</span>
<span class="nc" id="L421">      CloseWithLogging(tryFrom.toOption)</span>
<span class="nc" id="L422">      CloseWithLogging(tryTo.toOption)</span>
    }
  }
}

<span class="nc" id="L427">object SchemaCopier {</span>

  sealed trait ClusterCredentials
<span class="nc bnc" id="L430" title="All 18 branches missed.">  case class ClusterPassword(password: String) extends ClusterCredentials</span>
<span class="nc bnc" id="L431" title="All 18 branches missed.">  case class ClusterKeytab(keytabPath: String) extends ClusterCredentials</span>

  sealed trait PartitionId
<span class="nc bnc" id="L434" title="All 18 branches missed.">  case class PartitionName(name: String) extends PartitionId</span>
<span class="nc bnc" id="L435" title="All 14 branches missed.">  case class PartitionValue(value: AnyRef) extends PartitionId</span>

  /**
   * Connection info for an Accumulo cluster
   *
   * @param instanceName instance name
   * @param zookeepers zookeepers
   * @param user user
   * @param credentials credentials
   * @param catalog catalog table containing the feature type
   * @param configFiles additional hadoop *-site.xml files for configuring hdfs operations
   */
<span class="nc bnc" id="L447" title="All 53 branches missed.">  case class ClusterConfig(</span>
<span class="nc" id="L448">    instanceName: String, zookeepers: String, user: String, credentials: ClusterCredentials, catalog: String, configFiles: Seq[File])</span>

  /**
   * Options
   *
   * @param tableConcurrency number of tables to copy in parallel
   * @param fileConcurrency number of files to copy in parallel, per table (when not using distcp)
   * @param distCp use hadoop distcp to copy files, instead of the hadoop client
   */
<span class="nc bnc" id="L457" title="All 22 branches missed.">  case class CopyOptions(tableConcurrency: Int = 1, fileConcurrency: Int = 4, distCp: Boolean = false)</span>

  /**
   * Planned copy operations
   *
   * @param index index id planned to copy
   * @param partition partition planned to copy
   */
<span class="nc bnc" id="L465" title="All 25 branches missed.">  case class CopyPlan(index: String, partition: Option[String])</span>

  /**
   * Result of a copy operation
   *
   * @param index index id being copied
   * @param partition partition being copied, if table is partitioned
   * @param table table being copied
   * @param files list of files that were successfully copied
   * @param error error, if any
   * @param start start of operation, in unix time
   * @param finish end of operation, in unix time
   */
<span class="nc bnc" id="L478" title="All 52 branches missed.">  case class CopyResult(</span>
<span class="nc" id="L479">      index: String,</span>
<span class="nc" id="L480">      partition: Option[String],</span>
<span class="nc" id="L481">      table: String,</span>
<span class="nc" id="L482">      files: Seq[String],</span>
<span class="nc" id="L483">      error: Option[Throwable],</span>
<span class="nc" id="L484">      start: Long,</span>
<span class="nc" id="L485">      finish: Long) {</span>
<span class="nc" id="L486">    def withError(e: Throwable): CopyResult = copy(error = Option(e))</span>
  }

  /**
   * Holds state for a given Accumulo cluster
   *
   * @param instance instance name
   * @param zk instance zookeepers
   * @param user username
   * @param credentials credentials - either Right(password) or Left(keytab)
   * @param catalog catalog table
   * @param configs additional hadoop configuration files
   */
<span class="nc" id="L499">  private class Cluster(</span>
      instance: String,
      zk: String,
      user: String,
      credentials: ClusterCredentials,
      catalog: String,
      configs: Seq[File]
<span class="nc" id="L506">    ) extends Closeable {</span>

    import scala.collection.JavaConverters._

<span class="nc" id="L510">    private val auth = credentials match {</span>
<span class="nc bnc" id="L511" title="All 2 branches missed.">      case ClusterPassword(password) =&gt; AccumuloDataStoreParams.PasswordParam.key -&gt; password</span>
<span class="nc bnc" id="L512" title="All 2 branches missed.">      case ClusterKeytab(keytabPath) =&gt; AccumuloDataStoreParams.KeytabPathParam.key -&gt; keytabPath</span>
    }

<span class="nc" id="L515">    private val params = Map(</span>
<span class="nc" id="L516">      AccumuloDataStoreParams.InstanceNameParam.key -&gt; instance,</span>
<span class="nc" id="L517">      AccumuloDataStoreParams.ZookeepersParam.key -&gt; zk,</span>
<span class="nc" id="L518">      AccumuloDataStoreParams.UserParam.key -&gt; user,</span>
<span class="nc" id="L519">      AccumuloDataStoreParams.CatalogParam.key -&gt; catalog,</span>
<span class="nc" id="L520">      auth</span>
    )

<span class="nc" id="L523">    val conf = new Configuration()</span>
<span class="nc" id="L524">    configs.foreach(f =&gt; conf.addResource(f.toURI.toURL))</span>

<span class="nc" id="L526">    val ds: AccumuloDataStore =</span>
<span class="nc" id="L527">      try { DataStoreFinder.getDataStore(params.asJava).asInstanceOf[AccumuloDataStore] } catch {</span>
        case NonFatal(e) =&gt; throw new IOException(&quot;Unable to load datastore:&quot;, e)
      }

<span class="nc bnc" id="L531" title="All 2 branches missed.">    if (ds == null) {</span>
<span class="nc bnc" id="L532" title="All 4 branches missed.">      val maskedParams = params.map {</span>
<span class="nc bnc" id="L533" title="All 6 branches missed.">        case (AccumuloDataStoreParams.PasswordParam.key , _) =&gt; s&quot;${AccumuloDataStoreParams.PasswordParam.key }=******&quot;</span>
<span class="nc" id="L534">        case (k, v) =&gt; s&quot;$k=$v&quot;</span>
      }
<span class="nc" id="L536">      throw new IOException(s&quot;Unable to load datastore using provided values: ${maskedParams.mkString(&quot;, &quot;)}&quot;)</span>
    }

<span class="nc" id="L539">    val tableOps: TableOperations = ds.client.tableOperations()</span>

<span class="nc" id="L541">    override def close(): Unit = ds.dispose()</span>
  }

<span class="nc" id="L544">  private object Cluster {</span>
    def apply(config: ClusterConfig): Cluster =
<span class="nc" id="L546">      new Cluster(config.instanceName, config.zookeepers, config.user, config.credentials, config.catalog, config.configFiles)</span>
  }
<span class="nc" id="L548">}</span>
</pre><div class="footer"><span class="right">Created with <a href="http://www.jacoco.org/jacoco">JaCoCo</a> 0.8.14.202510111229</span></div></body></html>