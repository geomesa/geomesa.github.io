<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml" lang=""><head><meta http-equiv="Content-Type" content="text/html;charset=UTF-8"/><link rel="stylesheet" href="../jacoco-resources/report.css" type="text/css"/><link rel="shortcut icon" href="../jacoco-resources/report.gif" type="image/gif"/><title>AccumuloBulkIngestCommand.scala</title><link rel="stylesheet" href="../jacoco-resources/prettify.css" type="text/css"/><script type="text/javascript" src="../jacoco-resources/prettify.js"></script></head><body onload="window['PR_TAB_WIDTH']=4;prettyPrint()"><div class="breadcrumb" id="breadcrumb"><span class="info"><a href="../jacoco-sessions.html" class="el_session">Sessions</a></span><a href="../index.html" class="el_report">GeoMesa Accumulo Tools</a> &gt; <a href="index.source.html" class="el_package">org.locationtech.geomesa.accumulo.tools.ingest</a> &gt; <span class="el_source">AccumuloBulkIngestCommand.scala</span></div><h1>AccumuloBulkIngestCommand.scala</h1><pre class="source lang-java linenums">/***********************************************************************
 * Copyright (c) 2013-2025 General Atomics Integrated Intelligence, Inc.
 * All rights reserved. This program and the accompanying materials
 * are made available under the terms of the Apache License, Version 2.0
 * which accompanies this distribution and is available at
 * https://www.apache.org/licenses/LICENSE-2.0
 ***********************************************************************/

package org.locationtech.geomesa.accumulo.tools.ingest

import com.beust.jcommander.{Parameter, ParameterException, Parameters}
import com.typesafe.config.Config
import org.apache.hadoop.conf.Configuration
import org.apache.hadoop.fs.{FileContext, Path}
import org.apache.hadoop.mapreduce.Job
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat
import org.geotools.api.feature.simple.SimpleFeatureType
import org.locationtech.geomesa.accumulo.data.AccumuloDataStore
import org.locationtech.geomesa.accumulo.jobs.mapreduce.GeoMesaAccumuloFileOutputFormat
import org.locationtech.geomesa.accumulo.tools.AccumuloDataStoreCommand.AccumuloDistributedCommand
import org.locationtech.geomesa.accumulo.tools.AccumuloDataStoreParams
import org.locationtech.geomesa.accumulo.tools.ingest.AccumuloBulkIngestCommand.AccumuloBulkIngestParams
import org.locationtech.geomesa.index.conf.partition.TablePartition
import org.locationtech.geomesa.jobs.JobResult.JobSuccess
import org.locationtech.geomesa.jobs.mapreduce.ConverterCombineInputFormat
import org.locationtech.geomesa.jobs.{Awaitable, JobResult, StatusCallback}
import org.locationtech.geomesa.tools.DistributedRunParam.RunModes
import org.locationtech.geomesa.tools.DistributedRunParam.RunModes.RunMode
import org.locationtech.geomesa.tools._
import org.locationtech.geomesa.tools.ingest.IngestCommand.{IngestParams, Inputs}
import org.locationtech.geomesa.tools.ingest._
import org.locationtech.geomesa.tools.utils.{DistributedCopy, Prompt}
import org.locationtech.geomesa.utils.hadoop.HadoopDelegate
import org.locationtech.geomesa.utils.index.IndexMode

import java.io.File

<span class="nc" id="L38">class AccumuloBulkIngestCommand extends IngestCommand[AccumuloDataStore] with AccumuloDistributedCommand {</span>

<span class="nc" id="L40">  override val name = &quot;bulk-ingest&quot;</span>
<span class="nc" id="L41">  override val params = new AccumuloBulkIngestParams()</span>

  override protected def startIngest(
      mode: RunMode,
      ds: AccumuloDataStore,
      sft: SimpleFeatureType,
      converter: Config,
      inputs: Inputs): Awaitable = {

    val maxSplitSize =
<span class="nc bnc" id="L51" title="All 2 branches missed.">      if (!params.combineInputs) { None } else { Option(params.maxSplitSize).map(_.intValue()).orElse(Some(0)) }</span>

    // validate index param now that we have a datastore and the sft has been created
<span class="nc" id="L54">    val index = params.loadIndex(ds, sft.getTypeName, IndexMode.Write).map(_.identifier)</span>

<span class="nc" id="L56">    val partitions = TablePartition(ds, sft).map { tp =&gt;</span>
<span class="nc bnc" id="L57" title="All 2 branches missed.">      if (params.cqlFilter == null) {</span>
<span class="nc" id="L58">        throw new ParameterException(</span>
<span class="nc" id="L59">          s&quot;Schema '${sft.getTypeName}' is a partitioned store. In order to bulk load, the '--cql' parameter &quot; +</span>
<span class="nc" id="L60">              &quot;must be used to specify the range of the input data set&quot;)</span>
      }
<span class="nc" id="L62">      tp.partitions(params.cqlFilter).filter(_.nonEmpty).getOrElse {</span>
<span class="nc" id="L63">        throw new ParameterException(</span>
<span class="nc" id="L64">          s&quot;Partition filter does not correspond to partition scheme ${tp.getClass.getSimpleName}. Please specify &quot; +</span>
<span class="nc" id="L65">              &quot;a valid filter using the '--cql' parameter&quot;)</span>
      }
    }


<span class="nc" id="L70">    mode match {</span>
<span class="nc bnc" id="L71" title="All 6 branches missed.">      case RunModes.Local =&gt;</span>
<span class="nc" id="L72">        throw new IllegalArgumentException(&quot;Bulk ingest must be run in distributed mode&quot;)</span>

<span class="nc bnc" id="L74" title="All 6 branches missed.">      case RunModes.Distributed =&gt;</span>
<span class="nc" id="L75">        val conf = new Configuration()</span>
        // file output format doesn't let you write to an existing directory
<span class="nc" id="L77">        val output = new Path(params.outputPath)</span>
<span class="nc" id="L78">        val context = FileContext.getFileContext(output.toUri, conf)</span>
<span class="nc bnc" id="L79" title="All 2 branches missed.">        if (context.util.exists(output)) {</span>
<span class="nc" id="L80">          val warning = s&quot;Output directory '$output' exists&quot;</span>
<span class="nc bnc" id="L81" title="All 2 branches missed.">          if (params.force) {</span>
<span class="nc bnc" id="L82" title="All 2 branches missed.">            Command.user.warn(s&quot;$warning - deleting it&quot;)</span>
<span class="nc bnc" id="L83" title="All 2 branches missed.">          } else if (!Prompt.confirm(s&quot;WARNING DATA MAY BE LOST: $warning. Delete it and continue (y/n)? &quot;)) {</span>
<span class="nc" id="L84">            throw new ParameterException(s&quot;Output directory '$output' exists&quot;)</span>
          }
<span class="nc" id="L86">          context.delete(output, true)</span>
        }

<span class="nc" id="L89">        val tempPath = Option(params.tempPath).map { temp =&gt;</span>
<span class="nc" id="L90">          val path = new Path(temp)</span>
          // get a new file context as this is likely to be a different filesystem (i.e. hdfs vs s3)
<span class="nc" id="L92">          val tempContext = FileContext.getFileContext(path.toUri, conf)</span>
<span class="nc" id="L93">          val dir = tempContext.makeQualified(path)</span>
<span class="nc bnc" id="L94" title="All 2 branches missed.">          if (tempContext.util.exists(dir)) {</span>
<span class="nc bnc" id="L95" title="All 2 branches missed.">            Command.user.info(s&quot;Deleting temp output path $dir&quot;)</span>
<span class="nc" id="L96">            tempContext.delete(dir, true)</span>
          }
<span class="nc" id="L98">          dir</span>
        }

<span class="nc bnc" id="L101" title="All 4 branches missed.">        Command.user.info(s&quot;Running bulk ingestion in distributed ${if (params.combineInputs) &quot;combine &quot; else &quot;&quot; }mode&quot;)</span>
<span class="nc" id="L102">        new BulkConverterIngest(ds, connection, sft, converter, inputs.paths, output, tempPath, maxSplitSize,</span>
<span class="nc" id="L103">          index, partitions, libjarsFiles, libjarsPaths)</span>

      case _ =&gt;
<span class="nc" id="L106">        throw new UnsupportedOperationException(s&quot;Missing implementation for mode $mode&quot;)</span>
    }
  }

<span class="nc bnc" id="L110" title="All 2 branches missed.">  class BulkConverterIngest(</span>
<span class="nc" id="L111">      ds: AccumuloDataStore,</span>
<span class="nc" id="L112">      dsParams: Map[String, String],</span>
<span class="nc" id="L113">      sft: SimpleFeatureType,</span>
      converterConfig: Config,
      paths: Seq[String],
<span class="nc" id="L116">      output: Path,</span>
<span class="nc" id="L117">      tempOutput: Option[Path],</span>
<span class="nc" id="L118">      maxSplitSize: Option[Int],</span>
<span class="nc" id="L119">      index: Option[String],</span>
<span class="nc" id="L120">      partitions: Option[Seq[String]],</span>
      libjarsFiles: Seq[String],
      libjarsPaths: Iterator[() =&gt; Seq[File]]
<span class="nc" id="L123">    ) extends ConverterIngestJob(dsParams, sft, converterConfig, paths, libjarsFiles, libjarsPaths) {</span>

<span class="nc" id="L125">    private var libjars: String = _</span>

    override def configureJob(job: Job): Unit = {
<span class="nc" id="L128">      super.configureJob(job)</span>
<span class="nc" id="L129">      val dest = tempOutput.getOrElse(output)</span>
<span class="nc" id="L130">      GeoMesaAccumuloFileOutputFormat.configure(job, ds, dsParams, sft, dest, index, partitions)</span>
<span class="nc" id="L131">      maxSplitSize.foreach { max =&gt;</span>
<span class="nc" id="L132">        job.setInputFormatClass(classOf[ConverterCombineInputFormat])</span>
<span class="nc bnc" id="L133" title="All 2 branches missed.">        if (max &gt; 0) {</span>
<span class="nc" id="L134">          FileInputFormat.setMaxInputSplitSize(job, max.toLong)</span>
        }
      }
<span class="nc" id="L137">      this.libjars = job.getConfiguration.get(&quot;tmpjars&quot;)</span>
    }

    override def await(reporter: StatusCallback): JobResult = {
<span class="nc" id="L141">      super.await(reporter).merge {</span>
<span class="nc" id="L142">        tempOutput.map { dir =&gt;</span>
<span class="nc" id="L143">          reporter.reset()</span>
<span class="nc" id="L144">          val conf = new Configuration()</span>
<span class="nc" id="L145">          conf.set(&quot;tmpjars&quot;, this.libjars) // copy over out libjars so s3 apis are on the classpath</span>
<span class="nc" id="L146">          new DistributedCopy(conf).copy(Seq(dir), output, reporter) match {</span>
<span class="nc bnc" id="L147" title="All 2 branches missed.">            case JobSuccess(message, counts) =&gt;</span>
<span class="nc bnc" id="L148" title="All 2 branches missed.">              Command.user.info(message)</span>
<span class="nc" id="L149">              JobSuccess(&quot;&quot;, counts)</span>

<span class="nc" id="L151">            case j =&gt; j</span>
          }
        }
<span class="nc" id="L154">      }.merge {</span>
<span class="nc bnc" id="L155" title="All 2 branches missed.">        if (params.skipImport) {</span>
<span class="nc bnc" id="L156" title="All 2 branches missed.">          Command.user.info(&quot;Skipping import of RFiles into Accumulo&quot;)</span>
<span class="nc bnc" id="L157" title="All 2 branches missed.">          Command.user.info(</span>
<span class="nc" id="L158">            &quot;Files may be imported for each table through the Accumulo shell with the `importdirectory` command&quot;)</span>
<span class="nc" id="L159">        } else {</span>
<span class="nc bnc" id="L160" title="All 2 branches missed.">          Command.user.info(&quot;Importing RFiles into Accumulo&quot;)</span>
<span class="nc" id="L161">          val tableOps = ds.client.tableOperations()</span>
<span class="nc" id="L162">          val filesPath = new Path(output, GeoMesaAccumuloFileOutputFormat.FilesPath)</span>
<span class="nc" id="L163">          val fc = FileContext.getFileContext(filesPath.toUri, new Configuration())</span>
<span class="nc" id="L164">          val files = fc.listLocatedStatus(filesPath)</span>
<span class="nc bnc" id="L165" title="All 2 branches missed.">          while (files.hasNext) {</span>
<span class="nc" id="L166">            val file = files.next()</span>
<span class="nc" id="L167">            val path = file.getPath</span>
<span class="nc" id="L168">            val table = path.getName</span>
<span class="nc bnc" id="L169" title="All 6 branches missed.">            if (file.isDirectory &amp;&amp; HadoopDelegate.HiddenFileFilter.accept(path) &amp;&amp; tableOps.exists(table)) {</span>
<span class="nc bnc" id="L170" title="All 2 branches missed.">              Command.user.info(s&quot;Importing $table&quot;)</span>
<span class="nc" id="L171">              tableOps.importDirectory(path.toString).to(table).load()</span>
            }
          }
        }
<span class="nc" id="L175">        None</span>
      }
    }
  }
}

<span class="nc" id="L181">object AccumuloBulkIngestCommand {</span>
  @Parameters(commandDescription = &quot;Convert various file formats into bulk loaded Accumulo RFiles&quot;)
<span class="nc bnc" id="L183" title="All 4 branches missed.">  class AccumuloBulkIngestParams extends IngestParams with AccumuloDataStoreParams</span>
      with OutputPathParam with OptionalIndexParam with OptionalCqlFilterParam with TempPathParam {
    @Parameter(names = Array(&quot;--skip-import&quot;), description = &quot;Generate the files but skip the bulk import into Accumulo&quot;)
<span class="nc" id="L186">    var skipImport: Boolean = false</span>
  }
<span class="nc" id="L188">}</span>
</pre><div class="footer"><span class="right">Created with <a href="http://www.jacoco.org/jacoco">JaCoCo</a> 0.8.14.202510111229</span></div></body></html>