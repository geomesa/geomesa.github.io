<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml" lang=""><head><meta http-equiv="Content-Type" content="text/html;charset=UTF-8"/><link rel="stylesheet" href="../jacoco-resources/report.css" type="text/css"/><link rel="shortcut icon" href="../jacoco-resources/report.gif" type="image/gif"/><title>package.scala</title><link rel="stylesheet" href="../jacoco-resources/prettify.css" type="text/css"/><script type="text/javascript" src="../jacoco-resources/prettify.js"></script></head><body onload="window['PR_TAB_WIDTH']=4;prettyPrint()"><div class="breadcrumb" id="breadcrumb"><span class="info"><a href="../jacoco-sessions.html" class="el_session">Sessions</a></span><a href="../index.html" class="el_report">GeoMesa Accumulo Jobs</a> &gt; <a href="index.source.html" class="el_package">org.locationtech.geomesa.accumulo.jobs.mapreduce</a> &gt; <span class="el_source">package.scala</span></div><h1>package.scala</h1><pre class="source lang-java linenums">/***********************************************************************
 * Copyright (c) 2013-2025 General Atomics Integrated Intelligence, Inc.
 * All rights reserved. This program and the accompanying materials
 * are made available under the terms of the Apache License, Version 2.0
 * which accompanies this distribution and is available at
 * https://www.apache.org/licenses/LICENSE-2.0
 ***********************************************************************/

package org.locationtech.geomesa.accumulo.jobs

import com.github.benmanes.caffeine.cache.{CacheLoader, Caffeine}
import org.apache.accumulo.core.data.Key
import org.apache.hadoop.conf.{Configurable, Configuration}
import org.apache.hadoop.fs.Options.CreateOpts
import org.apache.hadoop.fs.{CreateFlag, FileContext, FileSystem, Path}
import org.apache.hadoop.io.{BinaryComparable, Text, Writable, WritableComparable}
import org.apache.hadoop.mapreduce.{Job, Partitioner}
import org.locationtech.geomesa.utils.io.WithClose

import java.io.{BufferedOutputStream, DataInput, DataOutput, PrintStream}
import java.nio.charset.StandardCharsets
import java.util.{Base64, Scanner}
import scala.collection.mutable.ArrayBuffer

<span class="nc" id="L25">package object mapreduce {</span>

<span class="nc" id="L27">  object Configurator {</span>

<span class="nc" id="L29">    private val TypeNameKey   = &quot;org.locationtech.geomesa.accumulo.typename&quot;</span>
<span class="nc" id="L30">    private val PartitionsKey = &quot;org.locationtech.geomesa.accumulo.partitions&quot;</span>

<span class="nc" id="L32">    def setTypeName(conf: Configuration, typeName: String): Unit = conf.set(TypeNameKey, typeName)</span>
<span class="nc" id="L33">    def getTypeName(conf: Configuration): String = conf.get(TypeNameKey)</span>
    def setPartitions(conf: Configuration, partitions: Seq[String]): Unit =
<span class="nc" id="L35">      conf.set(PartitionsKey, partitions.mkString(&quot;,&quot;))</span>
<span class="nc" id="L36">    def getPartitions(conf: Configuration): Option[Seq[String]] = Option(conf.get(PartitionsKey)).map(_.split(&quot;,&quot;))</span>
  }

<span class="nc" id="L39">  class TableAndKey extends WritableComparable[TableAndKey] {</span>

<span class="nc" id="L41">    private var table: Text = _</span>
<span class="nc" id="L42">    private var key: Key = _</span>

<span class="nc" id="L44">    def this(table: Text, key: Key) = {</span>
<span class="nc" id="L45">      this()</span>
<span class="nc" id="L46">      this.table = table</span>
<span class="nc" id="L47">      this.key = key</span>
    }

<span class="nc" id="L50">    def getTable: Text = table</span>
<span class="nc" id="L51">    def setTable(table: Text): Unit = this.table = table</span>
<span class="nc" id="L52">    def getKey: Key = key</span>
<span class="nc" id="L53">    def setKey(key: Key): Unit = this.key = key</span>

    override def write(out: DataOutput): Unit = {
<span class="nc" id="L56">      table.write(out)</span>
<span class="nc" id="L57">      key.write(out)</span>
    }

    override def readFields(in: DataInput): Unit = {
<span class="nc" id="L61">      table = new Text()</span>
<span class="nc" id="L62">      table.readFields(in)</span>
<span class="nc" id="L63">      key = new Key()</span>
<span class="nc" id="L64">      key.readFields(in)</span>
    }

    override def compareTo(o: TableAndKey): Int = {
<span class="nc" id="L68">      val c = table.compareTo(o.table)</span>
<span class="nc bnc" id="L69" title="All 2 branches missed.">      if (c != 0) { c } else {</span>
<span class="nc" id="L70">        key.compareTo(o.key)</span>
      }
    }
  }

<span class="nc" id="L75">  class TableRangePartitioner extends Partitioner[TableAndKey, Writable] with Configurable {</span>

<span class="nc" id="L77">    private var conf: Configuration = _</span>

<span class="nc bnc" id="L79" title="All 2 branches missed.">    private val splitsPerTable = Caffeine.newBuilder().build(new CacheLoader[Text, (Int, Array[AnyRef])]() {</span>
      override def load(k: Text): (Int, Array[AnyRef]) = {
<span class="nc" id="L81">        val splits = ArrayBuffer.empty[Text]</span>
        // the should be available due to our calling job.addCacheFile
<span class="nc" id="L83">        WithClose(FileSystem.getLocal(conf)) { fs =&gt;</span>
<span class="nc" id="L84">          val path = new Path(s&quot;${k.toString}.txt&quot;)</span>
<span class="nc" id="L85">          WithClose(new Scanner(fs.open(path), StandardCharsets.UTF_8.name)) { scanner =&gt;</span>
<span class="nc bnc" id="L86" title="All 2 branches missed.">            while (scanner.hasNextLine) {</span>
<span class="nc" id="L87">              splits += new Text(Base64.getDecoder.decode(scanner.nextLine))</span>
            }
          }
        }
<span class="nc" id="L91">        val sorted = splits.distinct.sorted(Ordering.by[Text, BinaryComparable](identity)).toArray[AnyRef]</span>
<span class="nc" id="L92">        val offset = TableRangePartitioner.getTableOffset(conf, k.toString)</span>
<span class="nc" id="L93">        (offset, sorted)</span>
      }
    })

    override def getPartition(key: TableAndKey, value: Writable, total: Int): Int = {
<span class="nc bnc" id="L98" title="All 2 branches missed.">      val (offset, splits) = splitsPerTable.get(key.getTable)</span>
<span class="nc" id="L99">      val i = java.util.Arrays.binarySearch(splits, key.getKey.getRow)</span>
      // account for negative results indicating the spot between 2 values
<span class="nc bnc" id="L101" title="All 2 branches missed.">      val index = if (i &lt; 0) { (i + 1) * -1 } else { i }</span>
<span class="nc" id="L102">      offset + index</span>
    }

<span class="nc" id="L105">    override def setConf(configuration: Configuration): Unit = this.conf = configuration</span>
<span class="nc" id="L106">    override def getConf: Configuration = conf</span>
  }

<span class="nc" id="L109">  object TableRangePartitioner {</span>

<span class="nc" id="L111">    private val SplitsPath = &quot;org.locationtech.geomesa.accumulo.splits.path&quot;</span>
<span class="nc" id="L112">    private val TableOffset = &quot;org.locationtech.geomesa.accumulo.table.offset&quot;</span>

    // must be called after setSplitsPath
    def setTableSplits(job: Job, table: String, splits: Iterable[Text]): Unit = {
<span class="nc" id="L116">      val dir = getSplitsPath(job.getConfiguration)</span>
<span class="nc" id="L117">      val file = s&quot;$table.txt&quot;</span>
<span class="nc" id="L118">      val output = new Path(s&quot;$dir/$file&quot;)</span>
<span class="nc" id="L119">      val fc = FileContext.getFileContext(output.toUri, job.getConfiguration)</span>
<span class="nc" id="L120">      val flags = java.util.EnumSet.of(CreateFlag.CREATE)</span>
<span class="nc" id="L121">      WithClose(new PrintStream(new BufferedOutputStream(fc.create(output, flags, CreateOpts.createParent)))) { out =&gt;</span>
<span class="nc" id="L122">        splits.foreach(split =&gt; out.println(Base64.getEncoder.encodeToString(split.copyBytes)))</span>
      }
      // this makes the file accessible as a local file on the cluster
<span class="nc" id="L125">      job.addCacheFile(output.toUri)</span>
    }

<span class="nc" id="L128">    def setSplitsPath(conf: Configuration, path: String): Unit = conf.set(SplitsPath, path)</span>
<span class="nc" id="L129">    def getSplitsPath(conf: Configuration): String = conf.get(SplitsPath)</span>

    def setTableOffset(conf: Configuration, table: String, offset: Int): Unit =
<span class="nc" id="L132">      conf.setInt(s&quot;$TableOffset.$table&quot;, offset)</span>
<span class="nc" id="L133">    def getTableOffset(conf: Configuration, table: String): Int = conf.get(s&quot;$TableOffset.$table&quot;).toInt</span>
  }
<span class="nc" id="L135">}</span>
</pre><div class="footer"><span class="right">Created with <a href="http://www.jacoco.org/jacoco">JaCoCo</a> 0.8.14.202510111229</span></div></body></html>