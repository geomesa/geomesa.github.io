<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml" lang=""><head><meta http-equiv="Content-Type" content="text/html;charset=UTF-8"/><link rel="stylesheet" href="../jacoco-resources/report.css" type="text/css"/><link rel="shortcut icon" href="../jacoco-resources/report.gif" type="image/gif"/><title>GeoMesaAccumuloFileOutputFormat.scala</title><link rel="stylesheet" href="../jacoco-resources/prettify.css" type="text/css"/><script type="text/javascript" src="../jacoco-resources/prettify.js"></script></head><body onload="window['PR_TAB_WIDTH']=4;prettyPrint()"><div class="breadcrumb" id="breadcrumb"><span class="info"><a href="../jacoco-sessions.html" class="el_session">Sessions</a></span><a href="../index.html" class="el_report">GeoMesa Accumulo Jobs</a> &gt; <a href="index.source.html" class="el_package">org.locationtech.geomesa.accumulo.jobs.mapreduce</a> &gt; <span class="el_source">GeoMesaAccumuloFileOutputFormat.scala</span></div><h1>GeoMesaAccumuloFileOutputFormat.scala</h1><pre class="source lang-java linenums">/***********************************************************************
 * Copyright (c) 2013-2025 General Atomics Integrated Intelligence, Inc.
 * All rights reserved. This program and the accompanying materials
 * are made available under the terms of the Apache License, Version 2.0
 * which accompanies this distribution and is available at
 * https://www.apache.org/licenses/LICENSE-2.0
 ***********************************************************************/

package org.locationtech.geomesa.accumulo.jobs.mapreduce

import com.typesafe.scalalogging.LazyLogging
import org.apache.accumulo.core.data.{Key, Value}
import org.apache.accumulo.hadoop.mapreduce.AccumuloFileOutputFormat
import org.apache.hadoop.fs.Path
import org.apache.hadoop.io.{Text, Writable}
import org.apache.hadoop.mapreduce.lib.output.{LazyOutputFormat, MultipleOutputs}
import org.apache.hadoop.mapreduce.{Counter, Job, Mapper, Reducer}
import org.geotools.api.data.DataStoreFinder
import org.geotools.api.feature.simple.{SimpleFeature, SimpleFeatureType}
import org.locationtech.geomesa.accumulo.data.AccumuloDataStore
import org.locationtech.geomesa.accumulo.data.writer.VisibilityCache
import org.locationtech.geomesa.index.api.WritableFeature.FeatureWrapper
import org.locationtech.geomesa.index.api._
import org.locationtech.geomesa.index.conf.partition.TablePartition
import org.locationtech.geomesa.jobs.GeoMesaConfigurator
import org.locationtech.geomesa.jobs.mapreduce.GeoMesaOutputFormat.OutputCounters
import org.locationtech.geomesa.utils.concurrent.CachedThreadPool
import org.locationtech.geomesa.utils.index.IndexMode

import scala.util.control.NonFatal

/**
 * Output format for writing RFiles directly to hdfs instead of using batch writers
 */
<span class="nc bnc" id="L35" title="All 4 branches missed.">object GeoMesaAccumuloFileOutputFormat extends LazyLogging {</span>

  import scala.collection.JavaConverters._

<span class="nc" id="L39">  val FilesPath  = &quot;files&quot;</span>
<span class="nc" id="L40">  val SplitsPath = &quot;splits&quot;</span>

  /**
   * Sets mapper class, reducer class, output format and associated options
   *
   * @param job job
   * @param ds data store for output data
   * @param params data store parameters for output data
   * @param sft feature type to write (schema must exist already)
   * @param output output path for rFiles
   * @param index optional index to write
   * @param partitions if writing to a partitioned store, the partitions being written to
   */
  def configure(
      job: Job,
      ds: AccumuloDataStore,
      params: Map[String, String],
      sft: SimpleFeatureType,
      output: Path,
      index: Option[String],
      partitions: Option[Seq[String]]): Unit = {

<span class="nc" id="L62">    val indices = index match {</span>
<span class="nc bnc" id="L63" title="All 2 branches missed.">      case None    =&gt; ds.manager.indices(sft, IndexMode.Write)</span>
<span class="nc bnc" id="L64" title="All 2 branches missed.">      case Some(i) =&gt; Seq(ds.manager.index(sft, i, IndexMode.Write))</span>
    }

<span class="nc" id="L67">    val tables = partitions match {</span>
<span class="nc bnc" id="L68" title="All 2 branches missed.">      case None =&gt; indices.flatMap(_.getTableNames())</span>
<span class="nc bnc" id="L69" title="All 2 branches missed.">      case Some(parts) =&gt;</span>
<span class="nc" id="L70">        Configurator.setPartitions(job.getConfiguration, parts)</span>
<span class="nc bnc" id="L71" title="All 2 branches missed.">        logger.debug(s&quot;Creating index tables for ${parts.length} partitions&quot;)</span>
<span class="nc" id="L72">        parts.flatMap { p =&gt;</span>
          // create the partitions up front so we know the number of splits and reducers - this call is idempotent
          def createOne(index: GeoMesaFeatureIndex[_, _]): Unit =
<span class="nc" id="L75">            ds.adapter.createTable(index, Some(p), index.getSplits(Some(p)))</span>
<span class="nc" id="L76">          indices.toList.map(index =&gt; CachedThreadPool.submit(() =&gt; createOne(index))).foreach(_.get)</span>
<span class="nc" id="L77">          indices.flatMap(_.getTableNames(Some(p)))</span>
        }
    }

<span class="nc bnc" id="L81" title="All 2 branches missed.">    if (tables.isEmpty) {</span>
<span class="nc" id="L82">      throw new IllegalArgumentException(&quot;No tables found for output&quot;)</span>
    }

<span class="nc" id="L85">    GeoMesaConfigurator.setDataStoreOutParams(job.getConfiguration, params)</span>
<span class="nc" id="L86">    GeoMesaConfigurator.setIndicesOut(job.getConfiguration, indices.map(_.identifier))</span>
<span class="nc" id="L87">    GeoMesaConfigurator.setSerialization(job.getConfiguration, sft)</span>
<span class="nc" id="L88">    Configurator.setTypeName(job.getConfiguration, sft.getTypeName)</span>
    // using LazyOutputFormat prevents creating empty output files for regions with no data
<span class="nc" id="L90">    LazyOutputFormat.setOutputFormatClass(job, classOf[AccumuloFileOutputFormat])</span>
    // note: this is equivalent to FileOutputFormat.setOutputPath(job, output)
<span class="nc" id="L92">    AccumuloFileOutputFormat.configure.outputPath(new Path(output, FilesPath)).store(job)</span>

<span class="nc" id="L94">    job.setPartitionerClass(classOf[TableRangePartitioner])</span>
<span class="nc" id="L95">    TableRangePartitioner.setSplitsPath(job.getConfiguration, new Path(output, SplitsPath).toString)</span>

<span class="nc" id="L97">    var numReducers = 0</span>
<span class="nc" id="L98">    tables.foreach { table =&gt;</span>
<span class="nc" id="L99">      val splits = ds.client.tableOperations.listSplits(table).asScala</span>
<span class="nc" id="L100">      TableRangePartitioner.setTableOffset(job.getConfiguration, table, numReducers)</span>
<span class="nc" id="L101">      TableRangePartitioner.setTableSplits(job, table, splits)</span>
<span class="nc" id="L102">      numReducers += (splits.size + 1) // add one for the region before the first split point</span>
    }

<span class="nc" id="L105">    job.setMapperClass(classOf[AccumuloFileMapper])</span>
<span class="nc" id="L106">    job.setMapOutputKeyClass(classOf[TableAndKey])</span>
<span class="nc" id="L107">    job.setMapOutputValueClass(classOf[Value])</span>
<span class="nc" id="L108">    job.setReducerClass(classOf[AccumuloFileReducer])</span>
<span class="nc" id="L109">    job.setOutputKeyClass(classOf[Key])</span>
<span class="nc" id="L110">    job.setOutputValueClass(classOf[Value])</span>
<span class="nc" id="L111">    job.setNumReduceTasks(numReducers)</span>
  }

<span class="nc bnc" id="L114" title="All 4 branches missed.">  class AccumuloFileMapper extends Mapper[Writable, SimpleFeature, TableAndKey, Value] with LazyLogging {</span>

    type MapContext = Mapper[Writable, SimpleFeature, TableAndKey, Value]#Context

<span class="nc" id="L118">    private var ds: AccumuloDataStore = _</span>
<span class="nc" id="L119">    private var sft: SimpleFeatureType = _</span>
<span class="nc" id="L120">    private var wrapper: FeatureWrapper[WritableFeature] = _</span>
<span class="nc" id="L121">    private var partitioner: Option[TablePartition]  = _</span>
<span class="nc" id="L122">    private var writers: Seq[(GeoMesaFeatureIndex[_, _], WriteConverter[_])] = _</span>

<span class="nc" id="L124">    private val visCache = new VisibilityCache()</span>
<span class="nc" id="L125">    private val tableAndKey = new TableAndKey(new Text(), null)</span>

<span class="nc" id="L127">    private var features: Counter = _</span>
<span class="nc" id="L128">    private var entries: Counter = _</span>
<span class="nc" id="L129">    private var failed: Counter = _</span>

    override def setup(context: MapContext): Unit = {
<span class="nc" id="L132">      val params = GeoMesaConfigurator.getDataStoreOutParams(context.getConfiguration).asJava</span>
<span class="nc" id="L133">      ds = DataStoreFinder.getDataStore(params).asInstanceOf[AccumuloDataStore]</span>
<span class="nc bnc" id="L134" title="All 2 branches missed.">      require(ds != null, &quot;Could not find data store - check your configuration and hbase-site.xml&quot;)</span>
<span class="nc" id="L135">      sft = ds.getSchema(Configurator.getTypeName(context.getConfiguration))</span>
<span class="nc bnc" id="L136" title="All 2 branches missed.">      require(sft != null, &quot;Could not find schema - check your configuration&quot;)</span>

<span class="nc" id="L138">      val indexIds = GeoMesaConfigurator.getIndicesOut(context.getConfiguration).orNull</span>
<span class="nc bnc" id="L139" title="All 2 branches missed.">      require(indexIds != null, &quot;Indices to write was not set in the job configuration&quot;)</span>
<span class="nc" id="L140">      val indices = indexIds.map(ds.manager.index(sft, _, IndexMode.Write))</span>
<span class="nc" id="L141">      wrapper = WritableFeature.wrapper(sft, ds.adapter.groups)</span>
<span class="nc" id="L142">      partitioner = TablePartition(ds, sft)</span>
<span class="nc" id="L143">      writers = indices.map(i =&gt; (i, i.createConverter()))</span>

<span class="nc" id="L145">      features = context.getCounter(OutputCounters.Group, OutputCounters.Written)</span>
<span class="nc" id="L146">      entries = context.getCounter(OutputCounters.Group, &quot;entries&quot;)</span>
<span class="nc" id="L147">      failed = context.getCounter(OutputCounters.Group, OutputCounters.Failed)</span>
    }

<span class="nc bnc" id="L150" title="All 2 branches missed.">    override def cleanup(context: MapContext): Unit = if (ds != null) { ds.dispose() }</span>

    override def map(key: Writable, value: SimpleFeature, context: MapContext): Unit = {
<span class="nc" id="L153">      try {</span>
<span class="nc" id="L154">        val feature = wrapper.wrap(value)</span>
<span class="nc" id="L155">        val partition = partitioner.map(_.partition(value))</span>
<span class="nc bnc" id="L156" title="All 2 branches missed.">        writers.foreach { case (index, writer) =&gt;</span>
<span class="nc" id="L157">          tableAndKey.getTable.set(index.getTableName(partition))</span>

<span class="nc" id="L159">          writer.convert(feature) match {</span>
<span class="nc bnc" id="L160" title="All 2 branches missed.">            case kv: SingleRowKeyValue[_] =&gt;</span>
<span class="nc" id="L161">              kv.values.foreach { value =&gt;</span>
<span class="nc" id="L162">                tableAndKey.setKey(new Key(kv.row, value.cf, value.cq, visCache(value.vis), Long.MaxValue))</span>
<span class="nc" id="L163">                context.write(tableAndKey, new Value(value.value))</span>
<span class="nc" id="L164">                entries.increment(1L)</span>
              }

<span class="nc bnc" id="L167" title="All 2 branches missed.">            case mkv: MultiRowKeyValue[_] =&gt;</span>
<span class="nc" id="L168">              mkv.rows.foreach { row =&gt;</span>
<span class="nc" id="L169">                mkv.values.foreach { value =&gt;</span>
<span class="nc" id="L170">                  tableAndKey.setKey(new Key(row, value.cf, value.cq, visCache(value.vis), Long.MaxValue))</span>
<span class="nc" id="L171">                  context.write(tableAndKey, new Value(value.value))</span>
<span class="nc" id="L172">                  entries.increment(1L)</span>
                }
              }
          }
        }

<span class="nc" id="L178">        features.increment(1L)</span>
      } catch {
<span class="nc bnc" id="L180" title="All 2 branches missed.">        case NonFatal(e) =&gt;</span>
<span class="nc bnc" id="L181" title="All 2 branches missed.">          logger.error(s&quot;Error writing feature ${Option(value).orNull}&quot;, e)</span>
<span class="nc" id="L182">          failed.increment(1L)</span>
      }
    }
  }

<span class="nc" id="L187">  class AccumuloFileReducer extends Reducer[TableAndKey, Value, Key, Value] {</span>

    type ReducerContext = Reducer[TableAndKey, Value, Key, Value]#Context

<span class="nc" id="L191">    private var id: String = _</span>
<span class="nc" id="L192">    private var out: MultipleOutputs[Key, Value] = _</span>

    override def setup(context: ReducerContext): Unit = {
<span class="nc" id="L195">      id = context.getJobID.appendTo(new java.lang.StringBuilder(&quot;gm&quot;)).toString</span>
<span class="nc" id="L196">      out = new MultipleOutputs(context)</span>
    }
<span class="nc bnc" id="L198" title="All 2 branches missed.">    override def cleanup(context: ReducerContext): Unit = if (out != null) { out.close() }</span>

    override def reduce(key: TableAndKey, values: java.lang.Iterable[Value], context: ReducerContext): Unit = {
<span class="nc" id="L201">      val path = s&quot;${key.getTable}/$id&quot;</span>
<span class="nc" id="L202">      val iter = values.iterator()</span>
<span class="nc bnc" id="L203" title="All 2 branches missed.">      while (iter.hasNext) {</span>
<span class="nc" id="L204">        out.write(key.getKey, iter.next, path)</span>
      }
    }
  }
}

</pre><div class="footer"><span class="right">Created with <a href="http://www.jacoco.org/jacoco">JaCoCo</a> 0.8.14.202510111229</span></div></body></html>