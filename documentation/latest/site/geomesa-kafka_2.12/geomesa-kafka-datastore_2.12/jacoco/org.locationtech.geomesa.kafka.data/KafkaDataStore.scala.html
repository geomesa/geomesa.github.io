<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml" lang=""><head><meta http-equiv="Content-Type" content="text/html;charset=UTF-8"/><link rel="stylesheet" href="../jacoco-resources/report.css" type="text/css"/><link rel="shortcut icon" href="../jacoco-resources/report.gif" type="image/gif"/><title>KafkaDataStore.scala</title><link rel="stylesheet" href="../jacoco-resources/prettify.css" type="text/css"/><script type="text/javascript" src="../jacoco-resources/prettify.js"></script></head><body onload="window['PR_TAB_WIDTH']=4;prettyPrint()"><div class="breadcrumb" id="breadcrumb"><span class="info"><a href="../jacoco-sessions.html" class="el_session">Sessions</a></span><a href="../index.html" class="el_report">GeoMesa Kafka Datastore</a> &gt; <a href="index.source.html" class="el_package">org.locationtech.geomesa.kafka.data</a> &gt; <span class="el_source">KafkaDataStore.scala</span></div><h1>KafkaDataStore.scala</h1><pre class="source lang-java linenums">/***********************************************************************
 * Copyright (c) 2013-2025 General Atomics Integrated Intelligence, Inc.
 * All rights reserved. This program and the accompanying materials
 * are made available under the terms of the Apache License, Version 2.0
 * which accompanies this distribution and is available at
 * https://www.apache.org/licenses/LICENSE-2.0
 ***********************************************************************/

package org.locationtech.geomesa.kafka.data

import com.github.benmanes.caffeine.cache.{CacheLoader, Caffeine, Ticker}
import com.typesafe.scalalogging.LazyLogging
import org.apache.kafka.clients.admin.NewTopic
import org.apache.kafka.clients.consumer.ConsumerConfig.GROUP_ID_CONFIG
import org.apache.kafka.clients.consumer.{Consumer, KafkaConsumer}
import org.apache.kafka.clients.producer.ProducerConfig.{ACKS_CONFIG, PARTITIONER_CLASS_CONFIG}
import org.apache.kafka.clients.producer.{KafkaProducer, Producer}
import org.apache.kafka.common.serialization.{ByteArrayDeserializer, ByteArraySerializer}
import org.geotools.api.data.{Query, SimpleFeatureStore, Transaction}
import org.geotools.api.feature.simple.SimpleFeatureType
import org.geotools.api.filter.Filter
import org.locationtech.geomesa.features.SerializationOption
import org.locationtech.geomesa.features.avro.serialization.AvroSerialization
import org.locationtech.geomesa.filter.factory.FastFilterFactory
import org.locationtech.geomesa.index.FlushableFeatureWriter
import org.locationtech.geomesa.index.audit.AuditWriter
import org.locationtech.geomesa.index.geotools.GeoMesaDataStoreFactory.{MetricsConfig, NamespaceConfig}
import org.locationtech.geomesa.index.geotools.{GeoMesaFeatureReader, MetadataBackedDataStore}
import org.locationtech.geomesa.index.metadata.GeoMesaMetadata
import org.locationtech.geomesa.index.stats.{GeoMesaStats, HasGeoMesaStats, RunnableStats}
import org.locationtech.geomesa.index.utils.DistributedLocking.LocalLocking
import org.locationtech.geomesa.index.zk.ZookeeperLocking
import org.locationtech.geomesa.kafka.consumer.ThreadedConsumer.ConsumerErrorHandler
import org.locationtech.geomesa.kafka.data.KafkaCacheLoader.KafkaCacheLoaderImpl
import org.locationtech.geomesa.kafka.data.KafkaDataStore.KafkaDataStoreConfig
import org.locationtech.geomesa.kafka.data.KafkaFeatureWriter._
import org.locationtech.geomesa.kafka.index._
import org.locationtech.geomesa.kafka.utils.GeoMessageProcessor.GeoMessageConsumer
import org.locationtech.geomesa.kafka.utils.GeoMessageSerializer.{GeoMessagePartitioner, GeoMessageSerializerFactory}
import org.locationtech.geomesa.kafka.utils.{GeoMessageProcessor, GeoMessageSerializer}
import org.locationtech.geomesa.kafka.versions.KafkaConsumerVersions
import org.locationtech.geomesa.memory.cqengine.utils.CQIndexType.CQIndexType
import org.locationtech.geomesa.security.AuthorizationsProvider
import org.locationtech.geomesa.utils.conf.GeoMesaSystemProperties.SystemProperty
import org.locationtech.geomesa.utils.geotools.SimpleFeatureTypes.Configs.TableSharing
import org.locationtech.geomesa.utils.geotools.SimpleFeatureTypes.InternalConfigs.TableSharingPrefix
import org.locationtech.geomesa.utils.geotools.Transform.Transforms
import org.locationtech.geomesa.utils.geotools.{SimpleFeatureTypes, Transform}
import org.locationtech.geomesa.utils.io.CloseWithLogging

import java.io.{Closeable, IOException, StringReader}
import java.util.concurrent.{ConcurrentHashMap, ScheduledExecutorService}
import java.util.{Collections, Properties, UUID}
import scala.concurrent.duration.{Duration, FiniteDuration}
import scala.util.control.NonFatal

<span class="nc" id="L57">class KafkaDataStore(</span>
<span class="nc" id="L58">    val config: KafkaDataStoreConfig,</span>
<span class="nc" id="L59">    val metadata: GeoMesaMetadata[String],</span>
<span class="nc" id="L60">    private[kafka] val serialization: GeoMessageSerializerFactory</span>
<span class="nc" id="L61">  ) extends MetadataBackedDataStore(config) with HasGeoMesaStats with LocalLocking {</span>

  import KafkaDataStore.TopicKey
  import org.apache.kafka.clients.producer.ProducerConfig.TRANSACTIONAL_ID_CONFIG
  import org.locationtech.geomesa.utils.geotools.RichSimpleFeatureType.RichSimpleFeatureType

  import scala.collection.JavaConverters._

<span class="nc" id="L69">  override val stats: GeoMesaStats = new RunnableStats(this)</span>

<span class="nc" id="L71">  private val registry = config.metricsRegistry.map(_.register())</span>

  // note: sharing a single producer is generally faster
  // https://kafka.apache.org/39/javadoc/org/apache/kafka/clients/producer/KafkaProducer.html

  // only instantiate the producer if needed
<span class="nc" id="L77">  private val defaultProducer = new LazyProducer(KafkaDataStore.producer(config.brokers, config.producers.properties))</span>
  // noinspection ScalaDeprecation
<span class="nc" id="L79">  private val partitionedProducer = new LazyProducer(KafkaDataStore.producer(config))</span>

  // view type name -&gt; actual type name
<span class="nc" id="L82">  private val layerViewLookup =</span>
<span class="nc bnc" id="L83" title="All 2 branches missed.">    config.layerViewsConfig.flatMap { case (typeName, views) =&gt; views.map(_.typeName -&gt; typeName).toMap }</span>

<span class="nc" id="L85">  private val cleared = Collections.newSetFromMap(new ConcurrentHashMap[String, java.lang.Boolean]())</span>

<span class="nc bnc" id="L87" title="All 2 branches missed.">  private val caches = Caffeine.newBuilder().build[String, KafkaCacheLoader](new CacheLoader[String, KafkaCacheLoader] {</span>
    override def load(key: String): KafkaCacheLoader = {
<span class="nc bnc" id="L89" title="All 2 branches missed.">      if (config.consumers.count &lt; 1) {</span>
<span class="nc bnc" id="L90" title="All 2 branches missed.">        logger.info(&quot;Kafka consumers disabled for this data store instance&quot;)</span>
<span class="nc" id="L91">        KafkaCacheLoader.NoOpLoader</span>
      } else {
<span class="nc" id="L93">        val sft = KafkaDataStore.super.getSchema(key)</span>
<span class="nc" id="L94">        val views = config.layerViewsConfig.getOrElse(key, Seq.empty).map(KafkaDataStore.createLayerView(sft, _))</span>
<span class="nc" id="L95">        val tags = KafkaDataStore.this.tags(sft.getTypeName)</span>
        // if the expiry is zero, this will return a NoOpFeatureCache
<span class="nc" id="L97">        val cache = KafkaFeatureCache(sft, config.indices, views, tags)</span>
<span class="nc" id="L98">        val topic = KafkaDataStore.topic(sft)</span>
<span class="nc" id="L99">        val consumers = KafkaDataStore.consumers(config.brokers, topic, config.consumers)</span>
<span class="nc" id="L100">        val frequency = java.time.Duration.ofMillis(KafkaDataStore.LoadIntervalProperty.toDuration.get.toMillis)</span>
<span class="nc" id="L101">        val serializer = serialization.apply(sft)</span>
<span class="nc" id="L102">        val readBack = config.consumers.readBack</span>
<span class="nc" id="L103">        val expiry = config.indices.expiry</span>
<span class="nc" id="L104">        val offsetCommitInterval = config.consumers.offsetCommitInterval</span>
<span class="nc" id="L105">        val loader = new KafkaCacheLoaderImpl(sft, cache, consumers, topic, frequency, offsetCommitInterval, serializer, readBack, expiry, tags)</span>
<span class="nc" id="L106">        try { loader.start() } catch {</span>
<span class="nc bnc" id="L107" title="All 2 branches missed.">          case NonFatal(e) =&gt;</span>
<span class="nc" id="L108">            CloseWithLogging(loader)</span>
<span class="nc" id="L109">            throw e</span>
        }
<span class="nc" id="L111">        loader</span>
      }
    }
  })

<span class="nc" id="L116">  private val runner = new KafkaQueryRunner(this, cache)</span>

  /**
    * Start consuming from all topics. Consumers are normally only started for a simple feature type
    * when it is first queried - this will start them immediately.
    */
<span class="nc" id="L122">  def startAllConsumers(): Unit = super.getTypeNames.foreach(caches.get)</span>

  /**
   * Create a message consumer for the given feature type. This can be used for guaranteed at-least-once
   * message processing
   *
   * @param typeName type name
   * @param groupId consumer group id
   * @param processor message processor
   * @return
   */
  def createConsumer(typeName: String, groupId: String, processor: GeoMessageProcessor): Closeable =
<span class="nc" id="L134">    createConsumer(typeName, groupId, processor, None)</span>

  /**
   * Create a message consumer for the given feature type. This can be used for guaranteed at-least-once
   * message processing
   *
   * @param typeName type name
   * @param groupId consumer group id
   * @param processor message processor
   * @param errorHandler error handler
   * @return
   */
  def createConsumer(
      typeName: String,
      groupId: String,
      processor: GeoMessageProcessor,
      errorHandler: Option[ConsumerErrorHandler]): Closeable = {
<span class="nc" id="L151">    val sft = getSchema(typeName)</span>
<span class="nc bnc" id="L152" title="All 2 branches missed.">    if (sft == null) {</span>
<span class="nc" id="L153">      throw new IllegalArgumentException(s&quot;Schema '$typeName' does not exist; call `createSchema` first&quot;)</span>
    }
<span class="nc" id="L155">    val topic = KafkaDataStore.topic(sft)</span>
    val consumers =
<span class="nc" id="L157">      KafkaDataStore.consumers(config.brokers, topic,</span>
<span class="nc" id="L158">        config.consumers.copy(properties = config.consumers.properties + (GROUP_ID_CONFIG -&gt; groupId))) // add group id</span>
<span class="nc" id="L159">    consumers.foreach(KafkaConsumerVersions.subscribe(_, topic))</span>
<span class="nc" id="L160">    val frequency = java.time.Duration.ofMillis(KafkaDataStore.LoadIntervalProperty.toDuration.get.toMillis)</span>
<span class="nc" id="L161">    val serializer = serialization.apply(sft)</span>
<span class="nc" id="L162">    val consumer = new GeoMessageConsumer(consumers, frequency, serializer, processor)</span>
<span class="nc" id="L163">    consumer.startConsumers(errorHandler)</span>
<span class="nc" id="L164">    consumer</span>
  }

  override def getSchema(typeName: String): SimpleFeatureType = {
<span class="nc" id="L168">    layerViewLookup.get(typeName) match {</span>
<span class="nc bnc" id="L169" title="All 2 branches missed.">      case None =&gt; super.getSchema(typeName)</span>
<span class="nc bnc" id="L170" title="All 2 branches missed.">      case Some(orig) =&gt;</span>
<span class="nc" id="L171">        val parent = super.getSchema(orig)</span>
<span class="nc bnc" id="L172" title="All 2 branches missed.">        if (parent == null) {</span>
<span class="nc bnc" id="L173" title="All 2 branches missed.">          logger.warn(s&quot;Backing schema '$orig' for configured layer view '$typeName' does not exist&quot;)</span>
<span class="nc" id="L174">          null</span>
        } else {
<span class="nc bnc" id="L176" title="All 6 branches missed.">          val view = config.layerViewsConfig.get(orig).flatMap(_.find(_.typeName == typeName)).getOrElse {</span>
            // this should be impossible since we created the lookup from the view config
<span class="nc" id="L178">            throw new IllegalStateException(&quot;Inconsistent layer view config&quot;)</span>
          }
<span class="nc" id="L180">          KafkaDataStore.createLayerView(parent, view).viewSft</span>
        }
    }
  }

  override def getTypeNames: Array[String] = {
<span class="nc" id="L186">    val nonViews = super.getTypeNames</span>
<span class="nc bnc" id="L187" title="All 2 branches missed.">    nonViews ++ layerViewLookup.toArray.flatMap { case (k, v) =&gt;</span>
<span class="nc bnc" id="L188" title="All 2 branches missed.">      if (nonViews.contains(v)) {</span>
<span class="nc" id="L189">        Some(k)</span>
      } else {
<span class="nc bnc" id="L191" title="All 2 branches missed.">        logger.warn(s&quot;Backing schema '$v' for configured layer view '$k' does not exist&quot;)</span>
<span class="nc" id="L192">        None</span>
      }
    }
  }

  @throws(classOf[IllegalArgumentException])
  override protected def preSchemaCreate(sft: SimpleFeatureType): Unit = {
    // note: kafka doesn't allow slashes in topic names
<span class="nc" id="L200">    KafkaDataStore.topic(sft) match {</span>
<span class="nc bnc" id="L201" title="All 2 branches missed.">      case null  =&gt; KafkaDataStore.setTopic(sft, s&quot;${config.catalog}-${sft.getTypeName}&quot;.replaceAll(&quot;/&quot;, &quot;-&quot;))</span>
<span class="nc bnc" id="L202" title="All 2 branches missed.">      case topic if topic.contains(&quot;/&quot;) =&gt; throw new IllegalArgumentException(s&quot;Topic cannot contain '/': $topic&quot;)</span>
<span class="nc bnc" id="L203" title="All 2 branches missed.">      case topic =&gt; logger.debug(s&quot;Using user-defined topic [$topic]&quot;)</span>
    }
    // disable our custom partitioner by default, as it messes with Kafka streams co-partition joining
    // and it's not required since we switched our keys to be feature ids
<span class="nc bnc" id="L207" title="All 2 branches missed.">    if (!sft.getUserData.containsKey(KafkaDataStore.PartitioningKey)) {</span>
<span class="nc" id="L208">      sft.getUserData.put(KafkaDataStore.PartitioningKey, KafkaDataStore.PartitioningDefault)</span>
    }
    // remove table sharing as it's not relevant
<span class="nc" id="L211">    sft.getUserData.remove(TableSharing)</span>
<span class="nc" id="L212">    sft.getUserData.remove(TableSharingPrefix)</span>
  }

  // create kafka topic
  override protected def onSchemaCreated(sft: SimpleFeatureType): Unit = {
<span class="nc" id="L217">    val topic = KafkaDataStore.topic(sft)</span>
<span class="nc" id="L218">    adminClientOp(config) { admin =&gt;</span>
<span class="nc bnc" id="L219" title="All 2 branches missed.">      if (admin.listTopics().names().get.contains(topic)) {</span>
<span class="nc bnc" id="L220" title="All 2 branches missed.">        logger.warn(</span>
<span class="nc" id="L221">          s&quot;Topic [$topic] already exists - it may contain invalid data and/or not &quot; +</span>
<span class="nc" id="L222">              &quot;match the expected topic configuration&quot;)</span>
      } else {
        val newTopic =
<span class="nc" id="L225">          new NewTopic(topic, config.topics.partitions, config.topics.replication.toShort)</span>
<span class="nc" id="L226">              .configs(KafkaDataStore.topicConfig(sft))</span>
<span class="nc" id="L227">        admin.createTopics(Collections.singletonList(newTopic)).all().get</span>
      }
    }
<span class="nc" id="L230">    metadata.insert(sft.getTypeName,</span>
<span class="nc bnc" id="L231" title="All 2 branches missed.">      Map(&quot;avro-schema-native&quot; -&gt; Set(SerializationOption.NativeCollections), &quot;avro-schema&quot; -&gt; Set.empty).map { case (k, opts) =&gt;</span>
<span class="nc" id="L232">        k -&gt; AvroSerialization(sft, GeoMessageSerializer.DefaultOpts ++ opts).schema.toString()</span>
      }
    )
  }

  @throws(classOf[IllegalArgumentException])
  override protected def preSchemaUpdate(sft: SimpleFeatureType, previous: SimpleFeatureType): Unit = {
<span class="nc" id="L239">    requireNotLayerView(sft.getTypeName)</span>
<span class="nc" id="L240">    val topic = KafkaDataStore.topic(sft)</span>
<span class="nc bnc" id="L241" title="All 2 branches missed.">    if (topic == null) {</span>
<span class="nc" id="L242">      throw new IllegalArgumentException(s&quot;Topic must be defined in user data under '$TopicKey'&quot;)</span>
<span class="nc bnc" id="L243" title="All 6 branches missed.">    } else if (topic != KafkaDataStore.topic(previous)) {</span>
<span class="nc bnc" id="L244" title="All 2 branches missed.">      if (topic.contains(&quot;/&quot;)) {</span>
<span class="nc" id="L245">        throw new IllegalArgumentException(s&quot;Topic cannot contain '/': $topic&quot;)</span>
      }
<span class="nc" id="L247">      onSchemaDeleted(previous)</span>
<span class="nc" id="L248">      onSchemaCreated(sft)</span>
    }
  }

  // invalidate any cached consumers in order to reload the new schema
  override protected def onSchemaUpdated(sft: SimpleFeatureType, previous: SimpleFeatureType): Unit =
<span class="nc" id="L254">    closeCache(sft.getTypeName)</span>

  // stop consumers and delete kafka topic
  override protected def onSchemaDeleted(sft: SimpleFeatureType): Unit = {
<span class="nc" id="L258">    requireNotLayerView(sft.getTypeName)</span>
<span class="nc" id="L259">    closeCache(sft.getTypeName)</span>
<span class="nc" id="L260">    val topic = KafkaDataStore.topic(sft)</span>
<span class="nc" id="L261">    adminClientOp(config) { admin =&gt;</span>
<span class="nc bnc" id="L262" title="All 2 branches missed.">      if (admin.listTopics().names().get.contains(topic)) {</span>
<span class="nc bnc" id="L263" title="All 2 branches missed.">        if (config.onSchemaDeleteTruncate) {</span>
<span class="nc bnc" id="L264" title="All 2 branches missed.">          logger.debug(s&quot;Truncating topic $topic&quot;)</span>
<span class="nc" id="L265">          TruncateTopic.apply(admin, topic)</span>
        } else {
<span class="nc bnc" id="L267" title="All 2 branches missed.">          logger.debug(s&quot;Deleting topic $topic&quot;)</span>
<span class="nc" id="L268">          admin.deleteTopics(Collections.singletonList(topic)).all().get</span>
        }
      } else {
<span class="nc bnc" id="L271" title="All 2 branches missed.">        logger.warn(s&quot;Topic [$topic] does not exist, can't delete it&quot;)</span>
      }
    }
  }

  /**
    * @see org.geotools.api.data.DataStore#getFeatureSource(org.geotools.api.feature.type.Name)
    * @param typeName simple feature type name
    * @return featureStore, suitable for reading and writing
    */
  override def getFeatureSource(typeName: String): SimpleFeatureStore = {
<span class="nc" id="L282">    val sft = getSchema(typeName)</span>
<span class="nc bnc" id="L283" title="All 2 branches missed.">    if (sft == null) {</span>
<span class="nc" id="L284">      throw new IOException(s&quot;Schema '$typeName' has not been initialized. Please call 'createSchema' first.&quot;)</span>
    }
<span class="nc" id="L286">    new KafkaFeatureStore(this, sft, cache(typeName))</span>
  }

  override private[geomesa] def getFeatureReader(
      sft: SimpleFeatureType,
      transaction: Transaction,
      query: Query): GeoMesaFeatureReader = {
    // kick off the kafka consumers for this sft, if not already started
<span class="nc" id="L294">    caches.get(layerViewLookup.getOrElse(query.getTypeName, query.getTypeName))</span>
<span class="nc" id="L295">    GeoMesaFeatureReader(sft, query, runner, config.audit)</span>
  }

  override private[geomesa] def getFeatureWriter(
      sft: SimpleFeatureType,
      transaction: Transaction,
      filter: Option[Filter]): FlushableFeatureWriter = {
<span class="nc" id="L302">    requireNotLayerView(sft.getTypeName)</span>
<span class="nc" id="L303">    val producer = getTransactionalProducer(sft, transaction)</span>
<span class="nc" id="L304">    val vis = sft.isVisibilityRequired</span>
<span class="nc" id="L305">    val serializer = serialization.apply(sft)</span>
<span class="nc" id="L306">    val tags = this.tags(sft.getTypeName)</span>
<span class="nc" id="L307">    val writer = filter match {</span>
<span class="nc bnc" id="L308" title="All 4 branches missed.">      case None if vis    =&gt; new AppendKafkaFeatureWriter(sft, producer, serializer, tags) with RequiredVisibilityWriter</span>
<span class="nc bnc" id="L309" title="All 2 branches missed.">      case None           =&gt; new AppendKafkaFeatureWriter(sft, producer, serializer, tags)</span>
<span class="nc bnc" id="L310" title="All 4 branches missed.">      case Some(f) if vis =&gt; new ModifyKafkaFeatureWriter(sft, producer, serializer, tags, f) with RequiredVisibilityWriter</span>
<span class="nc bnc" id="L311" title="All 2 branches missed.">      case Some(f)        =&gt; new ModifyKafkaFeatureWriter(sft, producer, serializer, tags, f)</span>
    }
<span class="nc bnc" id="L313" title="All 4 branches missed.">    if (config.clearOnStart &amp;&amp; cleared.add(sft.getTypeName)) {</span>
<span class="nc" id="L314">      writer.clear()</span>
    }
<span class="nc" id="L316">    writer</span>
  }

  override def dispose(): Unit = {
<span class="nc" id="L320">    CloseWithLogging(defaultProducer)</span>
<span class="nc" id="L321">    CloseWithLogging(partitionedProducer)</span>
<span class="nc" id="L322">    CloseWithLogging(caches.asMap.asScala.values)</span>
<span class="nc" id="L323">    CloseWithLogging(registry)</span>
<span class="nc" id="L324">    caches.invalidateAll()</span>
<span class="nc" id="L325">    super.dispose()</span>
  }

  private def getTransactionalProducer(sft: SimpleFeatureType, transaction: Transaction): KafkaFeatureProducer = {
<span class="nc" id="L329">    val useDefaultPartitioning = KafkaDataStore.usesDefaultPartitioning(sft)</span>

<span class="nc bnc" id="L331" title="All 8 branches missed.">    if (transaction == null || transaction == Transaction.AUTO_COMMIT) {</span>
<span class="nc bnc" id="L332" title="All 2 branches missed.">      val producer = if (useDefaultPartitioning) { defaultProducer.instance } else { partitionedProducer.instance }</span>
<span class="nc" id="L333">      return AutoCommitProducer(producer)</span>
    }

<span class="nc" id="L336">    val state = transaction.getState(KafkaDataStore.TransactionStateKey)</span>
<span class="nc bnc" id="L337" title="All 2 branches missed.">    if (state == null) {</span>
<span class="nc bnc" id="L338" title="All 2 branches missed.">      val partitioner = if (useDefaultPartitioning) { Map.empty } else {</span>
<span class="nc" id="L339">        Map(PARTITIONER_CLASS_CONFIG -&gt; classOf[GeoMessagePartitioner].getName)</span>
      }
      // add kafka transactional id if it's not set, but force acks to &quot;all&quot; as required by kafka
      val props =
<span class="nc" id="L343">        Map(TRANSACTIONAL_ID_CONFIG -&gt; UUID.randomUUID().toString) ++</span>
<span class="nc" id="L344">            partitioner ++</span>
<span class="nc" id="L345">            config.producers.properties ++</span>
<span class="nc" id="L346">            Map(ACKS_CONFIG -&gt; &quot;all&quot;)</span>
<span class="nc" id="L347">      val producer = KafkaTransactionState(KafkaDataStore.producer(config.brokers, props))</span>
<span class="nc" id="L348">      transaction.putState(KafkaDataStore.TransactionStateKey, producer)</span>
<span class="nc" id="L349">      producer</span>
    } else {
<span class="nc" id="L351">      state match {</span>
<span class="nc bnc" id="L352" title="All 2 branches missed.">        case p: KafkaTransactionState =&gt; p</span>
<span class="nc" id="L353">        case _ =&gt; throw new IllegalArgumentException(s&quot;Found non-kafka state in transaction: $state&quot;)</span>
      }
    }
  }

  /**
   * Get the feature cache for the type name, which may be a real feature type or a view
   *
   * @param typeName type name
   * @return
   */
  private def cache(typeName: String): KafkaFeatureCache = {
<span class="nc" id="L365">    layerViewLookup.get(typeName) match {</span>
<span class="nc bnc" id="L366" title="All 2 branches missed.">      case None =&gt; caches.get(typeName).cache</span>
<span class="nc bnc" id="L367" title="All 2 branches missed.">      case Some(orig) =&gt;</span>
<span class="nc bnc" id="L368" title="All 6 branches missed.">        caches.get(orig).cache.views.find(_.sft.getTypeName == typeName).getOrElse {</span>
<span class="nc" id="L369">          throw new IllegalStateException(</span>
<span class="nc" id="L370">            s&quot;Could not find layer view for typeName '$typeName' in cache ${caches.get(orig)}&quot;)</span>
        }
    }
  }

  private def closeCache(typeName: String): Unit = {
<span class="nc" id="L376">    Option(caches.getIfPresent(typeName)).foreach { cache =&gt;</span>
<span class="nc" id="L377">      cache.close()</span>
<span class="nc" id="L378">      caches.invalidate(typeName)</span>
    }
  }

  @throws[IllegalArgumentException]
  private def requireNotLayerView(typeName: String): Unit = {
<span class="nc bnc" id="L384" title="All 2 branches missed.">    if (layerViewLookup.contains(typeName)) {</span>
<span class="nc" id="L385">      throw new IllegalArgumentException(s&quot;Schema '$typeName' is a read-only view of '${layerViewLookup(typeName)}'&quot;)</span>
    }
  }
}

<span class="nc bnc" id="L390" title="All 4 branches missed.">object KafkaDataStore extends LazyLogging {</span>

<span class="nc" id="L392">  val TopicKey = &quot;geomesa.kafka.topic&quot;</span>
<span class="nc" id="L393">  val TopicConfigKey = &quot;kafka.topic.config&quot;</span>
<span class="nc" id="L394">  val PartitioningKey = &quot;geomesa.kafka.partitioning&quot;</span>

<span class="nc" id="L396">  val MetadataPath = &quot;metadata&quot;</span>

<span class="nc" id="L398">  val TransactionStateKey = &quot;geomesa.kafka.state&quot;</span>

<span class="nc" id="L400">  val PartitioningDefault = &quot;default&quot;</span>

<span class="nc" id="L402">  val MetricsPrefix = &quot;geomesa.kafka&quot;</span>

<span class="nc" id="L404">  val LoadIntervalProperty: SystemProperty = SystemProperty(&quot;geomesa.kafka.load.interval&quot;, &quot;1s&quot;)</span>

  // marker to trigger the cq engine index when using the deprecated enable flag
<span class="nc" id="L407">  private[kafka] val CqIndexFlag: (String, CQIndexType) = null</span>

<span class="nc" id="L409">  def topic(sft: SimpleFeatureType): String = sft.getUserData.get(TopicKey).asInstanceOf[String]</span>

<span class="nc" id="L411">  def setTopic(sft: SimpleFeatureType, topic: String): Unit = sft.getUserData.put(TopicKey, topic)</span>

  def topicConfig(sft: SimpleFeatureType): java.util.Map[String, String] = {
<span class="nc" id="L414">    val props = new Properties()</span>
<span class="nc" id="L415">    val config = sft.getUserData.get(TopicConfigKey).asInstanceOf[String]</span>
<span class="nc bnc" id="L416" title="All 2 branches missed.">    if (config != null) {</span>
<span class="nc" id="L417">      props.load(new StringReader(config))</span>
    }
<span class="nc" id="L419">    props.asInstanceOf[java.util.Map[String, String]]</span>
  }

  def usesDefaultPartitioning(sft: SimpleFeatureType): Boolean =
<span class="nc bnc" id="L423" title="All 6 branches missed.">    sft.getUserData.get(PartitioningKey) == PartitioningDefault</span>

  // kept for back-compatibility - uses a custom partitioner which creates issues with Kafka streams
  private def producer(config: KafkaDataStoreConfig): Producer[Array[Byte], Array[Byte]] = {
    val props =
<span class="nc bnc" id="L428" title="All 2 branches missed.">      if (config.producers.properties.contains(PARTITIONER_CLASS_CONFIG)) {</span>
<span class="nc" id="L429">        config.producers.properties</span>
      } else {
<span class="nc" id="L431">        config.producers.properties + (PARTITIONER_CLASS_CONFIG -&gt; classOf[GeoMessagePartitioner].getName)</span>
      }
<span class="nc" id="L433">    producer(config.brokers, props)</span>
  }

  /**
   * Create a Kafka producer
   *
   * @param bootstrapServers Kafka bootstrap servers config
   * @param properties Kafka producer properties
   * @return
   */
  def producer(bootstrapServers: String, properties: Map[String, String]): Producer[Array[Byte], Array[Byte]] = {
    import org.apache.kafka.clients.producer.ProducerConfig._

<span class="nc" id="L446">    val props = new Properties()</span>
    // set some defaults but allow them to be overridden
<span class="nc" id="L448">    props.put(ACKS_CONFIG, &quot;1&quot;) // mix of reliability and performance</span>
<span class="nc" id="L449">    props.put(RETRIES_CONFIG, Int.box(3))</span>
<span class="nc" id="L450">    props.put(LINGER_MS_CONFIG, Int.box(3)) // helps improve batching at the expense of slight delays in write</span>
<span class="nc" id="L451">    props.put(KEY_SERIALIZER_CLASS_CONFIG, classOf[ByteArraySerializer].getName)</span>
<span class="nc" id="L452">    props.put(VALUE_SERIALIZER_CLASS_CONFIG, classOf[ByteArraySerializer].getName)</span>
<span class="nc" id="L453">    props.put(BOOTSTRAP_SERVERS_CONFIG, bootstrapServers)</span>
<span class="nc bnc" id="L454" title="All 2 branches missed.">    properties.foreach { case (k, v) =&gt; props.put(k, v) }</span>
<span class="nc" id="L455">    new KafkaProducer[Array[Byte], Array[Byte]](props)</span>
  }

  def consumer(config: KafkaDataStoreConfig, group: String): Consumer[Array[Byte], Array[Byte]] =
<span class="nc" id="L459">    consumer(config.brokers, Map(GROUP_ID_CONFIG -&gt; group) ++ config.consumers.properties)</span>

  def consumer(brokers: String, properties: Map[String, String]): Consumer[Array[Byte], Array[Byte]] = {
    import org.apache.kafka.clients.consumer.ConsumerConfig._

<span class="nc" id="L464">    val props = new Properties()</span>
<span class="nc" id="L465">    props.put(BOOTSTRAP_SERVERS_CONFIG, brokers)</span>
<span class="nc" id="L466">    props.put(ENABLE_AUTO_COMMIT_CONFIG, &quot;false&quot;)</span>
<span class="nc" id="L467">    props.put(KEY_DESERIALIZER_CLASS_CONFIG, classOf[ByteArrayDeserializer].getName)</span>
<span class="nc" id="L468">    props.put(VALUE_DESERIALIZER_CLASS_CONFIG, classOf[ByteArrayDeserializer].getName)</span>
<span class="nc bnc" id="L469" title="All 2 branches missed.">    properties.foreach { case (k, v) =&gt; props.put(k, v) }</span>

<span class="nc" id="L471">    new KafkaConsumer[Array[Byte], Array[Byte]](props)</span>
  }

  // creates a consumer and sets to the latest offsets
  private[kafka] def consumers(
      brokers: String,
      topic: String,
      config: ConsumerConfig): Seq[Consumer[Array[Byte], Array[Byte]]] = {
<span class="nc bnc" id="L479" title="All 2 branches missed.">    require(config.count &gt; 0, &quot;Number of consumers must be greater than 0&quot;)</span>
<span class="nc" id="L480">    val props = Map(GROUP_ID_CONFIG -&gt; s&quot;${config.groupPrefix}${UUID.randomUUID()}&quot;) ++ config.properties</span>
<span class="nc bnc" id="L481" title="All 2 branches missed.">    logger.debug(s&quot;Creating ${config.count} consumers for topic [$topic] with group-id [${props(GROUP_ID_CONFIG)}]&quot;)</span>
<span class="nc" id="L482">    Seq.fill(config.count)(KafkaDataStore.consumer(brokers, props))</span>
  }

  /**
   * Create a layer view based on a config and the actual feature type
   *
   * @param sft simple feature type the view is based on
   * @param config layer view config
   * @return
   */
  private[kafka] def createLayerView(sft: SimpleFeatureType, config: LayerViewConfig): LayerView = {
<span class="nc" id="L493">    val viewSft = SimpleFeatureTypes.renameSft(sft, config.typeName)</span>
<span class="nc" id="L494">    val filter = config.filter.map(FastFilterFactory.optimize(viewSft, _))</span>
<span class="nc" id="L495">    val transform = config.transform.map(Transforms(viewSft, _))</span>
<span class="nc" id="L496">    val finalSft = transform.map(Transforms.schema(viewSft, _)).getOrElse(viewSft)</span>
<span class="nc" id="L497">    LayerView(finalSft, filter, transform)</span>
  }

<span class="nc" id="L500">  class KafkaDataStoreWithZk(</span>
      config: KafkaDataStoreConfig,
      metadata: GeoMesaMetadata[String],
      serialization: GeoMessageSerializerFactory,
<span class="nc" id="L504">      override protected val zookeepers: String</span>
<span class="nc" id="L505">    ) extends KafkaDataStore(config, metadata, serialization) with ZookeeperLocking</span>

<span class="nc bnc" id="L507" title="All 110 branches missed.">  case class KafkaDataStoreConfig(</span>
<span class="nc" id="L508">      catalog: String,</span>
<span class="nc" id="L509">      brokers: String,</span>
<span class="nc" id="L510">      zookeepers: Option[String],</span>
<span class="nc" id="L511">      consumers: ConsumerConfig,</span>
<span class="nc" id="L512">      producers: ProducerConfig,</span>
<span class="nc" id="L513">      clearOnStart: Boolean,</span>
<span class="nc" id="L514">      onSchemaDeleteTruncate: Boolean,</span>
<span class="nc" id="L515">      topics: TopicConfig,</span>
<span class="nc" id="L516">      indices: IndexConfig,</span>
<span class="nc" id="L517">      looseBBox: Boolean,</span>
<span class="nc" id="L518">      layerViewsConfig: Map[String, Seq[LayerViewConfig]],</span>
<span class="nc" id="L519">      authProvider: AuthorizationsProvider,</span>
<span class="nc" id="L520">      audit: Option[AuditWriter],</span>
<span class="nc" id="L521">      metricsRegistry: Option[MetricsConfig],</span>
<span class="nc" id="L522">      namespace: Option[String]) extends NamespaceConfig</span>

<span class="nc bnc" id="L524" title="All 42 branches missed.">  case class ConsumerConfig(</span>
<span class="nc" id="L525">      count: Int,</span>
<span class="nc" id="L526">      groupPrefix: String,</span>
<span class="nc" id="L527">      properties: Map[String, String],</span>
<span class="nc" id="L528">      readBack: Option[Duration],</span>
<span class="nc" id="L529">      offsetCommitInterval: FiniteDuration,</span>
    )

<span class="nc bnc" id="L532" title="All 18 branches missed.">  case class ProducerConfig(properties: Map[String, String])</span>

<span class="nc bnc" id="L534" title="All 17 branches missed.">  case class TopicConfig(partitions: Int, replication: Int)</span>

<span class="nc bnc" id="L536" title="All 51 branches missed.">  case class IndexConfig(</span>
<span class="nc" id="L537">      expiry: ExpiryTimeConfig,</span>
<span class="nc" id="L538">      resolution: IndexResolution,</span>
<span class="nc" id="L539">      ssiTiers: Seq[(Double, Double)],</span>
<span class="nc" id="L540">      cqAttributes: Seq[(String, CQIndexType)],</span>
      @deprecated(&quot;unused&quot;)
<span class="nc" id="L542">      lazyDeserialization: Boolean,</span>
<span class="nc" id="L543">      executor: Option[(ScheduledExecutorService, Ticker)]</span>
    )

<span class="nc bnc" id="L546" title="All 17 branches missed.">  case class IndexResolution(x: Int, y: Int)</span>

  sealed trait ExpiryTimeConfig
<span class="nc" id="L549">  case object NeverExpireConfig extends ExpiryTimeConfig</span>
<span class="nc" id="L550">  case object ImmediatelyExpireConfig extends ExpiryTimeConfig</span>
<span class="nc bnc" id="L551" title="All 18 branches missed.">  case class IngestTimeConfig(expiry: Duration) extends ExpiryTimeConfig</span>
<span class="nc bnc" id="L552" title="All 30 branches missed.">  case class EventTimeConfig(expiry: Duration, expression: String, ordered: Boolean) extends ExpiryTimeConfig</span>
<span class="nc bnc" id="L553" title="All 18 branches missed.">  case class FilteredExpiryConfig(expiry: Seq[(String, ExpiryTimeConfig)]) extends ExpiryTimeConfig</span>

<span class="nc bnc" id="L555" title="All 32 branches missed.">  case class LayerViewConfig(typeName: String, filter: Option[Filter], transform: Option[Seq[String]])</span>
<span class="nc bnc" id="L556" title="All 32 branches missed.">  case class LayerView(viewSft: SimpleFeatureType, filter: Option[Filter], transform: Option[Seq[Transform]])</span>
}
</pre><div class="footer"><span class="right">Created with <a href="http://www.jacoco.org/jacoco">JaCoCo</a> 0.8.14.202510111229</span></div></body></html>