<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml" lang=""><head><meta http-equiv="Content-Type" content="text/html;charset=UTF-8"/><link rel="stylesheet" href="../jacoco-resources/report.css" type="text/css"/><link rel="shortcut icon" href="../jacoco-resources/report.gif" type="image/gif"/><title>ExportJob.scala</title><link rel="stylesheet" href="../jacoco-resources/prettify.css" type="text/css"/><script type="text/javascript" src="../jacoco-resources/prettify.js"></script></head><body onload="window['PR_TAB_WIDTH']=4;prettyPrint()"><div class="breadcrumb" id="breadcrumb"><span class="info"><a href="../jacoco-sessions.html" class="el_session">Sessions</a></span><a href="../index.html" class="el_report">GeoMesa Tools</a> &gt; <a href="index.source.html" class="el_package">org.locationtech.geomesa.tools.export</a> &gt; <span class="el_source">ExportJob.scala</span></div><h1>ExportJob.scala</h1><pre class="source lang-java linenums">/***********************************************************************
 * Copyright (c) 2013-2025 General Atomics Integrated Intelligence, Inc.
 * All rights reserved. This program and the accompanying materials
 * are made available under the terms of the Apache License, Version 2.0
 * which accompanies this distribution and is available at
 * https://www.apache.org/licenses/LICENSE-2.0
 ***********************************************************************/

package org.locationtech.geomesa.tools.`export`

import com.typesafe.scalalogging.{LazyLogging, StrictLogging}
import org.apache.hadoop.conf.Configuration
import org.apache.hadoop.fs.Path
import org.apache.hadoop.io.{Text, WritableComparable}
import org.apache.hadoop.mapred.InvalidJobConfException
import org.apache.hadoop.mapreduce._
import org.apache.hadoop.mapreduce.lib.output.{FileOutputCommitter, FileOutputFormat}
import org.apache.hadoop.mapreduce.lib.partition.InputSampler.{RandomSampler, Sampler, SplitSampler}
import org.apache.hadoop.mapreduce.lib.partition.{InputSampler, TotalOrderPartitioner}
import org.apache.hadoop.mapreduce.security.TokenCache
import org.geotools.api.feature.simple.{SimpleFeature, SimpleFeatureType}
import org.geotools.data.DataUtilities
import org.geotools.util.factory.Hints
import org.locationtech.geomesa.features.ScalaSimpleFeature
import org.locationtech.geomesa.features.exporters.FeatureExporter
import org.locationtech.geomesa.index.conf.QueryHints
import org.locationtech.geomesa.index.geoserver.ViewParams
import org.locationtech.geomesa.index.index.attribute.AttributeIndexKey
import org.locationtech.geomesa.jobs.GeoMesaConfigurator
import org.locationtech.geomesa.jobs.mapreduce.JobWithLibJars
import org.locationtech.geomesa.tools.export.ExportCommand.{ExportOptions, Exporter}
import org.locationtech.geomesa.utils.index.ByteArrays
import org.locationtech.geomesa.utils.io.{FileSizeEstimator, PathUtils}

import java.io.File
import java.text.NumberFormat
import java.util.UUID

/**
 * Class that handles configuration and tracking of the remote job
 */
<span class="nc" id="L42">object ExportJob extends JobWithLibJars {</span>

  // mimic id formatting in job
<span class="nc" id="L45">  private val jobIdFormat = createFormat(4)</span>
<span class="nc" id="L46">  private val taskIdFormat = createFormat(5)</span>

  def configure(
      job: Job,
      dsParams: Map[String, String],
      sft: SimpleFeatureType,
      hints: Hints,
      filename: String,
      output: Path,
      format: ExportFormat,
      headers: Boolean,
      chunks: Option[Long],
      gzip: Option[Int],
      reducers: Int,
      libjarsFiles: Seq[String],
      libjarsPaths: Iterator[() =&gt; Seq[File]]): Job = {

<span class="nc" id="L63">    val conf = job.getConfiguration</span>

<span class="nc" id="L65">    setLibJars(job, libjarsFiles, libjarsPaths)</span>
<span class="nc" id="L66">    conf.set(&quot;mapreduce.job.user.classpath.first&quot;, &quot;true&quot;)</span>

<span class="nc" id="L68">    job.setJarByClass(getClass)</span>
<span class="nc" id="L69">    job.setMapperClass(classOf[PassThroughMapper])</span>
<span class="nc" id="L70">    job.setOutputFormatClass(classOf[ExportOutputFormat])</span>
<span class="nc" id="L71">    job.setOutputKeyClass(classOf[Text])</span>
<span class="nc" id="L72">    job.setOutputValueClass(classOf[ScalaSimpleFeature])</span>

<span class="nc" id="L74">    FileOutputFormat.setOutputPath(job, output)</span>
<span class="nc" id="L75">    Config.setOutputFile(conf, filename)</span>
<span class="nc" id="L76">    Config.setFormat(conf, format)</span>
<span class="nc" id="L77">    Config.setHeaders(conf, headers)</span>
<span class="nc" id="L78">    chunks.foreach(Config.setChunks(conf, _))</span>
<span class="nc" id="L79">    gzip.foreach(Config.setGzip(conf, _))</span>
<span class="nc" id="L80">    Config.setQueryHints(conf, hints)</span>

    // note: we assume that the input format has set sorting and reduce

<span class="nc bnc" id="L84" title="All 2 branches missed.">    if (conf.get(GeoMesaConfigurator.Keys.FeatureReducer) != null) {</span>
<span class="nc" id="L85">      throw new IllegalArgumentException(&quot;Query requires a feature reduce step, which is not &quot; +</span>
          &quot;supported in this job&quot;)
    }

<span class="nc" id="L89">    GeoMesaConfigurator.getSorting(conf) match {</span>
<span class="nc bnc" id="L90" title="All 2 branches missed.">      case None =&gt; job.setNumReduceTasks(0)</span>
<span class="nc bnc" id="L91" title="All 2 branches missed.">      case Some(sort) =&gt;</span>
<span class="nc bnc" id="L92" title="All 2 branches missed.">        require(reducers &gt; 0, &quot;Reducers must be a positive number&quot;)</span>
<span class="nc" id="L93">        job.setNumReduceTasks(reducers)</span>
<span class="nc" id="L94">        job.setReducerClass(classOf[Reducer[Text, SimpleFeature, Text, SimpleFeature]])</span>

<span class="nc" id="L96">        job.setMapperClass(classOf[SortKeyMapper]) // override the default mapper we configured earlier</span>

<span class="nc" id="L98">        val sortBy = sort.map(_._1)</span>
<span class="nc" id="L99">        val reverse = sort.head._2</span>
<span class="nc bnc" id="L100" title="All 4 branches missed.">        if (sortBy.exists(sft.indexOf(_) == -1)) {</span>
<span class="nc" id="L101">          throw new IllegalArgumentException(s&quot;Trying to sort by non-existing property: ${sortBy.mkString(&quot;, &quot;)}&quot;)</span>
<span class="nc bnc" id="L102" title="All 4 branches missed.">        } else if (sort.exists(_._2 != reverse)) {</span>
<span class="nc" id="L103">          throw new IllegalArgumentException(s&quot;Sort order is required to be the same across all sort properties: &quot; +</span>
<span class="nc" id="L104">              QueryHints.sortReadableString(sort))</span>
        }

<span class="nc bnc" id="L107" title="All 2 branches missed.">        if (reverse) {</span>
<span class="nc" id="L108">          job.setSortComparatorClass(classOf[ReverseComparator])</span>
        }

        // configure a global sort, as by default sort is not maintained between multiple reducers
        // if there is only 1 reducer, we don't need a global sort as each reducer is already sorted
<span class="nc bnc" id="L113" title="All 2 branches missed.">        if (reducers &gt; 1) {</span>
<span class="nc" id="L114">          TotalOrderPartitioner.setPartitionFile(conf,</span>
<span class="nc" id="L115">            new Path(FileOutputFormat.getOutputPath(job), s&quot;${UUID.randomUUID()}.partitions&quot;))</span>
          // calculate the global partitioning using our custom sampler
<span class="nc" id="L117">          InputSampler.writePartitionFile(job, new FallbackSampler(0.01, 1000, 100))</span>
<span class="nc" id="L118">          job.setPartitionerClass(classOf[TotalOrderPartitioner[Text, SimpleFeature]])</span>
        }
    }

<span class="nc" id="L122">    job.setMapSpeculativeExecution(false)</span>
<span class="nc" id="L123">    job.setReduceSpeculativeExecution(false)</span>
    // Ensure that the reducers don't start too early
    // (default is at 0.05 which takes all the map slots and isn't needed)
<span class="nc" id="L126">    conf.set(&quot;mapreduce.job.reduce.slowstart.completedmaps&quot;, &quot;.90&quot;)</span>

<span class="nc" id="L128">    job</span>
  }

  private def createFormat(digits: Int): NumberFormat = {
<span class="nc" id="L132">    val nf = NumberFormat.getInstance()</span>
<span class="nc" id="L133">    nf.setMinimumIntegerDigits(digits)</span>
<span class="nc" id="L134">    nf.setGroupingUsed(false)</span>
<span class="nc" id="L135">    nf</span>
  }

<span class="nc" id="L138">  object Counters {</span>
<span class="nc" id="L139">    val Group   = &quot;org.locationtech.geomesa.jobs.export&quot;</span>
<span class="nc" id="L140">    val Loaded  = &quot;loaded&quot;</span>
<span class="nc" id="L141">    val Written = &quot;written&quot;</span>

<span class="nc" id="L143">    def count(job: Job): Long = job.getCounters.findCounter(Group, Written).getValue</span>
<span class="nc" id="L144">    def mapping(job: Job): Seq[(String, Long)] = Seq(Loaded -&gt; job.getCounters.findCounter(Group, Loaded).getValue)</span>
<span class="nc" id="L145">    def reducing(job: Job): Seq[(String, Long)] = Seq(Written -&gt; count(job))</span>
  }

<span class="nc" id="L148">  object Config {</span>

<span class="nc" id="L150">    val FileNameKey  = &quot;geomesa.export.filename&quot;</span>
<span class="nc" id="L151">    val HintsKey     = &quot;geomesa.export.hints&quot;</span>
<span class="nc" id="L152">    val ChunksKey    = &quot;geomesa.export.chunks&quot;</span>
<span class="nc" id="L153">    val FormatKey    = &quot;geomesa.export.format&quot;</span>
<span class="nc" id="L154">    val GzipKey      = &quot;geomesa.export.gzip&quot;</span>
<span class="nc" id="L155">    val HeadersKey   = &quot;geomesa.export.headers&quot;</span>

<span class="nc" id="L157">    def setOutputFile(conf: Configuration, file: String): Unit = conf.set(FileNameKey, file)</span>
<span class="nc" id="L158">    def getOutputFile(conf: Configuration): String = conf.get(FileNameKey)</span>

<span class="nc" id="L160">    def setQueryHints(conf: Configuration, hints: Hints): Unit = conf.set(HintsKey, ViewParams.serialize(hints))</span>
    def getQueryHints(conf: Configuration): Hints =
<span class="nc" id="L162">      Option(conf.get(HintsKey)).map(ViewParams.deserialize).getOrElse(new Hints)</span>

<span class="nc" id="L164">    def setFormat(conf: Configuration, format: ExportFormat): Unit = conf.set(FormatKey, format.name)</span>
<span class="nc" id="L165">    def getFormat(conf: Configuration): ExportFormat = ExportFormat(conf.get(FormatKey)).getOrElse {</span>
<span class="nc" id="L166">      throw new IllegalArgumentException(s&quot;Unknown export format: ${conf.get(FormatKey)}&quot;)</span>
    }

<span class="nc" id="L169">    def setChunks(conf: Configuration, chunks: Long): Unit = conf.set(ChunksKey, chunks.toString)</span>
<span class="nc" id="L170">    def getChunks(conf: Configuration): Option[Long] = Option(conf.get(ChunksKey)).map(_.toLong)</span>

<span class="nc" id="L172">    def setGzip(conf: Configuration, compression: Int): Unit = conf.set(GzipKey, compression.toString)</span>
<span class="nc" id="L173">    def getGzip(conf: Configuration): Option[Int] = Option(conf.get(GzipKey)).map(_.toInt)</span>

<span class="nc" id="L175">    def setHeaders(conf: Configuration, headers: Boolean): Unit = conf.set(HeadersKey, headers.toString)</span>
<span class="nc" id="L176">    def getHeaders(conf: Configuration): Boolean = conf.get(HeadersKey).toBoolean</span>
  }

  /**
    * Sampler implementation for global sorting reducer splits.
    *
    * The random sampler can fail to return any data if the input set is small, which then causes
    * the TotalOrderPartitioner to fail.
    *
    * This class tries to use the random sampler, but falls back to first n if needed.
    */
<span class="nc" id="L187">  class FallbackSampler(frequency: Double, numSamples: Int, maxSplitsToSample: Int) extends Sampler[AnyRef, AnyRef] {</span>

<span class="nc" id="L189">    private val random = new RandomSampler[AnyRef, AnyRef](frequency, numSamples, maxSplitsToSample)</span>
<span class="nc" id="L190">    private val split = new SplitSampler[AnyRef, AnyRef](numSamples, maxSplitsToSample)</span>

    override def getSample(inf: InputFormat[AnyRef, AnyRef], job: Job): Array[AnyRef] = {
<span class="nc" id="L193">      val result = random.getSample(inf, job)</span>
<span class="nc bnc" id="L194" title="All 2 branches missed.">      if (result.nonEmpty) { result } else {</span>
<span class="nc" id="L195">        split.getSample(inf, job)</span>
      }
    }
  }

  /**
    * Takes the input and writes it to the output
    */
<span class="nc" id="L203">  class PassThroughMapper extends Mapper[Text, SimpleFeature, Text, SimpleFeature] with StrictLogging {</span>

    type Context = Mapper[Text, SimpleFeature, Text, SimpleFeature]#Context

<span class="nc" id="L207">    private var counter: Counter = _</span>

    override protected def setup(context: Context): Unit =
<span class="nc" id="L210">      counter = context.getCounter(Counters.Group, Counters.Loaded)</span>

    override protected def map(key: Text, sf: SimpleFeature, context: Context): Unit = {
<span class="nc bnc" id="L213" title="All 2 branches missed.">      logger.debug(s&quot;map key ${key.toString}, map value ${DataUtilities.encodeFeature(sf)}&quot;)</span>
<span class="nc" id="L214">      context.write(key, sf)</span>
<span class="nc" id="L215">      counter.increment(1L)</span>
    }
  }

  /**
    * Takes the input and writes it to the output, with a key suitable for sorting
    */
<span class="nc" id="L222">  class SortKeyMapper extends Mapper[Text, SimpleFeature, Text, SimpleFeature] with StrictLogging {</span>

    type Context = Mapper[Text, SimpleFeature, Text, SimpleFeature]#Context

<span class="nc" id="L226">    private val text = new Text</span>
<span class="nc" id="L227">    private val builder = new StringBuilder()</span>

<span class="nc" id="L229">    private var sortFields: Seq[Int] = _</span>
<span class="nc" id="L230">    private var counter: Counter = _</span>

    override protected def setup(context: Context): Unit = {
<span class="nc" id="L233">      val sft = GeoMesaConfigurator.getResultsToFeatures(context.getConfiguration).schema</span>
<span class="nc" id="L234">      val sorting = GeoMesaConfigurator.getSorting(context.getConfiguration).getOrElse {</span>
<span class="nc" id="L235">        throw new IllegalStateException(&quot;No sorting defined in configuration&quot;)</span>
      }
<span class="nc bnc" id="L237" title="All 2 branches missed.">      sortFields = sorting.map { case (f, _) =&gt; sft.indexOf(f) }</span>
<span class="nc" id="L238">      counter = context.getCounter(Counters.Group, Counters.Loaded)</span>
    }

    override protected def map(key: Text, sf: SimpleFeature, context: Context): Unit = {
<span class="nc bnc" id="L242" title="All 2 branches missed.">      logger.debug(s&quot;map key ${key.toString}, map value ${DataUtilities.encodeFeature(sf)}&quot;)</span>
<span class="nc" id="L243">      builder.clear()</span>
<span class="nc" id="L244">      sortFields.foreach { i =&gt;</span>
<span class="nc" id="L245">        val attribute = sf.getAttribute(i)</span>
<span class="nc bnc" id="L246" title="All 2 branches missed.">        val encoded = if (attribute == null) { &quot;&quot; } else { AttributeIndexKey.typeEncode(attribute) }</span>
<span class="nc" id="L247">        builder.append(encoded).append(ByteArrays.ZeroByte)</span>
      }
<span class="nc" id="L249">      text.set(builder.toString)</span>
<span class="nc" id="L250">      context.write(text, sf)</span>
<span class="nc" id="L251">      counter.increment(1L)</span>
    }
  }

  /**
    * Output format for export files
    */
<span class="nc bnc" id="L258" title="All 4 branches missed.">  class ExportOutputFormat extends FileOutputFormat[Text, SimpleFeature] with LazyLogging {</span>

    // duplicated from FileOutputFormat, but doesn't care if the directory exists
    override def checkOutputSpecs(job: JobContext): Unit = {
<span class="nc" id="L262">      val outDir = FileOutputFormat.getOutputPath(job)</span>
<span class="nc bnc" id="L263" title="All 2 branches missed.">      if (outDir == null) {</span>
<span class="nc" id="L264">        throw new InvalidJobConfException(&quot;Output directory not set&quot;)</span>
      }
      // get delegation token for outDir's file system
<span class="nc" id="L267">      TokenCache.obtainTokensForNamenodes(job.getCredentials, Array(outDir), job.getConfiguration)</span>
    }

    override def getRecordWriter(job: TaskAttemptContext): RecordWriter[Text, SimpleFeature] = {
<span class="nc" id="L271">      val conf = job.getConfiguration</span>
      val file = {
<span class="nc bnc" id="L273" title="All 2 branches missed.">        val (base, extension) = PathUtils.getBaseNameAndExtension(Config.getOutputFile(conf))</span>
<span class="nc" id="L274">        val workPath = getOutputCommitter(job).asInstanceOf[FileOutputCommitter].getWorkPath</span>
<span class="nc" id="L275">        val jobId = job.getJobID</span>
<span class="nc" id="L276">        val jobIdentifier = s&quot;${jobId.getJtIdentifier}_${jobIdFormat.format(jobId.getId)}&quot;</span>
<span class="nc" id="L277">        val taskId = job.getTaskAttemptID.getTaskID</span>
<span class="nc" id="L278">        val taskType = TaskID.getRepresentingCharacter(taskId.getTaskType)</span>
<span class="nc" id="L279">        val name = s&quot;$base-$taskType-${jobIdentifier}_${taskIdFormat.format(taskId.getId)}$extension&quot;</span>
<span class="nc" id="L280">        new Path(workPath, name).toString</span>
      }
<span class="nc" id="L282">      val opts = ExportOptions(Config.getFormat(conf), Some(file), Config.getGzip(conf), Config.getHeaders(conf), writeEmptyFiles = true)</span>
<span class="nc" id="L283">      val hints = Config.getQueryHints(conf)</span>
<span class="nc bnc" id="L284" title="All 2 branches missed.">      lazy val names = new IncrementingFileName(file)</span>
<span class="nc" id="L285">      Config.getChunks(conf) match {</span>
<span class="nc bnc" id="L286" title="All 2 branches missed.">        case None =&gt; new ExportRecordWriter(job, opts, hints)</span>
<span class="nc bnc" id="L287" title="All 4 branches missed.">        case Some(c) if opts.format.countable =&gt; new ExportChunkedRecordWriter(job, names, opts, hints, c)</span>
<span class="nc bnc" id="L288" title="All 2 branches missed.">        case Some(c) =&gt; new ExportUncountableChunkedRecordWriter(job, names, opts, hints, c)</span>
      }
    }
  }

  /**
    * Record writer that wraps a FeatureExporter
    *
    * @param context task context
    * @param opts options
    * @param hints query hints
    */
<span class="nc" id="L300">  class ExportRecordWriter(context: TaskAttemptContext, opts: ExportOptions, hints: Hints)</span>
<span class="nc" id="L301">      extends RecordWriter[Text, SimpleFeature] {</span>

<span class="nc" id="L303">    private val counter = context.getCounter(Counters.Group, Counters.Written)</span>

<span class="nc" id="L305">    private var exporter: FeatureExporter = _</span>

    override def write(key: Text, value: SimpleFeature): Unit = {
<span class="nc bnc" id="L308" title="All 2 branches missed.">      if (exporter == null) {</span>
<span class="nc" id="L309">        exporter = new Exporter(opts, hints)</span>
<span class="nc" id="L310">        exporter.start(value.getFeatureType)</span>
      }
<span class="nc" id="L312">      exporter.export(Iterator.single(value))</span>
<span class="nc" id="L313">      counter.increment(1L) // since aggregation is disabled, this will always be 1 to 1 with features</span>
    }

<span class="nc bnc" id="L316" title="All 2 branches missed.">    override def close(context: TaskAttemptContext): Unit = if (exporter != null) { exporter.close() }</span>
  }

  /**
    * Record writer that wraps a chunked countable FeatureExporter. Because the exporter is countable,
    * the bytes written should be fairly accurate after every feature
    *
    * @param context task context
    * @param files source for file names
    * @param opts options
    * @param hints query hints
    * @param chunks chunk size, in bytes
    */
<span class="nc bnc" id="L329" title="All 4 branches missed.">  class ExportChunkedRecordWriter(</span>
      context: TaskAttemptContext,
<span class="nc" id="L331">      files: Iterator[String],</span>
<span class="nc" id="L332">      opts: ExportOptions,</span>
<span class="nc" id="L333">      hints: Hints,</span>
<span class="nc" id="L334">      chunks: Long</span>
<span class="nc" id="L335">    ) extends RecordWriter[Text, SimpleFeature] with LazyLogging {</span>

<span class="nc" id="L337">    private val counter = context.getCounter(Counters.Group, Counters.Written)</span>

<span class="nc" id="L339">    private var exporter: Exporter = _</span>
<span class="nc" id="L340">    private var estimator: FileSizeEstimator = _</span>

    override def write(key: Text, value: SimpleFeature): Unit = {
<span class="nc bnc" id="L343" title="All 2 branches missed.">      if (exporter == null) {</span>
<span class="nc" id="L344">        exporter = new Exporter(opts.copy(file = Some(files.next)), hints)</span>
<span class="nc" id="L345">        exporter.start(value.getFeatureType)</span>
<span class="nc bnc" id="L346" title="All 2 branches missed.">        if (estimator == null) {</span>
<span class="nc" id="L347">          val bytesPerFeature = opts.format.bytesPerFeature(value.getFeatureType)</span>
<span class="nc" id="L348">          estimator = new FileSizeEstimator(chunks, 0.01f, bytesPerFeature) // 1% error threshold</span>
        }
      }

<span class="nc" id="L352">      exporter.export(Iterator.single(value))</span>
<span class="nc" id="L353">      counter.increment(1L) // since aggregation is disabled, this will always be 1 to 1 with features</span>

      // bytes on a streaming exporter should be cheap to check
<span class="nc bnc" id="L356" title="All 2 branches missed.">      if (estimator.done(exporter.bytes)) {</span>
<span class="nc" id="L357">        exporter.close()</span>
<span class="nc" id="L358">        exporter = null</span>
      }
    }

<span class="nc bnc" id="L362" title="All 2 branches missed.">    override def close(context: TaskAttemptContext): Unit = if (exporter != null) { exporter.close() }</span>
  }

  /**
    * Record writer that wraps a chunked non-countable FeatureExporter. Because the exporter is not
    * countable, the bytes written will generally only be accurate after the exporter is closed
    *
    * @param context task context
    * @param files source for file names
    * @param opts options
    * @param hints query hints
    * @param chunks chunk size, in bytes
    */
<span class="nc bnc" id="L375" title="All 4 branches missed.">  class ExportUncountableChunkedRecordWriter(</span>
      context: TaskAttemptContext,
<span class="nc" id="L377">      files: Iterator[String],</span>
<span class="nc" id="L378">      opts: ExportOptions,</span>
<span class="nc" id="L379">      hints: Hints,</span>
<span class="nc" id="L380">      chunks: Long</span>
<span class="nc" id="L381">    ) extends RecordWriter[Text, SimpleFeature] with LazyLogging {</span>

<span class="nc" id="L383">    private val counter = context.getCounter(Counters.Group, Counters.Written)</span>

<span class="nc" id="L385">    private var exporter: Exporter = _</span>
<span class="nc" id="L386">    private var estimator: FileSizeEstimator = _</span>
<span class="nc" id="L387">    private var estimate = 0L // estimated number of features to write to hit our chunk size</span>
<span class="nc" id="L388">    private var count = 0L // current number of features written since the last estimate</span>

    override def write(key: Text, value: SimpleFeature): Unit = {
<span class="nc bnc" id="L391" title="All 2 branches missed.">      if (exporter == null) {</span>
<span class="nc" id="L392">        exporter = new Exporter(opts.copy(file = Some(files.next)), hints)</span>
<span class="nc" id="L393">        exporter.start(value.getFeatureType)</span>
<span class="nc bnc" id="L394" title="All 2 branches missed.">        if (estimator == null) {</span>
<span class="nc" id="L395">          val bytesPerFeature = opts.format.bytesPerFeature(value.getFeatureType)</span>
<span class="nc" id="L396">          estimator = new FileSizeEstimator(chunks, 0.05f, bytesPerFeature) // 5% error threshold</span>
        }
<span class="nc" id="L398">        estimate = estimator.estimate(0L)</span>
      }

<span class="nc" id="L401">      exporter.export(Iterator.single(value))</span>
<span class="nc" id="L402">      counter.increment(1L) // since aggregation is disabled, this will always be 1 to 1 with features</span>
<span class="nc" id="L403">      count += 1</span>

<span class="nc bnc" id="L405" title="All 2 branches missed.">      if (count &gt;= estimate) {</span>
        // since the bytes aren't generally available until after closing the writer,
        // we have to go with our initial estimate and adjust after the first chunk
<span class="nc" id="L408">        exporter.close()</span>
        // update the estimator so that it's more accurate on the next run
<span class="nc" id="L410">        estimator.update(exporter.bytes, count)</span>
<span class="nc" id="L411">        exporter = null</span>
<span class="nc" id="L412">        count = 0</span>
      }
    }

<span class="nc bnc" id="L416" title="All 2 branches missed.">    override def close(context: TaskAttemptContext): Unit = if (exporter != null) { exporter.close() }</span>
  }

  /**
    * Used for reverse (descending) sorting
    */
<span class="nc" id="L422">  class ReverseComparator extends org.apache.hadoop.io.Text.Comparator {</span>
    override def compare(b1: Array[Byte], s1: Int, l1: Int, b2: Array[Byte], s2: Int, l2: Int): Int =
<span class="nc" id="L424">      super.compare(b1, s1, l1, b2, s2, l2) * -1</span>
<span class="nc" id="L425">    override def compare(o1: WritableComparable[_], o2: WritableComparable[_]): Int = super.compare(o1, o2) * -1</span>
  }
}
</pre><div class="footer"><span class="right">Created with <a href="http://www.jacoco.org/jacoco">JaCoCo</a> 0.8.14.202510111229</span></div></body></html>