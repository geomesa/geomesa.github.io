<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml" lang=""><head><meta http-equiv="Content-Type" content="text/html;charset=UTF-8"/><link rel="stylesheet" href="../jacoco-resources/report.css" type="text/css"/><link rel="shortcut icon" href="../jacoco-resources/report.gif" type="image/gif"/><title>ParquetSimpleFeatureInputFormat.scala</title><link rel="stylesheet" href="../jacoco-resources/prettify.css" type="text/css"/><script type="text/javascript" src="../jacoco-resources/prettify.js"></script></head><body onload="window['PR_TAB_WIDTH']=4;prettyPrint()"><div class="breadcrumb" id="breadcrumb"><span class="info"><a href="../jacoco-sessions.html" class="el_session">Sessions</a></span><a href="../index.html" class="el_report">GeoMesa FileSystem Storage Parquet</a> &gt; <a href="index.source.html" class="el_package">org.locationtech.geomesa.fs.storage.parquet.jobs</a> &gt; <span class="el_source">ParquetSimpleFeatureInputFormat.scala</span></div><h1>ParquetSimpleFeatureInputFormat.scala</h1><pre class="source lang-java linenums">/***********************************************************************
 * Copyright (c) 2013-2025 General Atomics Integrated Intelligence, Inc.
 * All rights reserved. This program and the accompanying materials
 * are made available under the terms of the Apache License, Version 2.0
 * which accompanies this distribution and is available at
 * https://www.apache.org/licenses/LICENSE-2.0
 ***********************************************************************/

package org.locationtech.geomesa.fs.storage.parquet.jobs

import org.apache.hadoop.conf.Configuration
import org.apache.hadoop.fs.Path
import org.apache.hadoop.mapreduce.lib.input.{FileInputFormat, FileSplit}
import org.apache.hadoop.mapreduce.{InputSplit, JobContext, RecordReader, TaskAttemptContext}
import org.apache.parquet.hadoop.ParquetInputFormat
import org.geotools.api.data.Query
import org.geotools.api.feature.simple.{SimpleFeature, SimpleFeatureType}
import org.geotools.api.filter.Filter
import org.locationtech.geomesa.features.TransformSimpleFeature
import org.locationtech.geomesa.fs.storage.common.jobs.StorageConfiguration
import org.locationtech.geomesa.fs.storage.parquet.io.SimpleFeatureReadSupport
import org.locationtech.geomesa.fs.storage.parquet.jobs.ParquetSimpleFeatureInputFormat.{ParquetSimpleFeatureInputFormatBase, ParquetSimpleFeatureRecordReaderBase, ParquetSimpleFeatureTransformRecordReaderBase}
import org.locationtech.geomesa.fs.storage.parquet.{ReadFilter, ReadSchema}
import org.locationtech.geomesa.index.planning.QueryRunner

/**
  * Input format for parquet files
  */
<span class="nc" id="L29">class ParquetSimpleFeatureInputFormat extends ParquetSimpleFeatureInputFormatBase[Void] {</span>

  override protected def createRecordReader(
      delegate: RecordReader[Void, SimpleFeature],
      conf: Configuration,
      split: FileSplit,
      sft: SimpleFeatureType,
      filter: Option[Filter],
      transform: Option[(String, SimpleFeatureType)]): RecordReader[Void, SimpleFeature] = {
<span class="nc" id="L38">    transform match {</span>
<span class="nc bnc" id="L39" title="All 2 branches missed.">      case None =&gt; new ParquetSimpleFeatureRecordReader(delegate, filter)</span>
<span class="nc bnc" id="L40" title="All 4 branches missed.">      case Some((tdefs, tsft)) =&gt; new ParquetSimpleFeatureTransformRecordReader(delegate, filter, sft, tsft, tdefs)</span>
    }
  }

<span class="nc bnc" id="L44" title="All 2 branches missed.">  class ParquetSimpleFeatureRecordReader(delegate: RecordReader[Void, SimpleFeature], filter: Option[Filter])</span>
<span class="nc" id="L45">      extends ParquetSimpleFeatureRecordReaderBase[Void](delegate, filter) {</span>
<span class="nc" id="L46">    override def getCurrentKey: Void = null</span>
  }

<span class="nc bnc" id="L49" title="All 2 branches missed.">  class ParquetSimpleFeatureTransformRecordReader(</span>
      delegate: RecordReader[Void, SimpleFeature],
      filter: Option[Filter],
      sft: SimpleFeatureType,
      tsft: SimpleFeatureType,
      tdefs: String
<span class="nc" id="L55">    ) extends ParquetSimpleFeatureTransformRecordReaderBase[Void](delegate, filter, sft, tsft, tdefs) {</span>
<span class="nc" id="L56">    override def getCurrentKey: Void = null</span>
  }
}

<span class="nc" id="L60">object ParquetSimpleFeatureInputFormat {</span>

  import org.locationtech.geomesa.index.conf.QueryHints._

  /**
    * Configure the input format
    *
    * @param conf conf
    * @param sft simple feature type
    * @param query query
    */
  def configure(conf: Configuration, sft: SimpleFeatureType, query: Query): Unit = {
<span class="nc" id="L72">    val q = QueryRunner.configureQuery(sft, query)</span>
<span class="nc bnc" id="L73" title="All 6 branches missed.">    val filter = Option(q.getFilter).filter(_ != Filter.INCLUDE)</span>

    // Parquet read schema and final transform
<span class="nc bnc" id="L76" title="All 2 branches missed.">    val ReadSchema(readSft, readTransform) = ReadSchema(sft, filter, q.getHints.getTransform)</span>
    // push-down Parquet predicates and remaining gt-filter
<span class="nc bnc" id="L78" title="All 2 branches missed.">    val ReadFilter(parquetFilter, residualFilter) = ReadFilter(readSft, filter)</span>

    // set the parquet push-down filter
<span class="nc" id="L81">    parquetFilter.foreach(ParquetInputFormat.setFilterPredicate(conf, _))</span>

    // set our residual filters and transforms
<span class="nc" id="L84">    StorageConfiguration.setSft(conf, readSft)</span>
<span class="nc" id="L85">    residualFilter.foreach(StorageConfiguration.setFilter(conf, _))</span>
<span class="nc" id="L86">    readTransform.foreach(StorageConfiguration.setTransforms(conf, _))</span>

    // need this for query planning
<span class="nc" id="L89">    conf.set(&quot;parquet.filter.dictionary.enabled&quot;, &quot;true&quot;)</span>

    // @see org.apache.parquet.hadoop.ParquetInputFormat.setReadSupportClass(org.apache.hadoop.mapred.JobConf, java.lang.Class&lt;?&gt;)
<span class="nc" id="L92">    conf.set(ParquetInputFormat.READ_SUPPORT_CLASS, classOf[SimpleFeatureReadSupport].getName)</span>

    // replicates parquet input format strategy of always recursively listing directories
<span class="nc" id="L95">    conf.set(FileInputFormat.INPUT_DIR_RECURSIVE, &quot;true&quot;)</span>
  }

<span class="nc" id="L98">  abstract class ParquetSimpleFeatureInputFormatBase[T] extends FileInputFormat[T, SimpleFeature] {</span>

<span class="nc" id="L100">    private val delegate = new ParquetInputFormat[SimpleFeature]()</span>

    override def createRecordReader(
        split: InputSplit,
        context: TaskAttemptContext): RecordReader[T, SimpleFeature] = {
<span class="nc" id="L105">      val rr = delegate.createRecordReader(split, context)</span>

<span class="nc" id="L107">      val conf = context.getConfiguration</span>
<span class="nc" id="L108">      val sft = StorageConfiguration.getSft(conf)</span>
<span class="nc" id="L109">      val filter = StorageConfiguration.getFilter(conf, sft)</span>
<span class="nc" id="L110">      val transform = StorageConfiguration.getTransforms(conf)</span>

<span class="nc" id="L112">      createRecordReader(rr, conf, split.asInstanceOf[FileSplit], sft, filter, transform)</span>
    }

<span class="nc" id="L115">    override def getSplits(context: JobContext): java.util.List[InputSplit] = delegate.getSplits(context)</span>

    override protected def isSplitable(context: JobContext, filename: Path): Boolean =
<span class="nc" id="L118">      context.getConfiguration.getBoolean(ParquetInputFormat.SPLIT_FILES, true)</span>

    protected def createRecordReader(
        delegate: RecordReader[Void, SimpleFeature],
        conf: Configuration,
        split: FileSplit,
        sft: SimpleFeatureType,
        filter: Option[Filter],
        transform: Option[(String, SimpleFeatureType)]): RecordReader[T, SimpleFeature]
  }

<span class="nc" id="L129">  abstract class ParquetSimpleFeatureRecordReaderBase[T](</span>
<span class="nc" id="L130">      delegate: RecordReader[Void, SimpleFeature],</span>
<span class="nc" id="L131">      filter: Option[Filter]</span>
<span class="nc" id="L132">    ) extends RecordReader[T, SimpleFeature] {</span>

<span class="nc" id="L134">    private var current: SimpleFeature = _</span>

    override def initialize(split: InputSplit, context: TaskAttemptContext): Unit =
<span class="nc" id="L137">      delegate.initialize(split, context)</span>

<span class="nc" id="L139">    override def getProgress: Float = delegate.getProgress</span>

    override def nextKeyValue(): Boolean = {
<span class="nc bnc" id="L142" title="All 2 branches missed.">      while (delegate.nextKeyValue()) {</span>
<span class="nc" id="L143">        current = delegate.getCurrentValue</span>
<span class="nc bnc" id="L144" title="All 2 branches missed.">        if (filter.forall(_.evaluate(current))) {</span>
<span class="nc" id="L145">           return true</span>
        }
      }
<span class="nc" id="L148">      false</span>
    }

<span class="nc" id="L151">    override def getCurrentValue: SimpleFeature = current</span>

<span class="nc" id="L153">    override def close(): Unit = delegate.close()</span>
  }

<span class="nc" id="L156">  abstract class ParquetSimpleFeatureTransformRecordReaderBase[T](</span>
      delegate: RecordReader[Void, SimpleFeature],
      filter: Option[Filter],
      sft: SimpleFeatureType,
      tsft: SimpleFeatureType,
      tdefs: String
<span class="nc" id="L162">    ) extends ParquetSimpleFeatureRecordReaderBase[T](delegate, filter) {</span>

<span class="nc" id="L164">    private val current = TransformSimpleFeature(sft, tsft, tdefs)</span>

    override def getCurrentValue: SimpleFeature = {
<span class="nc" id="L167">      current.setFeature(super.getCurrentValue)</span>
<span class="nc" id="L168">      current</span>
    }
  }
<span class="nc" id="L171">}</span>
</pre><div class="footer"><span class="right">Created with <a href="http://www.jacoco.org/jacoco">JaCoCo</a> 0.8.14.202510111229</span></div></body></html>