<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml" lang=""><head><meta http-equiv="Content-Type" content="text/html;charset=UTF-8"/><link rel="stylesheet" href="../jacoco-resources/report.css" type="text/css"/><link rel="shortcut icon" href="../jacoco-resources/report.gif" type="image/gif"/><title>FileSystemConverterJob.scala</title><link rel="stylesheet" href="../jacoco-resources/prettify.css" type="text/css"/><script type="text/javascript" src="../jacoco-resources/prettify.js"></script></head><body onload="window['PR_TAB_WIDTH']=4;prettyPrint()"><div class="breadcrumb" id="breadcrumb"><span class="info"><a href="../jacoco-sessions.html" class="el_session">Sessions</a></span><a href="../index.html" class="el_report">GeoMesa FileSystem Tools</a> &gt; <a href="index.source.html" class="el_package">org.locationtech.geomesa.fs.tools.ingest</a> &gt; <span class="el_source">FileSystemConverterJob.scala</span></div><h1>FileSystemConverterJob.scala</h1><pre class="source lang-java linenums">/***********************************************************************
 * Copyright (c) 2013-2025 General Atomics Integrated Intelligence, Inc.
 * All rights reserved. This program and the accompanying materials
 * are made available under the terms of the Apache License, Version 2.0
 * which accompanies this distribution and is available at
 * https://www.apache.org/licenses/LICENSE-2.0
 ***********************************************************************/

package org.locationtech.geomesa.fs.tools.ingest

import com.typesafe.config.Config
import com.typesafe.scalalogging.LazyLogging
import org.apache.hadoop.fs.Path
import org.apache.hadoop.io.{BytesWritable, LongWritable, Text}
import org.apache.hadoop.mapreduce._
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat
import org.geotools.api.feature.simple.{SimpleFeature, SimpleFeatureType}
import org.geotools.data.DataUtilities
import org.locationtech.geomesa.features.SerializationOption
import org.locationtech.geomesa.features.kryo.KryoFeatureSerializer
import org.locationtech.geomesa.fs.storage.api._
import org.locationtech.geomesa.fs.storage.common.jobs.StorageConfiguration
import org.locationtech.geomesa.fs.storage.common.utils.StorageUtils.FileType
import org.locationtech.geomesa.fs.storage.orc.jobs.OrcStorageConfiguration
import org.locationtech.geomesa.fs.storage.parquet.jobs.ParquetStorageConfiguration
import org.locationtech.geomesa.fs.tools.ingest.FileSystemConverterJob.{DummyReducer, FsIngestMapper}
import org.locationtech.geomesa.index.geotools.GeoMesaFeatureWriter
import org.locationtech.geomesa.jobs.JobResult.JobSuccess
import org.locationtech.geomesa.jobs.mapreduce.GeoMesaOutputFormat.OutputCounters
import org.locationtech.geomesa.jobs.{JobResult, StatusCallback}
import org.locationtech.geomesa.tools.Command
import org.locationtech.geomesa.tools.ingest.ConverterIngestJob
import org.locationtech.geomesa.tools.ingest.IngestCommand.IngestCounters
import org.locationtech.geomesa.tools.utils.DistributedCopy

import java.io.File

<span class="nc bnc" id="L38" title="All 4 branches missed.">abstract class FileSystemConverterJob(</span>
    dsParams: Map[String, String],
<span class="nc" id="L40">    sft: SimpleFeatureType,</span>
    converterConfig: Config,
    paths: Seq[String],
    libjarsFiles: Seq[String],
    libjarsPaths: Iterator[() =&gt; Seq[File]],
<span class="nc" id="L45">    reducers: Int,</span>
<span class="nc" id="L46">    root: Path,</span>
<span class="nc" id="L47">    tmpPath: Option[Path],</span>
<span class="nc" id="L48">    targetFileSize: Option[Long]</span>
<span class="nc" id="L49">  ) extends ConverterIngestJob(dsParams, sft, converterConfig, paths, libjarsFiles, libjarsPaths)</span>
    with StorageConfiguration with LazyLogging {

  override protected def reduceCounters(job: Job): Seq[(String, Long)] =
<span class="nc" id="L53">    Seq((IngestCounters.Persisted, job.getCounters.findCounter(OutputCounters.Group, &quot;reduced&quot;).getValue))</span>

  override def configureJob(job: Job): Unit = {
<span class="nc" id="L56">    super.configureJob(job)</span>

<span class="nc" id="L58">    job.setMapperClass(classOf[FsIngestMapper])</span>
<span class="nc" id="L59">    job.setMapOutputKeyClass(classOf[Text])</span>
<span class="nc" id="L60">    job.setMapOutputValueClass(classOf[BytesWritable])</span>
<span class="nc" id="L61">    job.setOutputKeyClass(classOf[Void])</span>
<span class="nc" id="L62">    job.setOutputValueClass(classOf[SimpleFeature])</span>
<span class="nc" id="L63">    job.setReducerClass(classOf[DummyReducer])</span>
<span class="nc" id="L64">    job.setNumReduceTasks(reducers)</span>
    // Ensure that the reducers don't start too early
    // (default is at 0.05 which takes all the map slots and isn't needed)
<span class="nc" id="L67">    job.getConfiguration.set(&quot;mapreduce.job.reduce.slowstart.completedmaps&quot;, &quot;.90&quot;)</span>

<span class="nc" id="L69">    StorageConfiguration.setRootPath(job.getConfiguration, root)</span>
<span class="nc" id="L70">    StorageConfiguration.setFileType(job.getConfiguration, FileType.Written)</span>
<span class="nc" id="L71">    targetFileSize.foreach(StorageConfiguration.setTargetFileSize(job.getConfiguration, _))</span>

<span class="nc" id="L73">    FileOutputFormat.setOutputPath(job, tmpPath.getOrElse(root))</span>

<span class="nc" id="L75">    configureOutput(sft, job)</span>
  }

  override def await(reporter: StatusCallback): JobResult = {
<span class="nc" id="L79">    super.await(reporter).merge {</span>
<span class="nc" id="L80">      tmpPath.map { tp =&gt;</span>
<span class="nc" id="L81">        reporter.reset()</span>
<span class="nc" id="L82">        new DistributedCopy().copy(Seq(tp), root, reporter) match {</span>
<span class="nc bnc" id="L83" title="All 2 branches missed.">          case JobSuccess(message, counts) =&gt;</span>
<span class="nc bnc" id="L84" title="All 2 branches missed.">            Command.user.info(message)</span>
<span class="nc" id="L85">            JobSuccess(&quot;&quot;, counts)</span>

<span class="nc" id="L87">          case j =&gt; j</span>
        }
      }
    }
  }
}

<span class="nc" id="L94">object FileSystemConverterJob {</span>

<span class="nc" id="L96">  class ParquetConverterJob(</span>
      dsParams: Map[String, String],
      sft: SimpleFeatureType,
      converterConfig: Config,
      paths: Seq[String],
      libjarsFiles: Seq[String],
      libjarsPaths: Iterator[() =&gt; Seq[File]],
      reducers: Int,
      root: Path,
      tmpPath: Option[Path],
      targetFileSize: Option[Long]
<span class="nc" id="L107">    ) extends FileSystemConverterJob(</span>
<span class="nc" id="L108">        dsParams, sft, converterConfig, paths, libjarsFiles, libjarsPaths, reducers, root, tmpPath, targetFileSize)</span>
          with ParquetStorageConfiguration

<span class="nc" id="L111">  class OrcConverterJob(</span>
      dsParams: Map[String, String],
      sft: SimpleFeatureType,
      converterConfig: Config,
      paths: Seq[String],
      libjarsFiles: Seq[String],
      libjarsPaths: Iterator[() =&gt; Seq[File]],
      reducers: Int,
      root: Path,
      tmpPath: Option[Path],
      targetFileSize: Option[Long]
<span class="nc" id="L122">    ) extends FileSystemConverterJob(</span>
<span class="nc" id="L123">        dsParams, sft, converterConfig, paths, libjarsFiles, libjarsPaths, reducers, root, tmpPath, targetFileSize)</span>
          with OrcStorageConfiguration

<span class="nc bnc" id="L126" title="All 4 branches missed.">  class FsIngestMapper extends Mapper[LongWritable, SimpleFeature, Text, BytesWritable] with LazyLogging {</span>

    type Context = Mapper[LongWritable, SimpleFeature, Text, BytesWritable]#Context

<span class="nc" id="L130">    private var serializer: KryoFeatureSerializer = _</span>
<span class="nc" id="L131">    private var metadata: StorageMetadata = _</span>
<span class="nc" id="L132">    private var scheme: PartitionScheme = _</span>

<span class="nc" id="L134">    var mapped: Counter = _</span>
<span class="nc" id="L135">    var written: Counter = _</span>
<span class="nc" id="L136">    var failed: Counter = _</span>

    override def setup(context: Context): Unit = {
<span class="nc" id="L139">      val root = StorageConfiguration.getRootPath(context.getConfiguration)</span>
      // note: we don't call `reload` (to get the partition metadata) as we aren't using it
<span class="nc" id="L141">      metadata = StorageMetadataFactory.load(FileSystemContext(root, context.getConfiguration)).getOrElse {</span>
<span class="nc" id="L142">        throw new IllegalArgumentException(s&quot;Could not load storage instance at path $root&quot;)</span>
      }
<span class="nc" id="L144">      serializer = KryoFeatureSerializer(metadata.sft, SerializationOption.defaults)</span>
<span class="nc" id="L145">      scheme = metadata.scheme</span>

<span class="nc" id="L147">      mapped = context.getCounter(OutputCounters.Group, &quot;mapped&quot;)</span>
<span class="nc" id="L148">      written = context.getCounter(OutputCounters.Group, OutputCounters.Written)</span>
<span class="nc" id="L149">      failed = context.getCounter(OutputCounters.Group, OutputCounters.Failed)</span>
    }

    override def map(key: LongWritable, sf: SimpleFeature, context: Context): Unit = {
      // partitionKey is important because this needs to be correct for the parquet file
      try {
<span class="nc" id="L155">        mapped.increment(1)</span>
<span class="nc" id="L156">        val sfWithFid = GeoMesaFeatureWriter.featureWithFid(sf)</span>
<span class="nc" id="L157">        val partitionKey = new Text(scheme.getPartitionName(sfWithFid))</span>
<span class="nc" id="L158">        context.write(partitionKey, new BytesWritable(serializer.serialize(sfWithFid)))</span>
<span class="nc" id="L159">        written.increment(1)</span>
      } catch {
        case e: Throwable =&gt;
<span class="nc bnc" id="L162" title="All 2 branches missed.">          logger.error(s&quot;Failed to write '${DataUtilities.encodeFeature(sf)}'&quot;, e)</span>
<span class="nc" id="L163">          failed.increment(1)</span>
      }
    }

<span class="nc" id="L167">    override def cleanup(context: Context): Unit = metadata.close()</span>
  }

<span class="nc" id="L170">  class DummyReducer extends Reducer[Text, BytesWritable, Void, SimpleFeature] {</span>

    import scala.collection.JavaConverters._

    type Context = Reducer[Text, BytesWritable, Void, SimpleFeature]#Context

<span class="nc" id="L176">    private var serializer: KryoFeatureSerializer = _</span>
<span class="nc" id="L177">    private var reduced: Counter = _</span>

    override def setup(context: Context): Unit = {
<span class="nc" id="L180">      val root = StorageConfiguration.getRootPath(context.getConfiguration)</span>
      // note: we don't call `reload` (to get the partition metadata) as we aren't using it
<span class="nc" id="L182">      val metadata = StorageMetadataFactory.load(FileSystemContext(root, context.getConfiguration)).getOrElse {</span>
<span class="nc" id="L183">        throw new IllegalArgumentException(s&quot;Could not load storage instance at path $root&quot;)</span>
      }
<span class="nc" id="L185">      serializer = KryoFeatureSerializer(metadata.sft, SerializationOption.defaults)</span>
<span class="nc" id="L186">      reduced = context.getCounter(OutputCounters.Group, &quot;reduced&quot;)</span>
<span class="nc" id="L187">      metadata.close()</span>
    }

    override def reduce(key: Text, values: java.lang.Iterable[BytesWritable], context: Context): Unit = {
<span class="nc" id="L191">      values.asScala.foreach { bw =&gt;</span>
<span class="nc" id="L192">        val sf = serializer.deserialize(bw.getBytes)</span>
<span class="nc" id="L193">        context.write(null, sf)</span>
<span class="nc" id="L194">        reduced.increment(1)</span>
      }
    }
  }
<span class="nc" id="L198">}</span>
</pre><div class="footer"><span class="right">Created with <a href="http://www.jacoco.org/jacoco">JaCoCo</a> 0.8.14.202510111229</span></div></body></html>