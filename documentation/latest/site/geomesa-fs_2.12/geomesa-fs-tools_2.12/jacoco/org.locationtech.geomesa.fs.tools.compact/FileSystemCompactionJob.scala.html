<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml" lang=""><head><meta http-equiv="Content-Type" content="text/html;charset=UTF-8"/><link rel="stylesheet" href="../jacoco-resources/report.css" type="text/css"/><link rel="shortcut icon" href="../jacoco-resources/report.gif" type="image/gif"/><title>FileSystemCompactionJob.scala</title><link rel="stylesheet" href="../jacoco-resources/prettify.css" type="text/css"/><script type="text/javascript" src="../jacoco-resources/prettify.js"></script></head><body onload="window['PR_TAB_WIDTH']=4;prettyPrint()"><div class="breadcrumb" id="breadcrumb"><span class="info"><a href="../jacoco-sessions.html" class="el_session">Sessions</a></span><a href="../index.html" class="el_report">GeoMesa FileSystem Tools</a> &gt; <a href="index.source.html" class="el_package">org.locationtech.geomesa.fs.tools.compact</a> &gt; <span class="el_source">FileSystemCompactionJob.scala</span></div><h1>FileSystemCompactionJob.scala</h1><pre class="source lang-java linenums">/***********************************************************************
 * Copyright (c) 2013-2025 General Atomics Integrated Intelligence, Inc.
 * All rights reserved. This program and the accompanying materials
 * are made available under the terms of the Apache License, Version 2.0
 * which accompanies this distribution and is available at
 * https://www.apache.org/licenses/LICENSE-2.0
 ***********************************************************************/

package org.locationtech.geomesa.fs.tools.compact

import com.typesafe.scalalogging.LazyLogging
import org.apache.hadoop.conf.Configuration
import org.apache.hadoop.fs.Path
import org.apache.hadoop.mapreduce._
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat
import org.geotools.api.feature.simple.SimpleFeature
import org.geotools.util.factory.Hints
import org.locationtech.geomesa.fs.storage.api.FileSystemStorage
import org.locationtech.geomesa.fs.storage.api.StorageMetadata.PartitionMetadata
import org.locationtech.geomesa.fs.storage.common.SizeableFileSystemStorage
import org.locationtech.geomesa.fs.storage.common.jobs.StorageConfiguration
import org.locationtech.geomesa.fs.storage.common.utils.StorageUtils.FileType
import org.locationtech.geomesa.fs.storage.orc.jobs.OrcStorageConfiguration
import org.locationtech.geomesa.fs.storage.parquet.jobs.ParquetStorageConfiguration
import org.locationtech.geomesa.fs.tools.compact.FileSystemCompactionJob.CompactionMapper
import org.locationtech.geomesa.jobs.JobResult.JobSuccess
import org.locationtech.geomesa.jobs.mapreduce.GeoMesaOutputFormat.OutputCounters
import org.locationtech.geomesa.jobs.mapreduce.JobWithLibJars
import org.locationtech.geomesa.jobs.{JobResult, StatusCallback}
import org.locationtech.geomesa.tools.Command
import org.locationtech.geomesa.tools.utils.{DistributedCopy, JobRunner}
import org.locationtech.geomesa.utils.text.TextTools

import java.io.File

<span class="nc" id="L36">trait FileSystemCompactionJob extends StorageConfiguration with JobWithLibJars {</span>

  import FileSystemCompactionJob.{FailedCounter, MappedCounter}

  def run(
      storage: FileSystemStorage,
      partitions: Seq[PartitionMetadata],
      targetFileSize: Option[Long],
      tempPath: Option[Path],
      libjarsFiles: Seq[String],
      libjarsPaths: Iterator[() =&gt; Seq[File]],
      statusCallback: StatusCallback): JobResult = {

<span class="nc" id="L49">    val job = Job.getInstance(new Configuration(storage.context.conf), &quot;GeoMesa Storage Compaction&quot;)</span>

<span class="nc" id="L51">    setLibJars(job, libjarsFiles, libjarsPaths)</span>
<span class="nc" id="L52">    job.setJarByClass(this.getClass)</span>

    // InputFormat and Mappers
<span class="nc" id="L55">    job.setInputFormatClass(classOf[PartitionInputFormat])</span>
<span class="nc" id="L56">    job.setMapperClass(classOf[CompactionMapper])</span>

    // No reducers - Mapper will read/write its own things
<span class="nc" id="L59">    job.setNumReduceTasks(0)</span>

<span class="nc" id="L61">    job.setMapOutputKeyClass(classOf[Void])</span>
<span class="nc" id="L62">    job.setMapOutputValueClass(classOf[SimpleFeature])</span>
<span class="nc" id="L63">    job.setOutputKeyClass(classOf[Void])</span>
<span class="nc" id="L64">    job.setOutputValueClass(classOf[SimpleFeature])</span>

<span class="nc" id="L66">    val qualifiedTempPath = tempPath.map(storage.context.fs.makeQualified)</span>

<span class="nc" id="L68">    StorageConfiguration.setRootPath(job.getConfiguration, storage.context.root)</span>
<span class="nc" id="L69">    StorageConfiguration.setPartitions(job.getConfiguration, partitions.map(_.name).toArray)</span>
<span class="nc" id="L70">    StorageConfiguration.setFileType(job.getConfiguration, FileType.Compacted)</span>
<span class="nc" id="L71">    targetFileSize.foreach(StorageConfiguration.setTargetFileSize(job.getConfiguration, _))</span>

<span class="nc" id="L73">    FileOutputFormat.setOutputPath(job, qualifiedTempPath.getOrElse(storage.context.root))</span>

    // MapReduce options
<span class="nc" id="L76">    job.getConfiguration.set(&quot;mapred.map.tasks.speculative.execution&quot;, &quot;false&quot;)</span>
<span class="nc" id="L77">    job.getConfiguration.set(&quot;mapreduce.job.user.classpath.first&quot;, &quot;true&quot;)</span>

<span class="nc" id="L79">    configureOutput(storage.metadata.sft, job)</span>

    // save the existing files so we can delete them afterwards
    // mimic the filtering done in PartitionInputFormat
<span class="nc bnc" id="L83" title="All 4 branches missed.">    val sizeable = Option(storage).collect { case s: SizeableFileSystemStorage =&gt; s }</span>
<span class="nc" id="L84">    val sizeCheck = sizeable.flatMap(s =&gt; s.targetSize(targetFileSize).map(t =&gt; (p: Path) =&gt; s.fileIsSized(p, t)))</span>
<span class="nc" id="L85">    val existingDataFiles = partitions.toList.flatMap { p =&gt;</span>
<span class="nc" id="L86">      val files = storage.getFilePaths(p.name).filterNot(f =&gt; sizeCheck.exists(_.apply(f.path)))</span>
      // TODO get counts right... use m/r counters?
<span class="nc bnc" id="L88" title="All 2 branches missed.">      if (files.isEmpty) { None } else { Some(p.copy(files = files.map(_.file)) -&gt; files) }</span>
    }

<span class="nc" id="L91">    def mapCounters = Seq((MappedCounter, written(job)), (FailedCounter, failed(job)))</span>

<span class="nc" id="L93">    val result = JobRunner.run(job, statusCallback, mapCounters, Seq.empty).merge {</span>
<span class="nc" id="L94">      qualifiedTempPath.map { tp =&gt;</span>
<span class="nc" id="L95">        new DistributedCopy().copy(Seq(tp), storage.context.root, statusCallback)</span>
      }
    }

<span class="nc" id="L99">    result match {</span>
<span class="nc bnc" id="L100" title="All 2 branches missed.">      case JobSuccess(message, counts) =&gt;</span>
<span class="nc bnc" id="L101" title="All 2 branches missed.">        if (message.nonEmpty) {</span>
<span class="nc bnc" id="L102" title="All 2 branches missed.">          Command.user.info(message)</span>
        }
<span class="nc bnc" id="L104" title="All 2 branches missed.">        Command.user.info(&quot;Removing old files&quot;)</span>
<span class="nc bnc" id="L105" title="All 2 branches missed.">        existingDataFiles.foreach { case (partition, files) =&gt;</span>
<span class="nc" id="L106">          val counter = StorageConfiguration.Counters.partition(partition.name)</span>
<span class="nc" id="L107">          val count = Option(job.getCounters.findCounter(StorageConfiguration.Counters.Group, counter)).map(_.getValue)</span>
<span class="nc" id="L108">          files.foreach(f =&gt; storage.context.fs.delete(f.path, false))</span>
<span class="nc" id="L109">          storage.metadata.removePartition(partition.copy(count = count.getOrElse(0L)))</span>
<span class="nc" id="L110">          val removed = count.map(c =&gt; s&quot;containing $c features &quot;).getOrElse(&quot;&quot;)</span>
<span class="nc bnc" id="L111" title="All 2 branches missed.">          Command.user.info(s&quot;Removed ${TextTools.getPlural(files.size, &quot;file&quot;)} ${removed}in partition ${partition.name}&quot;)</span>
        }
<span class="nc bnc" id="L113" title="All 2 branches missed.">        Command.user.info(&quot;Compacting metadata&quot;)</span>
<span class="nc" id="L114">        storage.metadata.compact(None, None, threads = 4)</span>
<span class="nc" id="L115">        JobSuccess(&quot;&quot;, counts)</span>

<span class="nc" id="L117">      case j =&gt; j</span>
    }
  }

  private def written(job: Job): Long =
<span class="nc" id="L122">    job.getCounters.findCounter(OutputCounters.Group, OutputCounters.Written).getValue</span>

  private def failed(job: Job): Long =
<span class="nc" id="L125">    job.getCounters.findCounter(OutputCounters.Group, OutputCounters.Failed).getValue</span>
}

<span class="nc" id="L128">object FileSystemCompactionJob {</span>

<span class="nc" id="L130">  val MappedCounter = &quot;mapped&quot;</span>
<span class="nc" id="L131">  val FailedCounter = &quot;failed&quot;</span>

<span class="nc bnc" id="L133" title="All 4 branches missed.">  class ParquetCompactionJob extends FileSystemCompactionJob with ParquetStorageConfiguration</span>

<span class="nc" id="L135">  class OrcCompactionJob extends FileSystemCompactionJob with OrcStorageConfiguration</span>

  /**
    * Mapper that simply reads the input format and writes the output to the sample node. This mapper
    * is paired with the PartitionRecordReader which will feed all the features into a single map task
    */
<span class="nc bnc" id="L141" title="All 4 branches missed.">  class CompactionMapper extends Mapper[Void, SimpleFeature, Void, SimpleFeature] with LazyLogging {</span>

    type Context = Mapper[Void, SimpleFeature, Void, SimpleFeature]#Context

<span class="nc" id="L145">    private var written: Counter = _</span>
<span class="nc" id="L146">    private var mapped: Counter = _</span>

    override def setup(context: Context): Unit = {
<span class="nc" id="L149">      super.setup(context)</span>
<span class="nc" id="L150">      written = context.getCounter(OutputCounters.Group, OutputCounters.Written)</span>
<span class="nc" id="L151">      mapped = context.getCounter(&quot;org.locationtech.geomesa.fs.compaction&quot;, MappedCounter)</span>
    }

    override def map(key: Void, sf: SimpleFeature, context: Context): Unit = {
<span class="nc" id="L155">      sf.getUserData.put(Hints.USE_PROVIDED_FID, java.lang.Boolean.TRUE)</span>
<span class="nc" id="L156">      mapped.increment(1)</span>
<span class="nc" id="L157">      context.write(null, sf)</span>
<span class="nc" id="L158">      written.increment(1)</span>
    }
  }
<span class="nc" id="L161">}</span>
</pre><div class="footer"><span class="right">Created with <a href="http://www.jacoco.org/jacoco">JaCoCo</a> 0.8.14.202510111229</span></div></body></html>