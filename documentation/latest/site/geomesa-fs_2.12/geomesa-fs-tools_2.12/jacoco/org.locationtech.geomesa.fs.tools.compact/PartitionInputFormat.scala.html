<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml" lang=""><head><meta http-equiv="Content-Type" content="text/html;charset=UTF-8"/><link rel="stylesheet" href="../jacoco-resources/report.css" type="text/css"/><link rel="shortcut icon" href="../jacoco-resources/report.gif" type="image/gif"/><title>PartitionInputFormat.scala</title><link rel="stylesheet" href="../jacoco-resources/prettify.css" type="text/css"/><script type="text/javascript" src="../jacoco-resources/prettify.js"></script></head><body onload="window['PR_TAB_WIDTH']=4;prettyPrint()"><div class="breadcrumb" id="breadcrumb"><span class="info"><a href="../jacoco-sessions.html" class="el_session">Sessions</a></span><a href="../index.html" class="el_report">GeoMesa FileSystem Tools</a> &gt; <a href="index.source.html" class="el_package">org.locationtech.geomesa.fs.tools.compact</a> &gt; <span class="el_source">PartitionInputFormat.scala</span></div><h1>PartitionInputFormat.scala</h1><pre class="source lang-java linenums">/***********************************************************************
 * Copyright (c) 2013-2025 General Atomics Integrated Intelligence, Inc.
 * All rights reserved. This program and the accompanying materials
 * are made available under the terms of the Apache License, Version 2.0
 * which accompanies this distribution and is available at
 * https://www.apache.org/licenses/LICENSE-2.0
 ***********************************************************************/

package org.locationtech.geomesa.fs.tools.compact

import org.apache.hadoop.fs.Path
import org.apache.hadoop.io.Writable
import org.apache.hadoop.mapreduce._
import org.geotools.api.data.Query
import org.geotools.api.feature.simple.{SimpleFeature, SimpleFeatureType}
import org.geotools.api.filter.Filter
import org.locationtech.geomesa.fs.storage.api.StorageMetadata.{PartitionMetadata, StorageFile, StorageFileAction}
import org.locationtech.geomesa.fs.storage.api._
import org.locationtech.geomesa.fs.storage.common.SizeableFileSystemStorage
import org.locationtech.geomesa.fs.storage.common.jobs.StorageConfiguration
import org.locationtech.geomesa.fs.storage.common.utils.PathCache
import org.locationtech.geomesa.fs.tools.compact.PartitionInputFormat.{PartitionInputSplit, PartitionRecordReader}
import org.locationtech.geomesa.utils.io.{CloseWithLogging, WithClose}

import java.io.{DataInput, DataOutput}

/**
  * An Input format that creates splits based on FSDS Partitions. This is used for compaction, when we want a single
  * split per partition. Otherwise, use OrcSimpleFeatureInputFormat/ParquetSimpleFeatureInputFormat as those are
  * more efficient
  */
<span class="nc" id="L32">class PartitionInputFormat extends InputFormat[Void, SimpleFeature] {</span>

  override def getSplits(context: JobContext): java.util.List[InputSplit] = {
<span class="nc" id="L35">    val conf = context.getConfiguration</span>

<span class="nc" id="L37">    val root = StorageConfiguration.getRootPath(conf)</span>
<span class="nc" id="L38">    val fsc = FileSystemContext(root, conf)</span>
<span class="nc" id="L39">    val fileSize = StorageConfiguration.getTargetFileSize(conf)</span>

<span class="nc" id="L41">    val metadata = StorageMetadataFactory.load(fsc).getOrElse {</span>
<span class="nc" id="L42">      throw new IllegalArgumentException(s&quot;No storage defined under path '$root'&quot;)</span>
    }
<span class="nc" id="L44">    WithClose(metadata) { meta =&gt;</span>
<span class="nc" id="L45">      WithClose(FileSystemStorageFactory(fsc, meta)) { storage =&gt;</span>
<span class="nc bnc" id="L46" title="All 4 branches missed.">        val sizeable = Option(storage).collect { case s: SizeableFileSystemStorage =&gt; s }</span>
<span class="nc" id="L47">        val sizeCheck = sizeable.flatMap(s =&gt; s.targetSize(fileSize).map(t =&gt; (p: Path) =&gt; s.fileIsSized(p, t)))</span>
<span class="nc" id="L48">        val splits = StorageConfiguration.getPartitions(conf).map { partition =&gt;</span>
<span class="nc" id="L49">          var size = 0L</span>
<span class="nc" id="L50">          val files = storage.getFilePaths(partition).filter { f =&gt;</span>
<span class="nc bnc" id="L51" title="All 2 branches missed.">            if (sizeCheck.exists(_.apply(f.path))) { false } else {</span>
<span class="nc" id="L52">              size += PathCache.status(fsc.fs, f.path).getLen</span>
<span class="nc" id="L53">              true</span>
            }
          }
<span class="nc" id="L56">          new PartitionInputSplit(partition, files.map(_.file), size)</span>
        }
<span class="nc" id="L58">        java.util.Arrays.asList(splits: _*)</span>
      }
    }
  }

  override def createRecordReader(split: InputSplit, context: TaskAttemptContext): RecordReader[Void, SimpleFeature] = {
<span class="nc" id="L64">    val psplit = split.asInstanceOf[PartitionInputSplit]</span>
<span class="nc" id="L65">    new PartitionRecordReader(psplit.getName, psplit.getFiles)</span>
  }
}

<span class="nc" id="L69">object PartitionInputFormat {</span>

  /**
    * InputSplit corresponding to a single FileSystemDataStore PartitionScheme partition
    */
<span class="nc" id="L74">  class PartitionInputSplit extends InputSplit with Writable {</span>

<span class="nc" id="L76">    private var name: String = _</span>
<span class="nc" id="L77">    private var files: Seq[StorageFile] = _</span>
<span class="nc" id="L78">    private var length: java.lang.Long = _</span>

<span class="nc" id="L80">    def this(name: String, files: Seq[StorageFile], length: Long) = {</span>
<span class="nc" id="L81">      this()</span>
<span class="nc" id="L82">      this.name = name</span>
<span class="nc" id="L83">      this.files = files</span>
<span class="nc" id="L84">      this.length = length</span>
    }

    /**
      * @return the name of this partition
      */
<span class="nc" id="L90">    def getName: String = name</span>

<span class="nc" id="L92">    def getFiles: Seq[StorageFile] = files</span>

<span class="nc" id="L94">    override def getLength: Long = length</span>

    // TODO attempt to optimize the locations where this should run in the case of HDFS
    // With S3 this won't really matter
<span class="nc" id="L98">    override def getLocations: Array[String] = Array.empty[String]</span>

    override def write(out: DataOutput): Unit = {
<span class="nc" id="L101">      out.writeUTF(name)</span>
<span class="nc" id="L102">      out.writeLong(length)</span>
<span class="nc" id="L103">      out.writeInt(files.length)</span>
<span class="nc bnc" id="L104" title="All 2 branches missed.">      files.foreach { case StorageFile(file, ts, action, _ , _) =&gt;</span>
<span class="nc" id="L105">        out.writeUTF(file)</span>
<span class="nc" id="L106">        out.writeLong(ts)</span>
<span class="nc" id="L107">        out.writeUTF(action.toString)</span>
      }
    }

    override def readFields(in: DataInput): Unit = {
<span class="nc" id="L112">      this.name = in.readUTF()</span>
<span class="nc" id="L113">      this.length = in.readLong()</span>
<span class="nc" id="L114">      this.files = Seq.fill(in.readInt) {</span>
<span class="nc" id="L115">        StorageFile(in.readUTF(), in.readLong, StorageFileAction.withName(in.readUTF()))</span>
      }
    }
  }

<span class="nc" id="L120">  class PartitionRecordReader(partition: String, files: Seq[StorageFile]) extends RecordReader[Void, SimpleFeature] {</span>

<span class="nc" id="L122">    private var storage: FileSystemStorage = _</span>
<span class="nc" id="L123">    private var reader: CloseableFeatureIterator = _</span>

<span class="nc" id="L125">    private var curValue: SimpleFeature = _</span>

    override def initialize(split: InputSplit, context: TaskAttemptContext): Unit = {
<span class="nc" id="L128">      val conf = context.getConfiguration</span>
<span class="nc" id="L129">      val root = StorageConfiguration.getRootPath(conf)</span>
<span class="nc" id="L130">      val fsc = FileSystemContext(root, conf)</span>
<span class="nc" id="L131">      val metadata = StorageMetadataFactory.load(fsc).getOrElse {</span>
<span class="nc" id="L132">        throw new IllegalArgumentException(s&quot;No storage defined under path '$root'&quot;)</span>
      }
      // use a cached metadata impl instead of reloading
<span class="nc" id="L135">      val data = PartitionMetadata(partition, files, None, 0L)</span>
<span class="nc" id="L136">      val cached = new CachedMetadata(metadata.sft, metadata.encoding, metadata.scheme, metadata.leafStorage, data)</span>
<span class="nc" id="L137">      storage = FileSystemStorageFactory(fsc, cached)</span>
<span class="nc" id="L138">      reader = storage.getReader(new Query(&quot;&quot;, Filter.INCLUDE), Option(partition))</span>
<span class="nc" id="L139">      metadata.close()</span>
    }

    // TODO look at how the ParquetInputFormat provides progress and utilize something similar
<span class="nc" id="L143">    override def getProgress: Float = 0.0f</span>

    override def nextKeyValue(): Boolean = {
<span class="nc bnc" id="L146" title="All 2 branches missed.">      if (reader.hasNext) {</span>
<span class="nc" id="L147">        curValue = reader.next()</span>
<span class="nc" id="L148">        true</span>
      } else {
<span class="nc" id="L150">        curValue = null</span>
<span class="nc" id="L151">        false</span>
      }
    }

<span class="nc" id="L155">    override def getCurrentKey: Void = null</span>
<span class="nc" id="L156">    override def getCurrentValue: SimpleFeature = curValue</span>

<span class="nc" id="L158">    override def close(): Unit = CloseWithLogging(reader, storage)</span>
  }

<span class="nc" id="L161">  class CachedMetadata(</span>
<span class="nc" id="L162">      val sft: SimpleFeatureType,</span>
<span class="nc" id="L163">      val encoding: String,</span>
<span class="nc" id="L164">      val scheme: PartitionScheme,</span>
<span class="nc" id="L165">      val leafStorage: Boolean,</span>
<span class="nc" id="L166">      partition: PartitionMetadata</span>
<span class="nc" id="L167">  ) extends StorageMetadata {</span>
    override def getPartition(name: String): Option[PartitionMetadata] =
<span class="nc bnc" id="L169" title="All 6 branches missed.">      if (partition.name == name) { Some(partition) } else { None }</span>
    override def getPartitions(prefix: Option[String]): Seq[PartitionMetadata] =
<span class="nc bnc" id="L171" title="All 2 branches missed.">      if (prefix.forall(partition.name.startsWith)) { Seq(partition) } else { Seq.empty }</span>
<span class="nc" id="L172">    override def addPartition(partition: PartitionMetadata): Unit = throw new UnsupportedOperationException()</span>
<span class="nc" id="L173">    override def removePartition(partition: PartitionMetadata): Unit = throw new UnsupportedOperationException()</span>
<span class="nc" id="L174">    override def setPartitions(partitions: Seq[PartitionMetadata]): Unit = throw new UnsupportedOperationException()</span>
    override def compact(partition: Option[String], fileSize: Option[Long], threads: Int): Unit =
<span class="nc" id="L176">      throw new UnsupportedOperationException()</span>
<span class="nc" id="L177">    override def invalidate(): Unit = {}</span>
<span class="nc" id="L178">    override def close(): Unit = {}</span>
  }
<span class="nc" id="L180">}</span>
</pre><div class="footer"><span class="right">Created with <a href="http://www.jacoco.org/jacoco">JaCoCo</a> 0.8.14.202510111229</span></div></body></html>