<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml" lang=""><head><meta http-equiv="Content-Type" content="text/html;charset=UTF-8"/><link rel="stylesheet" href="../jacoco-resources/report.css" type="text/css"/><link rel="shortcut icon" href="../jacoco-resources/report.gif" type="image/gif"/><title>KafkaFeatureCache.scala</title><link rel="stylesheet" href="../jacoco-resources/prettify.css" type="text/css"/><script type="text/javascript" src="../jacoco-resources/prettify.js"></script></head><body onload="window['PR_TAB_WIDTH']=4;prettyPrint()"><div class="breadcrumb" id="breadcrumb"><span class="info"><a href="../jacoco-sessions.html" class="el_session">Sessions</a></span><a href="../index.html" class="el_report">GeoMesa Lambda DataStore</a> &gt; <a href="index.source.html" class="el_package">org.locationtech.geomesa.lambda.stream.kafka</a> &gt; <span class="el_source">KafkaFeatureCache.scala</span></div><h1>KafkaFeatureCache.scala</h1><pre class="source lang-java linenums">/***********************************************************************
 * Copyright (c) 2013-2025 General Atomics Integrated Intelligence, Inc.
 * All rights reserved. This program and the accompanying materials
 * are made available under the terms of the Apache License, Version 2.0
 * which accompanies this distribution and is available at
 * https://www.apache.org/licenses/LICENSE-2.0
 ***********************************************************************/

package org.locationtech.geomesa.lambda.stream.kafka

import com.typesafe.scalalogging.StrictLogging
import org.geotools.api.data.{DataStore, Transaction}
import org.geotools.api.feature.simple.{SimpleFeature, SimpleFeatureType}
import org.locationtech.geomesa.index.utils.FeatureWriterHelper
import org.locationtech.geomesa.lambda.data.LambdaDataStore.PersistenceConfig
import org.locationtech.geomesa.lambda.stream.OffsetManager
import org.locationtech.geomesa.lambda.stream.OffsetManager.OffsetListener
import org.locationtech.geomesa.lambda.stream.kafka.KafkaFeatureCache._
import org.locationtech.geomesa.utils.concurrent.ExitingExecutor
import org.locationtech.geomesa.utils.conf.GeoMesaSystemProperties.SystemProperty
import org.locationtech.geomesa.utils.geotools.FeatureUtils
import org.locationtech.geomesa.utils.io.{CloseWithLogging, WithClose}
import org.locationtech.geomesa.utils.metrics.MethodProfiling

import java.io.Closeable
import java.time.{Clock, Instant, ZoneOffset, ZonedDateTime}
import java.util.concurrent.atomic.AtomicLong
import java.util.concurrent.{ConcurrentHashMap, ScheduledThreadPoolExecutor, TimeUnit}
import scala.annotation.tailrec
import scala.collection.mutable.ArrayBuffer
import scala.util.Random
import scala.util.control.NonFatal

/**
  * Locally cached features
  */
<span class="nc" id="L37">class KafkaFeatureCache(</span>
<span class="nc" id="L38">    ds: DataStore,</span>
<span class="nc" id="L39">    sft: SimpleFeatureType,</span>
<span class="nc" id="L40">    offsetManager: OffsetManager,</span>
<span class="nc" id="L41">    topic: String,</span>
    persist: Option[PersistenceConfig])
<span class="nc" id="L43">   (implicit clock: Clock = Clock.systemUTC())</span>
<span class="nc" id="L44">  extends WritableFeatureCache with ReadableFeatureCache with OffsetListener</span>
    with Closeable with MethodProfiling with StrictLogging {

  import org.locationtech.geomesa.filter.ff

  import scala.collection.JavaConverters._

  // map of feature id -&gt; current feature
<span class="nc" id="L52">  private val features = new ConcurrentHashMap[String, FeatureReference]</span>

  // technically we should synchronize all access to the following array, since we expand it if needed;
  // however, in normal use it will be created up front and then only read.
  // if partitions are added at runtime, we may have synchronization issues...
<span class="nc" id="L57">  private val offsets = ArrayBuffer.empty[AtomicLong]</span>

<span class="nc" id="L59">  private val lockTimeout = KafkaFeatureCache.LockTimeout.toMillis.get</span>

<span class="nc bnc" id="L61" title="All 2 branches missed.">  private val persistence = persist.map { case PersistenceConfig(e, batchSize) =&gt; new Persistence(e.toMillis, batchSize) }</span>

<span class="nc" id="L63">  offsetManager.addOffsetListener(topic, this)</span>

  override def partitionAssigned(partition: Int, offset: Long): Unit = {
<span class="nc bnc" id="L66" title="All 2 branches missed.">    logger.debug(s&quot;Partition assigned: [$topic:$partition:$offset]&quot;)</span>
<span class="nc" id="L67">    ensurePartition(partition, offset)</span>
  }

  override def get(id: String): SimpleFeature = {
<span class="nc" id="L71">    val result = features.get(id)</span>
<span class="nc bnc" id="L72" title="All 2 branches missed.">    if (result == null) { null } else { result.feature }</span>
  }

<span class="nc" id="L75">  override def all(): Iterator[SimpleFeature] = features.values.iterator.asScala.map(_.feature)</span>

  override def add(feature: SimpleFeature, partition: Int, offset: Long, created: Long): Unit = {
<span class="nc bnc" id="L78" title="All 2 branches missed.">    if (offsets(partition).get &lt; offset) {</span>
<span class="nc bnc" id="L79" title="All 2 branches missed.">      logger.trace(s&quot;Adding [$partition:$offset] $feature created at &quot; +</span>
<span class="nc" id="L80">          s&quot;${ZonedDateTime.ofInstant(Instant.ofEpochMilli(created), ZoneOffset.UTC)}&quot;)</span>
<span class="nc" id="L81">      features.put(feature.getID, new FeatureReference(feature, partition, offset, created))</span>
    } else {
<span class="nc bnc" id="L83" title="All 2 branches missed.">      logger.trace(s&quot;Ignoring [$partition:$offset] $feature created at &quot; +</span>
<span class="nc" id="L84">          s&quot;${ZonedDateTime.ofInstant(Instant.ofEpochMilli(created), ZoneOffset.UTC)}&quot;)</span>
    }
  }

  override def delete(feature: SimpleFeature, partition: Int, offset: Long, created: Long): Unit = {
<span class="nc bnc" id="L89" title="All 2 branches missed.">    logger.trace(s&quot;Deleting [$partition:$offset] $feature created at &quot; +</span>
<span class="nc" id="L90">        s&quot;${ZonedDateTime.ofInstant(Instant.ofEpochMilli(created), ZoneOffset.UTC)}&quot;)</span>
<span class="nc" id="L91">    features.remove(feature.getID)</span>
<span class="nc" id="L92">    persistence.foreach { p =&gt;</span>
<span class="nc" id="L93">      try {</span>
<span class="nc bnc" id="L94" title="All 2 branches missed.">        if (p.delete(feature.getID)) {</span>
<span class="nc bnc" id="L95" title="All 2 branches missed.">          logger.trace(s&quot;Persistent store delete [$topic:$partition:$offset] $feature&quot;)</span>
        }
      } catch {
<span class="nc bnc" id="L98" title="All 4 branches missed.">        case NonFatal(e) =&gt; logger.error(s&quot;Error deleting feature in persistent store: $feature&quot;, e)</span>
      }
    }
  }

  override def offsetChanged(partition: Int, offset: Long): Unit = {
<span class="nc bnc" id="L104" title="All 2 branches missed.">    if (offsets.length &lt;= partition) {</span>
<span class="nc bnc" id="L105" title="All 2 branches missed.">      logger.debug(s&quot;Offsets changed for [$topic:$partition]: (unassigned) -&gt; $offset&quot;)</span>
<span class="nc" id="L106">      ensurePartition(partition, offset)</span>
<span class="nc" id="L107">      return</span>
    }

    // update the valid offset
<span class="nc" id="L111">    var last = offsets(partition).get</span>
<span class="nc bnc" id="L112" title="All 2 branches missed.">    logger.debug(s&quot;Offsets changed for [$topic:$partition]: $last -&gt; $offset&quot;)</span>
<span class="nc bnc" id="L113" title="All 4 branches missed.">    while (last &lt; offset &amp;&amp; !offsets(partition).compareAndSet(last, offset)) {</span>
<span class="nc" id="L114">      last = offsets(partition).get</span>
    }

    def onCompleted(count: Int, time: Long): Unit =
<span class="nc bnc" id="L118" title="All 2 branches missed.">      logger.debug(f&quot;Size of cached state for [$topic]: ${features.size}%d (removed $count%d entries in ${time}ms)&quot;)</span>

<span class="nc" id="L120">    profile(onCompleted _) {</span>
<span class="nc" id="L121">      var removed = 0</span>
<span class="nc" id="L122">      val iter = features.values().iterator()</span>
<span class="nc bnc" id="L123" title="All 2 branches missed.">      while (iter.hasNext) {</span>
<span class="nc" id="L124">        val next = iter.next()</span>
<span class="nc bnc" id="L125" title="All 4 branches missed.">        if (next.partition == partition &amp;&amp; next.offset &lt;= offset) {</span>
<span class="nc" id="L126">          iter.remove()</span>
<span class="nc" id="L127">          removed += 1</span>
        }
      }
<span class="nc" id="L130">      removed</span>
    }
  }

  /**
   * Persist any expired features
   *
   * @return list of last persisted offsets per partition
   */
  def persist(): Seq[PartitionOffset] =
<span class="nc" id="L140">    persistence.getOrElse(throw new IllegalStateException(&quot;Persistence is disabled for this store&quot;)).persist()</span>

  override def close(): Unit = {
<span class="nc" id="L143">    CloseWithLogging(persistence)</span>
<span class="nc" id="L144">    offsetManager.removeOffsetListener(topic, this)</span>
  }

  /**
   * Create an offset holder for the partition if it doesn't already exist
   *
   * @param partition partition
   * @param offset offset
   */
  private def ensurePartition(partition: Int, offset: Long): Unit = synchronized {
<span class="nc bnc" id="L154" title="All 2 branches missed.">    while (offsets.length &lt;= partition) {</span>
<span class="nc" id="L155">      offsets += new AtomicLong(-1L)</span>
    }
<span class="nc" id="L157">    offsets(partition).set(offset)</span>
  }

  /**
   * Wrapper for managing scheduled persistence runs
   */
<span class="nc bnc" id="L163" title="All 2 branches missed.">  private class Persistence(expiryMillis: Long, batchSize: Int) extends Closeable {</span>

<span class="nc" id="L165">    private val frequency = KafkaFeatureCache.PersistInterval.toMillis.get</span>
<span class="nc" id="L166">    private val executor = ExitingExecutor(new ScheduledThreadPoolExecutor(1))</span>
<span class="nc" id="L167">    private val schedule = executor.scheduleWithFixedDelay(() =&gt; persist(), frequency, frequency, TimeUnit.MILLISECONDS)</span>

    /**
     * Persist any expired features
     *
     * @return list of last persisted offsets per partition
     */
    def persist(): Seq[PartitionOffset] = {
<span class="nc bnc" id="L175" title="All 2 branches missed.">      logger.debug(s&quot;Running persistence for [$topic]&quot;)</span>
      // lock per-partition to allow for multiple write threads
      // randomly access the partitions to avoid contention if multiple data stores are all on the same schedule
<span class="nc" id="L178">      Random.shuffle(offsets.indices.toList).flatMap { partition =&gt;</span>
        // if we don't get the lock just try again next run
<span class="nc bnc" id="L180" title="All 2 branches missed.">        logger.trace(s&quot;Acquiring lock for [$topic:$partition]&quot;)</span>
<span class="nc" id="L181">        offsetManager.acquireLock(topic, partition, lockTimeout) match {</span>
<span class="nc bnc" id="L182" title="All 2 branches missed.">          case None =&gt;</span>
<span class="nc bnc" id="L183" title="All 2 branches missed.">            logger.trace(s&quot;Could not acquire lock for [$topic:$partition] within ${lockTimeout}ms&quot;)</span>
<span class="nc" id="L184">            None</span>
<span class="nc bnc" id="L185" title="All 2 branches missed.">          case Some(lock) =&gt;</span>
            try {
<span class="nc bnc" id="L187" title="All 2 branches missed.">              logger.trace(s&quot;Acquired lock for [$topic:$partition]&quot;)</span>
<span class="nc" id="L188">              persist(partition).map(PartitionOffset(partition, _))</span>
            } finally {
<span class="nc" id="L190">              lock.close()</span>
<span class="nc bnc" id="L191" title="All 2 branches missed.">              logger.trace(s&quot;Released lock for [$topic:$partition]&quot;)</span>
            }
        }
      }
    }

    /**
     * Delete a feature
     *
     * @param id feature id
     * @return true if feature was deleted
     */
    def delete(id: String): Boolean = {
<span class="nc" id="L204">      WithClose(ds.getFeatureWriter(sft.getTypeName, ff.id(ff.featureId(id)), Transaction.AUTO_COMMIT)) { writer =&gt;</span>
<span class="nc bnc" id="L205" title="All 2 branches missed.">        if (writer.hasNext) {</span>
<span class="nc" id="L206">          writer.next()</span>
<span class="nc" id="L207">          writer.remove()</span>
<span class="nc" id="L208">          true</span>
        } else {
<span class="nc" id="L210">          false</span>
        }
      }
    }

    /**
     * Persist any expired features that haven't yet been persisted
     *
     * @param partition partition to persist
     * @param lastPersistedOffset result from previous recursive run, if any
     * @return offset of latest persisted feature
     */
    @tailrec
<span class="nc" id="L223">    private def persist(partition: Int, lastPersistedOffset: Option[Long] = None): Option[Long] = {</span>
<span class="nc" id="L224">      val expiry = clock.millis() - expiryMillis</span>

<span class="nc" id="L226">      val lastOffset = offsetManager.getOffset(topic, partition)</span>
<span class="nc bnc" id="L227" title="All 2 branches missed.">      logger.debug(s&quot;Last persisted offsets for [$topic:$partition]: $lastOffset&quot;)</span>

<span class="nc" id="L229">      var nextOffset = -1L</span>
      // note: copy to a new collection so that any updates don't affect our persistence here
      val expired = {
<span class="nc" id="L232">        val builder = Map.newBuilder[String, FeatureReference]</span>
<span class="nc bnc" id="L233" title="All 2 branches missed.">        features.asScala.foreach { case (id, f) =&gt;</span>
          // note: offset &gt; lastOffset ensures we don't operate on features that have already been processed
<span class="nc bnc" id="L235" title="All 6 branches missed.">          if (f.partition == partition &amp;&amp; f.created &lt;= expiry &amp;&amp; f.offset &gt; lastOffset) {</span>
<span class="nc" id="L236">            nextOffset = math.max(nextOffset, f.offset)</span>
<span class="nc" id="L237">            builder += id -&gt; f</span>
          }
        }
<span class="nc" id="L240">        builder.result()</span>
      }
<span class="nc bnc" id="L242" title="All 2 branches missed.">      logger.debug(s&quot;Found ${expired.size} expired entries for [$topic:$partition]&quot;)</span>

<span class="nc bnc" id="L244" title="All 2 branches missed.">      if (expired.isEmpty) {</span>
<span class="nc" id="L245">        lastPersistedOffset</span>
      } else {
<span class="nc bnc" id="L247" title="All 2 branches missed.">        logger.trace(expired.values.map(e =&gt; s&quot;offset ${e.offset}: ${e.feature}&quot;).mkString(&quot;Expired entries:\n\t&quot;, &quot;\n\t&quot;, &quot;&quot;))</span>
        val batch =
<span class="nc bnc" id="L249" title="All 2 branches missed.">          if (expired.size &lt;= batchSize) {</span>
<span class="nc" id="L250">            expired</span>
          } else {
<span class="nc bnc" id="L252" title="All 2 branches missed.">            logger.trace(s&quot;Skipping persistence for ${expired.size - batchSize} features based on batch size of $batchSize&quot;)</span>
            // sort by offset since we track persistence based on offset
<span class="nc" id="L254">            val sorted = expired.toList.sortBy(_._2.offset).take(batchSize)</span>
<span class="nc" id="L255">            nextOffset = sorted.last._2.offset</span>
<span class="nc" id="L256">            sorted.toMap</span>
          }
<span class="nc" id="L258">        persist(partition, batch)</span>

<span class="nc bnc" id="L260" title="All 2 branches missed.">        logger.debug(s&quot;Committing offset [$topic:$partition:$nextOffset]&quot;)</span>
        // this will trigger our listener and cause the feature to be removed from the cache
<span class="nc" id="L262">        offsetManager.setOffset(topic, partition, nextOffset)</span>
<span class="nc bnc" id="L263" title="All 2 branches missed.">        if (batch.size &lt; expired.size) {</span>
          // we skipped some expired features, so run again immediately
<span class="nc" id="L265">          persist(partition, Some(nextOffset))</span>
        } else {
<span class="nc" id="L267">          Some(nextOffset)</span>
        }
      }
    }

    /**
     * Persist expired features
     *
     * @param partition partition
     * @param batch expired features
     */
    private def persist(partition: Int, batch: Map[String, FeatureReference]): Unit = {
      // do an update query first
<span class="nc" id="L280">      val remaining = persistUpdates(partition, batch)</span>
      // if any weren't updates, add them as inserts
<span class="nc bnc" id="L282" title="All 2 branches missed.">      if (remaining.nonEmpty) {</span>
<span class="nc" id="L283">        persistAppends(partition, remaining)</span>
      }
    }

    /**
     * Attempt to persist expired features through modifying writes
     *
     * @param partition partition being persisted
     * @param batch expired feature
     * @return any features that were not persisted
     */
    private def persistUpdates(partition: Int, batch: Map[String, FeatureReference]): Iterable[FeatureReference] = {
<span class="nc bnc" id="L295" title="All 2 branches missed.">      logger.debug(s&quot;Attempting persistent updates on ${batch.size} features&quot;)</span>
<span class="nc" id="L296">      val persistedKeys = scala.collection.mutable.Set.empty[String]</span>
<span class="nc bnc" id="L297" title="All 2 branches missed.">      profile((c: Int, time: Long) =&gt; logger.debug(s&quot;Wrote $c updated feature(s) to persistent storage in ${time}ms&quot;)) {</span>
<span class="nc" id="L298">        val filter = ff.id(batch.keys.map(ff.featureId).toSeq: _*)</span>
<span class="nc" id="L299">        WithClose(ds.getFeatureWriter(sft.getTypeName, filter, Transaction.AUTO_COMMIT)) { writer =&gt;</span>
<span class="nc" id="L300">          var count = 0</span>
<span class="nc bnc" id="L301" title="All 2 branches missed.">          while (writer.hasNext) {</span>
<span class="nc" id="L302">            val next = writer.next()</span>
<span class="nc" id="L303">            batch.get(next.getID).foreach { p =&gt;</span>
<span class="nc bnc" id="L304" title="All 2 branches missed.">              logger.trace(s&quot;Persistent store modify [$topic:$partition:${p.offset}] ${p.feature}&quot;)</span>
<span class="nc" id="L305">              persistedKeys += next.getID</span>
<span class="nc" id="L306">              FeatureUtils.copyToFeature(next, p.feature, useProvidedFid = true)</span>
<span class="nc" id="L307">              try {</span>
<span class="nc" id="L308">                writer.write()</span>
<span class="nc" id="L309">                count += 1</span>
              } catch {
<span class="nc bnc" id="L311" title="All 4 branches missed.">                case NonFatal(e) =&gt; logger.error(s&quot;Error persisting feature: ${p.feature}&quot;, e)</span>
              }
            }
          }
<span class="nc" id="L315">          count</span>
        }
      }
<span class="nc bnc" id="L318" title="All 8 branches missed.">      batch.collect { case (k, v) if !persistedKeys.contains(k) =&gt; v }</span>
    }

    /**
     * Attempt to persist expired features through appending writes
     *
     * @param partition partition being persisted
     * @param batch expired feature
     */
    private def persistAppends(partition: Int, batch: Iterable[FeatureReference]): Unit = {
<span class="nc bnc" id="L328" title="All 2 branches missed.">      logger.debug(s&quot;Attempting persistent appends on ${batch.size} features&quot;)</span>
<span class="nc bnc" id="L329" title="All 2 branches missed.">      profile((c: Int, time: Long) =&gt; logger.debug(s&quot;Wrote $c new feature(s) to persistent storage in ${time}ms&quot;)) {</span>
<span class="nc" id="L330">        WithClose(ds.getFeatureWriterAppend(sft.getTypeName, Transaction.AUTO_COMMIT)) { writer =&gt;</span>
<span class="nc" id="L331">          var count = 0</span>
<span class="nc" id="L332">          val helper = FeatureWriterHelper(writer, useProvidedFids = true)</span>
<span class="nc" id="L333">          batch.foreach { p =&gt;</span>
<span class="nc bnc" id="L334" title="All 2 branches missed.">            logger.trace(s&quot;Persistent store append [$topic:$partition:${p.offset}] ${p.feature}&quot;)</span>
<span class="nc" id="L335">            try {</span>
<span class="nc" id="L336">              helper.write(p.feature)</span>
<span class="nc" id="L337">              count += 1</span>
            } catch {
<span class="nc bnc" id="L339" title="All 4 branches missed.">              case NonFatal(e) =&gt; logger.error(s&quot;Error persisting feature: ${p.feature}&quot;, e)</span>
            }
          }
<span class="nc" id="L342">          count</span>
        }
      }
    }

    override def close(): Unit = {
<span class="nc" id="L348">      schedule.cancel(true)</span>
<span class="nc" id="L349">      executor.shutdownNow()</span>
<span class="nc" id="L350">      executor.awaitTermination(1, TimeUnit.SECONDS)</span>
    }
  }
}

<span class="nc" id="L355">object KafkaFeatureCache {</span>

<span class="nc" id="L357">  val PersistInterval: SystemProperty = SystemProperty(&quot;geomesa.lambda.persist.interval&quot;, &quot;1 minute&quot;)</span>
<span class="nc" id="L358">  val LockTimeout: SystemProperty = SystemProperty(&quot;geomesa.lambda.persist.lock.timeout&quot;, &quot;1 second&quot;)</span>

  trait ReadableFeatureCache {

    /**
      * Returns most recent versions of all features currently in this cache
      *
      * @return
      */
    def all(): Iterator[SimpleFeature]

    /**
      * Returns the most recent version of a feature in this cache, by feature ID
      *
      * @param id feature id
      * @return
      */
    def get(id: String): SimpleFeature
  }

  trait WritableFeatureCache {

    /**
      * Initialize this cached state for a given partition and offset
      *
      * @param partition partition
      * @param offset offset
      */
    def partitionAssigned(partition: Int, offset: Long): Unit

    /**
      * Add a feature to the cached state
      *
      * @param feature feature
      * @param partition partition corresponding to the add message
      * @param offset offset corresponding to the add message
      * @param created time feature was created
      */
    def add(feature: SimpleFeature, partition: Int, offset: Long, created: Long): Unit

    /**
      * Deletes a feature from the cached state
      *
      * @param feature feature
      * @param partition partition corresponding to the delete message
      * @param offset offset corresponding to the delete message
      * @param created time feature was deleted
      */
    def delete(feature: SimpleFeature, partition: Int, offset: Long, created: Long): Unit
  }

  /**
   * Partition and offset holder
   *
   * @param partition partition
   * @param offset offset
   */
<span class="nc bnc" id="L415" title="All 17 branches missed.">  case class PartitionOffset(partition: Int, offset: Long)</span>

  /**
   * Feature holder used to track the latest feature in our state. Comparison is only based on the
   * partition and offset (which are unique) so that we don't have to hold onto expired features
   * in memory
   *
   * @param feature simple feature
   * @param partition kafka partition
   * @param offset kafka offset
   * @param created create time
   */
<span class="nc" id="L427">  private class FeatureReference(</span>
<span class="nc" id="L428">      val feature: SimpleFeature,</span>
<span class="nc" id="L429">      val partition: Int,</span>
<span class="nc" id="L430">      val offset: Long,</span>
<span class="nc" id="L431">      val created: Long) {</span>
<span class="nc" id="L432">    override def equals(other: Any): Boolean = other match {</span>
<span class="nc bnc" id="L433" title="All 6 branches missed.">      case that: FeatureReference =&gt; partition == that.partition &amp;&amp; offset == that.offset</span>
<span class="nc" id="L434">      case _ =&gt; false</span>
    }
    override def hashCode(): Int = {
<span class="nc" id="L437">      val state = Seq(partition, offset)</span>
<span class="nc" id="L438">      state.map(_.hashCode()).foldLeft(0)((a, b) =&gt; 31 * a + b)</span>
    }
  }
<span class="nc" id="L441">}</span>
</pre><div class="footer"><span class="right">Created with <a href="http://www.jacoco.org/jacoco">JaCoCo</a> 0.8.14.202510111229</span></div></body></html>