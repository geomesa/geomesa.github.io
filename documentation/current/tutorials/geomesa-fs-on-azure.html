

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>GeoMesa FileSystem on Microsoft Azure &mdash; GeoMesa 3.2.0 Manuals</title>
  

  
  
  
  
    <link rel="canonical" href="https://www.geomesa.org/documentation/tutorials/geomesa-fs-on-azure.html"/>
  

  

  
  
    

  

  <link rel="stylesheet" href="../_static/css/theme_custom.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:400,700" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Map-Reduce Ingest of GDELT" href="geomesa-examples-gdelt.html" />
    <link rel="prev" title="Deploying GeoMesa HBase on Cloudera CDH 5.X" href="geomesa-hbase-on-cdh.html" />

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-53087457-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-53087457-1');
</script>



  
  <script src="../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../index.html" class="icon icon-home"> GeoMesa
          

          
          </a>

          
            
            
              <div class="version">
                3.3.0-SNAPSHOT
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../user/index.html">User Manual</a></li>
<li class="toctree-l1"><a class="reference internal" href="../developer/index.html">Developer Manual</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">Tutorials</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="index.html#quick-starts">Quick Starts</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="index.html#installation">Installation</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="geomesa-hbase-s3-on-aws.html">Bootstrapping GeoMesa HBase on AWS S3</a></li>
<li class="toctree-l3"><a class="reference internal" href="geomesa-hbase-on-cdh.html">Deploying GeoMesa HBase on Cloudera CDH 5.X</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">GeoMesa FileSystem on Microsoft Azure</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#prerequisites">Prerequisites</a></li>
<li class="toctree-l4"><a class="reference internal" href="#installing-configuring-azure-distributed-data-engineering-toolkit">Installing &amp; Configuring Azure Distributed Data Engineering Toolkit</a></li>
<li class="toctree-l4"><a class="reference internal" href="#customise-cluster-default-configuration">Customise Cluster Default Configuration</a></li>
<li class="toctree-l4"><a class="reference internal" href="#customise-hadoop-cluster-configuration">Customise Hadoop Cluster Configuration</a></li>
<li class="toctree-l4"><a class="reference internal" href="#create-an-azure-file-share">Create an Azure File Share</a></li>
<li class="toctree-l4"><a class="reference internal" href="#create-a-apache-spark-cluster-for-ingest">Create a Apache Spark Cluster for Ingest</a></li>
<li class="toctree-l4"><a class="reference internal" href="#connect-to-the-cluster">Connect to the Cluster</a></li>
<li class="toctree-l4"><a class="reference internal" href="#mount-your-azure-file-share">Mount your Azure File Share</a></li>
<li class="toctree-l4"><a class="reference internal" href="#install-configure-geomesa-filesystem-cli">Install &amp; Configure GeoMesa Filesystem CLI</a></li>
<li class="toctree-l4"><a class="reference internal" href="#ingest-data-into-azure-blob-storage">Ingest Data into Azure Blob Storage</a></li>
<li class="toctree-l4"><a class="reference internal" href="#install-jupyter-geomesa-jupyter-leaflet-apache-toree">Install Jupyter, GeoMesa Jupyter Leaflet &amp; Apache Toree</a></li>
<li class="toctree-l4"><a class="reference internal" href="#running-jupyter-and-opening-notebooks">Running Jupyter and opening Notebooks</a></li>
<li class="toctree-l4"><a class="reference internal" href="#deleting-your-ephemeral-cluster">Deleting your Ephemeral Cluster</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="index.html#data-in-out">Data In/Out</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#data-analysis">Data Analysis</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#security">Security</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#indexing-and-queries">Indexing and Queries</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#about-tutorial-versions">About Tutorial Versions</a></li>
</ul>
</li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">GeoMesa</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
          <li><a href="index.html">Tutorials</a> &raquo;</li>
        
      <li>GeoMesa FileSystem on Microsoft Azure</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="geomesa-filesystem-on-microsoft-azure">
<h1>GeoMesa FileSystem on Microsoft Azure<a class="headerlink" href="#geomesa-filesystem-on-microsoft-azure" title="Permalink to this headline">¶</a></h1>
<p>GeoMesa FileSystem can be used on top of Azure Blob storage, with Apache Spark analytics running using ephemeral
(temporary) Azure Batch clusters. This mode of running GeoMesa is cost-effective due to the separation of storage
(relatively cheap) and compute (relatively expensive, but only charged when required). The following guide describes
how to set up an Azure Batch cluster, ingest some data, then analyse it using Spark (Scala) in a Jupyter notebook.</p>
<div class="section" id="prerequisites">
<h2>Prerequisites<a class="headerlink" href="#prerequisites" title="Permalink to this headline">¶</a></h2>
<p>You will need a Microsoft Azure account with sufficient credit or an appropriate payment method. As a guide, running the
steps in this tutorial should cost no more than $5. If you don’t already have an account, you can sign up for a free
trial <a class="reference external" href="https://azure.microsoft.com/">here</a>.</p>
</div>
<div class="section" id="installing-configuring-azure-distributed-data-engineering-toolkit">
<h2>Installing &amp; Configuring Azure Distributed Data Engineering Toolkit<a class="headerlink" href="#installing-configuring-azure-distributed-data-engineering-toolkit" title="Permalink to this headline">¶</a></h2>
<p>This guide uses the <a class="reference external" href="https://github.com/Azure/aztk">Azure Distributed Data Engineering Toolkit (AZTK)</a> in order to set
up an ephemeral cluster. Alternatively, you may wish to deploy a more permanent
<a class="reference external" href="https://azure.microsoft.com/en-gb/services/hdinsight/">Azure HDInsight</a> cluster. This latter option is not covered here,
but much of the subsequent operations will be common.</p>
<p>Follow the <a class="reference external" href="https://github.com/Azure/aztk">AZTK instructions</a> to install AZTK.</p>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">Make sure you pick the correct branch of the documentation to match the latest release (not <code class="docutils literal notranslate"><span class="pre">main</span></code>).</p>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">It is recommended to install AZTK in an
<a class="reference external" href="https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html">Anaconda environment</a>, or
a <a class="reference external" href="https://docs.python.org/3/tutorial/venv.html">Python virtual environment</a>.</p>
</div>
<p>In summary, to install AZTK:</p>
<ol class="arabic simple">
<li><code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span> <span class="pre">aztk</span></code></li>
<li>In a directory of your choosing, <code class="docutils literal notranslate"><span class="pre">aztk</span> <span class="pre">spark</span> <span class="pre">init</span></code></li>
<li>Use <a class="reference external" href="https://shell.azure.com/">Azure Cloud Shell</a> and the <code class="docutils literal notranslate"><span class="pre">account_setup.sh</span></code> script to generate the contents of
<code class="docutils literal notranslate"><span class="pre">.aztk/secrets.yaml</span></code> for you. Alternatively, you can create the necessary Resource Groups, Batch Accounts, Storage
Accounts, etc. manually.</li>
<li>Optionally, generate a ssh key pair and reference the public key from <code class="docutils literal notranslate"><span class="pre">.aztk/secrets.yaml</span></code>.</li>
</ol>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">You may need to register the <code class="docutils literal notranslate"><span class="pre">Microsoft.Batch</span></code> provider in your Azure account. Check in the
<a class="reference external" href="https://portal.azure.com.">Azure Portal</a> under Subscriptions…Subscription Name…Settings…Resource providers.</p>
</div>
<div class="figure" id="id1">
<img alt="Microsoft Azure Resource Providers" src="../_images/azure-resource-providers.png" />
<p class="caption"><span class="caption-text">Microsoft Azure Resource Providers</span></p>
</div>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">Your <code class="docutils literal notranslate"><span class="pre">secrets.yaml</span></code> file now contains sensitive data and should be protected accordingly.</p>
</div>
</div>
<div class="section" id="customise-cluster-default-configuration">
<h2>Customise Cluster Default Configuration<a class="headerlink" href="#customise-cluster-default-configuration" title="Permalink to this headline">¶</a></h2>
<p>Edit <code class="docutils literal notranslate"><span class="pre">.aztk/cluster.yaml</span></code> as follows:</p>
<ol class="arabic simple">
<li>Comment out the <code class="docutils literal notranslate"><span class="pre">size:</span></code> line. We will specify the cluster size on the command line and specifying the size here
will cause issues with mixed normal and low priority nodes unless additional network configuration is performed (beyond
the scope of this guide, but covered <a class="reference external" href="https://github.com/Azure/aztk/blob/master/docs/10-clusters.md#mixed-mode">here</a>).</li>
<li>Set <code class="docutils literal notranslate"><span class="pre">environment:</span> <span class="pre">anaconda</span></code>.</li>
<li>Ensure the <code class="docutils literal notranslate"><span class="pre">jupyter</span></code> plugin is <em>not</em> enabled (we will manually install Jupyter later so we can add Spark Scala
support).</li>
<li>Set <code class="docutils literal notranslate"><span class="pre">worker_on_master</span></code> to <code class="docutils literal notranslate"><span class="pre">false</span></code> to disable running Apache Spark executors on the master itself.</li>
</ol>
</div>
<div class="section" id="customise-hadoop-cluster-configuration">
<h2>Customise Hadoop Cluster Configuration<a class="headerlink" href="#customise-hadoop-cluster-configuration" title="Permalink to this headline">¶</a></h2>
<p>Add the following lines to <code class="docutils literal notranslate"><span class="pre">.aztk/core-site.xml</span></code> to enable Hadoop access to your Azure Blob Storage account via the
secure (<code class="docutils literal notranslate"><span class="pre">wasbs</span></code>) protocol. Replace <code class="docutils literal notranslate"><span class="pre">[storage</span> <span class="pre">account</span> <span class="pre">name]</span></code> and <code class="docutils literal notranslate"><span class="pre">[key]</span></code> with the appropriate values.</p>
<div class="highlight-xml notranslate"><div class="highlight"><pre><span></span><span class="nt">&lt;property&gt;</span>
    <span class="nt">&lt;name&gt;</span>fs.AbstractFileSystem.wasbs.impl<span class="nt">&lt;/name&gt;</span>
    <span class="nt">&lt;value&gt;</span>org.apache.hadoop.fs.azure.Wasbs<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>

<span class="nt">&lt;property&gt;</span>
    <span class="nt">&lt;name&gt;</span>fs.wasbs.impl<span class="nt">&lt;/name&gt;</span>
    <span class="nt">&lt;value&gt;</span>org.apache.hadoop.fs.azure.NativeAzureFileSystem<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>

<span class="nt">&lt;property&gt;</span>
    <span class="nt">&lt;name&gt;</span>fs.azure.account.key.[storage account name].blob.core.windows.net<span class="nt">&lt;/name&gt;</span>
    <span class="nt">&lt;value&gt;</span>[key]<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>
</pre></div>
</div>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">Your <code class="docutils literal notranslate"><span class="pre">core-site.xml</span></code> file now contains sensitive data and should be protected accordingly.</p>
</div>
</div>
<div class="section" id="create-an-azure-file-share">
<h2>Create an Azure File Share<a class="headerlink" href="#create-an-azure-file-share" title="Permalink to this headline">¶</a></h2>
<p>Using the <a class="reference external" href="https://portal.azure.com">Azure Portal</a>, create a File Share inside your Storage Account and record the name
and key. This file share will be mounted by your cluster master and used to store persistent files e.g. your Jupyter
notebook.</p>
</div>
<div class="section" id="create-a-apache-spark-cluster-for-ingest">
<h2>Create a Apache Spark Cluster for Ingest<a class="headerlink" href="#create-a-apache-spark-cluster-for-ingest" title="Permalink to this headline">¶</a></h2>
<p>We will first create a minimal Apache Spark cluster and use the master to download and ingest some data:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>aztk spark cluster create --id geomesa --vm-size standard_f2 --size-low-priority <span class="m">2</span> --docker-run-options<span class="o">=</span><span class="s2">&quot;--privileged&quot;</span>
</pre></div>
</div>
<p>This should start the creation of a cluster using low priority (i.e. cheaper) nodes. The cluster is deployed as a Docker
container on each node; <code class="docutils literal notranslate"><span class="pre">--privileged</span></code> is required in order to be able to mount the Azure Files share you have just
created.</p>
<p>If you aren’t using ssh keys, you will be prompted to enter a password for the <code class="docutils literal notranslate"><span class="pre">spark</span></code> user. You can monitor cluster
creation progress using <code class="docutils literal notranslate"><span class="pre">aztk</span> <span class="pre">spark</span> <span class="pre">cluster</span> <span class="pre">list</span></code> &amp; <code class="docutils literal notranslate"><span class="pre">aztk</span> <span class="pre">spark</span> <span class="pre">cluster</span> <span class="pre">get</span> <span class="pre">--id</span> <span class="pre">geomesa</span></code>. You can also monitor
cluster creation and status using <a class="reference external" href="https://azure.github.io/BatchExplorer/">Batch Explorer</a>.
The cluster is ready when all nodes are shown in the idle state, which usually takes 5-10 minutes:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>aztk spark cluster get --id geomesa

Cluster         geomesa
------------------------------------------
State:          steady
Node Size:      standard_f2
Created:        <span class="m">2019</span>-08-30 <span class="m">15</span>:07:36
Nodes:          <span class="m">2</span>
<span class="p">|</span> Dedicated:    <span class="m">0</span>
<span class="p">|</span> Low priority: <span class="m">2</span>

<span class="p">|</span>               Nodes                <span class="p">|</span>        State        <span class="p">|</span>        IP:Port       <span class="p">|</span> Dedicated  <span class="p">|</span>  Master  <span class="p">|</span>
<span class="p">|</span>------------------------------------<span class="p">|</span>---------------------<span class="p">|</span>----------------------<span class="p">|</span>------------<span class="p">|</span>----------<span class="p">|</span>
<span class="p">|</span>tvmps_b2e6b9f170b73fe9f993d3e0f1cd2a40cd49041b54dfbf9774fbc07b2c883b03_p<span class="p">|</span>        idle         <span class="p">|</span>  <span class="m">51</span>.105.13.125:50001 <span class="p">|</span>            <span class="p">|</span>    *     <span class="p">|</span>
<span class="p">|</span>tvmps_cfd27f38197a963a04cb8363d6012067fd1d38ecb4fa86a406f89ed3e8f57154_p<span class="p">|</span>        idle         <span class="p">|</span>  <span class="m">51</span>.105.13.125:50000 <span class="p">|</span>            <span class="p">|</span>          <span class="p">|</span>
</pre></div>
</div>
</div>
<div class="section" id="connect-to-the-cluster">
<h2>Connect to the Cluster<a class="headerlink" href="#connect-to-the-cluster" title="Permalink to this headline">¶</a></h2>
<p>Usually you would use <code class="docutils literal notranslate"><span class="pre">aztk</span> <span class="pre">spark</span> <span class="pre">cluster</span> <span class="pre">ssh</span></code> in order to connect to the cluster, forwarding useful ports for the
various services over ssh. However, we will need to add a port forward for Jupyter, so instead perform the following:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>aztk spark cluster ssh --id geomesa -u spark --no-connect

-------------------------------------------
spark cluster id:              geomesa
open webui:                    http://localhost:8080
open jobui:                    http://localhost:4040
open jobhistoryui:             http://localhost:18080
ssh username:                  spark
connect:                       False
-------------------------------------------

Use the following <span class="nb">command</span> to connect to your spark head node:
      ssh -L <span class="m">8080</span>:localhost:8080 -L <span class="m">4040</span>:localhost:4040 -L <span class="m">18080</span>:localhost:18080 -t spark@51.105.13.125 -p <span class="m">50001</span> <span class="s1">&#39;sudo docker exec -it spark /bin/bash&#39;</span>
</pre></div>
</div>
<p>Use the provided command to connect to your cluster, with the following changes:</p>
<ol class="arabic simple">
<li>Add <code class="docutils literal notranslate"><span class="pre">-L</span> <span class="pre">8888:localhost:8888</span></code> in order to additionally port forward Jupyter</li>
<li>(Windows only, when using <code class="docutils literal notranslate"><span class="pre">cmd.exe</span></code>) remove the single quotes around the <code class="docutils literal notranslate"><span class="pre">sudo</span> <span class="pre">docker...</span></code> command.</li>
</ol>
<p>After entering your private key passphrase or the password you set for the <code class="docutils literal notranslate"><span class="pre">spark</span></code> user, you should get a root shell
inside the Docker container running Apache Spark.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>root@883aa5f49ee64425964d1eb085366173000001:/#
</pre></div>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Unless specified otherwise, all subsequent commands should be run inside this container.</p>
</div>
</div>
<div class="section" id="mount-your-azure-file-share">
<h2>Mount your Azure File Share<a class="headerlink" href="#mount-your-azure-file-share" title="Permalink to this headline">¶</a></h2>
<p>In order to provide persistent file storage within your ephemeral clusters, you will now mount your previously created
file share:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>apt-get update <span class="o">&amp;&amp;</span> apt-get install cifs-utils -y
mkdir /mnt/geomesa
mount -t cifs //&lt;storage account&gt;.file.core.windows.net/&lt;file share&gt; /mnt/geomesa <span class="se">\</span>
  -o <span class="nv">vers</span><span class="o">=</span><span class="m">3</span>.0,username<span class="o">=</span>&lt;storage account&gt;,password<span class="o">=</span>&lt;storage account key&gt;,dir_mode<span class="o">=</span><span class="m">0777</span>,file_mode<span class="o">=</span><span class="m">0777</span>,serverino
</pre></div>
</div>
<p>In the final command, replace <code class="docutils literal notranslate"><span class="pre">&lt;storage</span> <span class="pre">account&gt;</span></code>, <code class="docutils literal notranslate"><span class="pre">&lt;file</span> <span class="pre">share&gt;</span></code> &amp; <code class="docutils literal notranslate"><span class="pre">&lt;storage</span> <span class="pre">account</span> <span class="pre">key&gt;</span></code> with the appropriate
values. You should now be able to test writing a file and see that file in the Azure Portal.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>touch /mnt/geomesa/test      <span class="c1"># then check test is visible in Storage Account...Files...File Share</span>
</pre></div>
</div>
</div>
<div class="section" id="install-configure-geomesa-filesystem-cli">
<h2>Install &amp; Configure GeoMesa Filesystem CLI<a class="headerlink" href="#install-configure-geomesa-filesystem-cli" title="Permalink to this headline">¶</a></h2>
<p>In order to ingest data, we will first need to install and configure the GeoMesa Filesystem CLI tool. Replace
<code class="docutils literal notranslate"><span class="pre">${VERSION}</span></code> with the GeoMesa and Scala versions used (e.g. <code class="docutils literal notranslate"><span class="pre">2.12-3.2.0</span></code>):</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span> /mnt/geomesa
wget https://github.com/locationtech/geomesa/releases/download/geomesa_<span class="si">${</span><span class="nv">VERSION</span><span class="si">}</span>/geomesa-fs_<span class="si">${</span><span class="nv">VERSION</span><span class="si">}</span>-bin.tar.gz
tar -xzvf geomesa-fs_<span class="si">${</span><span class="nv">VERSION</span><span class="si">}</span>-bin.tar.gz
</pre></div>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">You may need to update the GeoMesa version in order to match the latest release.</p>
</div>
<p>In order to use GeoMesa Filesystem on Azure Blob Storage, you will need to copy the following JARs and also set the
Hadoop configuration directory environment variable so your <code class="docutils literal notranslate"><span class="pre">core-site.xml</span></code> file is picked up.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span> /home/spark-current/jars
cp azure-storage-2.2.0.jar <span class="se">\</span>
   commons-configuration-1.6.jar <span class="se">\</span>
   commons-logging-1.1.3.jar <span class="se">\</span>
   guava-11.0.2.jar <span class="se">\</span>
   hadoop-auth-2.8.3.jar <span class="se">\</span>
   hadoop-azure-2.8.3.jar <span class="se">\</span>
   hadoop-common-2.8.3.jar <span class="se">\</span>
   hadoop-hdfs-client-2.8.3.jar <span class="se">\</span>
   htrace-core4-4.0.1-incubating.jar <span class="se">\</span>
   jetty-util-6.1.26.jar <span class="se">\</span>
   /mnt/geomesa/geomesa-fs_<span class="si">${</span><span class="nv">VERSION</span><span class="si">}</span>/lib
<span class="nb">export</span> <span class="nv">HADOOP_CONF_DIR</span><span class="o">=</span>/home/spark-current/conf
</pre></div>
</div>
</div>
<div class="section" id="ingest-data-into-azure-blob-storage">
<h2>Ingest Data into Azure Blob Storage<a class="headerlink" href="#ingest-data-into-azure-blob-storage" title="Permalink to this headline">¶</a></h2>
<p>We will first download 2.6 GB of compressed data from <a class="reference external" href="https://marinecadastre.gov/ais/">Marine Cadastre</a>. This file
contains approx 70 million records of ships beaconing their position using
<a class="reference external" href="https://en.wikipedia.org/wiki/Automatic_identification_system">AIS</a> in the Gulf of Mexico in July 2017. Much more data
is available from Marine Cadastre, as well as numerous commercial suppliers.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span> /mnt/geomesa
mkdir data
<span class="nb">cd</span> data
wget https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2017/AIS_2017_07_Zone15.zip
</pre></div>
</div>
<p><em>Optional</em>: We can test the converter as follows.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span> /mnt/geomesa/geomesa-fs_<span class="si">${</span><span class="nv">VERSION</span><span class="si">}</span>/bin
./geomesa-fs convert <span class="se">\</span>
  --spec marinecadastre-ais-csv <span class="se">\</span>
  --converter marinecadastre-ais-csv <span class="se">\</span>
  --max-features <span class="m">10</span> <span class="se">\</span>
  ../../data/AIS_2017_07_Zone15.zip
</pre></div>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">When writing your own converters, it is highly recommended to use the <code class="docutils literal notranslate"><span class="pre">convert</span></code> command for iterative testing prior
to ingest.</p>
</div>
<p>Next, we can ingest the data as follows:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>./geomesa-fs ingest <span class="se">\</span>
  --path wasbs://&lt;blob container name&gt;@&lt;storage account&gt;.blob.core.windows.net/&lt;path&gt; <span class="se">\</span>
  --encoding orc <span class="se">\</span>
  --partition-scheme daily,z2-20bits <span class="se">\</span>
  --spec marinecadastre-ais-csv <span class="se">\</span>
  --converter marinecadastre-ais-csv <span class="se">\</span>
  ../../data/AIS_2017_07_Zone15.zip
</pre></div>
</div>
<p>You should replace <code class="docutils literal notranslate"><span class="pre">&lt;blob</span> <span class="pre">container</span> <span class="pre">name&gt;</span></code>, <code class="docutils literal notranslate"><span class="pre">&lt;storage</span> <span class="pre">account&gt;</span></code> and <code class="docutils literal notranslate"><span class="pre">&lt;path&gt;</span></code> with the appropriate values for your
environment.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Since our data is very concentrated in a particular area, we use a large number of bits for the <code class="docutils literal notranslate"><span class="pre">z2</span></code> index.
In a more realistic situation, index precision is a tradeoff between reading large blocks of data from storage
(favouring lower precision) and minimising the number of discrete files or blobs accesses (favouring higher
precision). This will depend on your data distribution and access/query patterns.</p>
</div>
</div>
<div class="section" id="install-jupyter-geomesa-jupyter-leaflet-apache-toree">
<h2>Install Jupyter, GeoMesa Jupyter Leaflet &amp; Apache Toree<a class="headerlink" href="#install-jupyter-geomesa-jupyter-leaflet-apache-toree" title="Permalink to this headline">¶</a></h2>
<p>Having created our Apache Spark cluster &amp; ingested some data, we are almost ready to run some analytics. We will use the
Jupyter notebook platform together with the Apache Toree kernel for Apache Spark to perform interactive scalable
analysis. In order to visualise our results, we will use the GeoMesa Jupyter Leaflet integration.</p>
<p><em>Optional</em>: Having used a minimal cluster for ingest, you may now wish to use more nodes to increase performance and the
size of datasets that can be analysed. If so, delete your existing cluster (<code class="docutils literal notranslate"><span class="pre">aztk</span> <span class="pre">spark</span> <span class="pre">cluster</span> <span class="pre">delete</span> <span class="pre">--id=geomesa</span></code>)
and create a new one as previously, increasing the number of nodes (<code class="docutils literal notranslate"><span class="pre">--size</span></code> and/or <code class="docutils literal notranslate"><span class="pre">--size-low-priority</span></code>) and/or
individual node size (<code class="docutils literal notranslate"><span class="pre">--vm-size</span></code>). Remember to remount the Azure Files share and export <code class="docutils literal notranslate"><span class="pre">HADOOP_CONF_DIR</span></code>.</p>
<p>Back inside the Apache Spark container on your master node run the following:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span> /mnt/geomesa
pip install toree
wget https://repo1.maven.org/maven2/org/locationtech/geomesa/geomesa-jupyter-leaflet_2.12/<span class="si">${</span><span class="nv">VERSION</span><span class="si">}</span>/geomesa-jupyter-leaflet_<span class="si">${</span><span class="nv">VERSION</span><span class="si">}</span>.jar
jupyter toree install <span class="se">\</span>
  --spark_home<span class="o">=</span>/home/spark-current <span class="se">\</span>
  --replace <span class="se">\</span>
  --spark_opts<span class="o">=</span><span class="s2">&quot;--master spark://`hostname -i`:7077 --num-executors 2 --conf spark.dynamicAllocation.enabled=false --jars /mnt/geomesa/geomesa-fs_</span><span class="si">${</span><span class="nv">VERSION</span><span class="si">}</span><span class="s2">/dist/spark/geomesa-fs-spark-runtime_</span><span class="si">${</span><span class="nv">VERSION</span><span class="si">}</span><span class="s2">.jar,/mnt/geomesa/geomesa-jupyter-leaflet_</span><span class="si">${</span><span class="nv">VERSION</span><span class="si">}</span><span class="s2">.jar&quot;</span>
</pre></div>
</div>
<p>If you have increased the size of your cluster, you should also increase <code class="docutils literal notranslate"><span class="pre">--num-executors</span></code> accordingly. You can also
set other executor and driver options by editing the <code class="docutils literal notranslate"><span class="pre">spark_opts</span></code> contents.</p>
</div>
<div class="section" id="running-jupyter-and-opening-notebooks">
<h2>Running Jupyter and opening Notebooks<a class="headerlink" href="#running-jupyter-and-opening-notebooks" title="Permalink to this headline">¶</a></h2>
<p>Finally, we will clone the tutorial repository in order to obtain our sample notebook, then launch Jupyter:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>git clone https://github.com/geomesa/geomesa-tutorials
jupyter notebook --allow-root <span class="p">&amp;</span>
</pre></div>
</div>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">You may need to check out the appropriate tag of the <code class="docutils literal notranslate"><span class="pre">geomesa-tutorials</span></code> repository in order to match your GeoMesa
Filesystem release.</p>
</div>
<p>Then open the URL provided by Jupyter on your local machine, including the long token. Navigate to
<code class="docutils literal notranslate"><span class="pre">geomesa-fs-on-azure</span></code> and open <code class="docutils literal notranslate"><span class="pre">GeoMesa</span> <span class="pre">FileSystem</span> <span class="pre">on</span> <span class="pre">Azure.ipynb</span></code>. Work through the notebook at your own pace.</p>
<div class="figure" id="id2">
<img alt="Jupyter notebook showing GeoMesa Leaflet integration" src="../_images/jupyter.png" />
<p class="caption"><span class="caption-text">Jupyter notebook showing GeoMesa Leaflet integration</span></p>
</div>
<p>You can access the Apache Spark Master interface via <a class="reference external" href="http://localhost:8080">http://localhost:8080</a>, and the Apache
Spark Jobs interface via <a class="reference external" href="http://localhost:4040">http://localhost:4040</a>.</p>
<div class="figure" id="id3">
<img alt="Apache Spark Master UI" src="../_images/spark-master-ui.png" />
<p class="caption"><span class="caption-text">Apache Spark Master UI</span></p>
</div>
<div class="figure" id="id4">
<img alt="Apache Spark Jobs UI" src="../_images/spark-job-ui.png" />
<p class="caption"><span class="caption-text">Apache Spark Jobs UI</span></p>
</div>
</div>
<div class="section" id="deleting-your-ephemeral-cluster">
<h2>Deleting your Ephemeral Cluster<a class="headerlink" href="#deleting-your-ephemeral-cluster" title="Permalink to this headline">¶</a></h2>
<p>It is important to remember to delete your Azure Batch cluster once you have finished with it, otherwise you <strong>will</strong>
incur unexpected charges.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>aztk spark cluster delete --id<span class="o">=</span>geomesa
</pre></div>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="geomesa-examples-gdelt.html" class="btn btn-neutral float-right" title="Map-Reduce Ingest of GDELT" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="geomesa-hbase-on-cdh.html" class="btn btn-neutral" title="Deploying GeoMesa HBase on Cloudera CDH 5.X" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>

    </p>
  </div>

<div role="contentinfo">
  <p>
    &copy; Copyright 2013-2021 <a href="https://www.ccri.com/">Commonwealth Computer Research, Inc.</a>
    <br/>
    Licensed under the <a href="http://www.opensource.org/licenses/apache2.0.php">Apache License, Version 2.0</a>
  </p>
</div>

<div role="contentinfo">
  <p>
    Built with <a href="http://sphinx-doc.org/">Sphinx</a>
    using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>
    
    - view <a href="../_sources/tutorials/geomesa-fs-on-azure.rst.txt" rel="nofollow">page source</a>
    
  </p>
</div>



</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../_static/jquery.js"></script>
        <script type="text/javascript" src="../_static/underscore.js"></script>
        <script type="text/javascript" src="../_static/doctools.js"></script>
    

  

  <script type="text/javascript" src="../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>