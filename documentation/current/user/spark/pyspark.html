

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>11.7. GeoMesa PySpark &mdash; GeoMesa 3.5.0 Manuals</title>
  

  
  
  
  
    <link rel="canonical" href="https://www.geomesa.org/documentation/user/spark/pyspark.html"/>
  

  

  
  
    

  

  <link rel="stylesheet" href="../../_static/css/theme_custom.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:400,700" type="text/css" />
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="11.8. Deploying GeoMesa Spark with Jupyter Notebook" href="jupyter.html" />
    <link rel="prev" title="11.6. SparkSQL Functions" href="sparksql_functions.html" />

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-53087457-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-53087457-1');
</script>



  
  <script src="../../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../../index.html" class="icon icon-home"> GeoMesa
          

          
          </a>

          
            
            
              <div class="version">
                4.0.0-SNAPSHOT
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">User Manual</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../introduction.html">1. Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../download.html">2. Versions and Downloads</a></li>
<li class="toctree-l2"><a class="reference internal" href="../install.html">3. Installation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../getting_started.html">4. Getting Started</a></li>
<li class="toctree-l2"><a class="reference internal" href="../geotools.html">5. GeoTools Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="../architecture.html">6. Architecture Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="../datastores/index.html">7. GeoMesa Data Stores</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cli/index.html">8. Command-Line Tools</a></li>
<li class="toctree-l2"><a class="reference internal" href="../convert/index.html">9. GeoMesa Convert</a></li>
<li class="toctree-l2"><a class="reference internal" href="../geoserver.html">10. GeoServer Plugins</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="index.html">11. GeoMesa Spark</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="architecture.html">11.1. Architecture</a></li>
<li class="toctree-l3"><a class="reference internal" href="spark_jts.html">11.2. Spark JTS</a></li>
<li class="toctree-l3"><a class="reference internal" href="core.html">11.3. Spark Core</a></li>
<li class="toctree-l3"><a class="reference internal" href="providers.html">11.4. Spatial RDD Providers</a></li>
<li class="toctree-l3"><a class="reference internal" href="sparksql.html">11.5. SparkSQL</a></li>
<li class="toctree-l3"><a class="reference internal" href="sparksql_functions.html">11.6. SparkSQL Functions</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">11.7. GeoMesa PySpark</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#prerequisites">11.7.1. Prerequisites</a></li>
<li class="toctree-l4"><a class="reference internal" href="#installation">11.7.2. Installation</a></li>
<li class="toctree-l4"><a class="reference internal" href="#using-geomesa-pyspark">11.7.3. Using GeoMesa PySpark</a></li>
<li class="toctree-l4"><a class="reference internal" href="#using-geomesa-udfs-in-pyspark">11.7.4. Using Geomesa UDFs in PySpark</a></li>
<li class="toctree-l4"><a class="reference internal" href="#using-custom-scala-udfs-from-pyspark">11.7.5. Using Custom Scala UDFs from PySpark</a></li>
<li class="toctree-l4"><a class="reference internal" href="#jupyter">11.7.6. Jupyter</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="jupyter.html">11.8. Deploying GeoMesa Spark with Jupyter Notebook</a></li>
<li class="toctree-l3"><a class="reference internal" href="zeppelin.html">11.9. Deploying GeoMesa Spark with Zeppelin</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../nifi/index.html">12. GeoMesa NiFi Bundle</a></li>
<li class="toctree-l2"><a class="reference internal" href="../process.html">13. GeoMesa Processes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../hbase/index.html">14. HBase Data Store</a></li>
<li class="toctree-l2"><a class="reference internal" href="../accumulo/index.html">15. Accumulo Data Store</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cassandra/index.html">16. Cassandra Data Store</a></li>
<li class="toctree-l2"><a class="reference internal" href="../bigtable/index.html">17. Bigtable Data Store</a></li>
<li class="toctree-l2"><a class="reference internal" href="../kafka/index.html">18. Kafka Data Store</a></li>
<li class="toctree-l2"><a class="reference internal" href="../redis/index.html">19. Redis Data Store</a></li>
<li class="toctree-l2"><a class="reference internal" href="../filesystem/index.html">20. FileSystem Data Store</a></li>
<li class="toctree-l2"><a class="reference internal" href="../kudu/index.html">21. Kudu Data Store</a></li>
<li class="toctree-l2"><a class="reference internal" href="../postgis/index.html">22. Partitioned PostGIS Data Store</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lambda/index.html">23. Lambda Data Store</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ds_views.html">24. Combined Data Store Views</a></li>
<li class="toctree-l2"><a class="reference internal" href="../geojson.html">25. GeoMesa GeoJSON</a></li>
<li class="toctree-l2"><a class="reference internal" href="../stream.html">26. GeoMesa Stream Processing</a></li>
<li class="toctree-l2"><a class="reference internal" href="../upgrade.html">27. Upgrade Guide</a></li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#appendix">Appendix</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../developer/index.html">Developer Manual</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tutorials/index.html">Tutorials</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">GeoMesa</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../index.html">Docs</a> &raquo;</li>
        
          <li><a href="../index.html">User Manual</a> &raquo;</li>
        
          <li><a href="index.html">11. GeoMesa Spark</a> &raquo;</li>
        
      <li>11.7. GeoMesa PySpark</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="geomesa-pyspark">
<h1>11.7. GeoMesa PySpark<a class="headerlink" href="#geomesa-pyspark" title="Permalink to this headline">¶</a></h1>
<p>GeoMesa provides integration with the Spark Python API for accessing data in GeoMesa data stores.</p>
<div class="section" id="prerequisites">
<h2>11.7.1. Prerequisites<a class="headerlink" href="#prerequisites" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><a class="reference external" href="http://spark.apache.org/">Spark</a> 2.4.x, 3.0.x or 3.1.x should be installed.</li>
<li><a class="reference external" href="https://www.python.org/">Python</a> 2.7 or 3.x should be installed.</li>
<li><a class="reference external" href="https://packaging.python.org/tutorials/installing-packages/">pip</a> or <code class="docutils literal notranslate"><span class="pre">pip3</span></code> should be installed.</li>
<li><a class="reference external" href="https://conda.github.io/conda-pack/">conda-pack</a> is optional.</li>
</ul>
</div>
<div class="section" id="installation">
<h2>11.7.2. Installation<a class="headerlink" href="#installation" title="Permalink to this headline">¶</a></h2>
<p>The <code class="docutils literal notranslate"><span class="pre">geomesa_pyspark</span></code> package is not available for download. Build the artifact locally with the profile
<code class="docutils literal notranslate"><span class="pre">-Ppython</span></code>. Then install using <code class="docutils literal notranslate"><span class="pre">pip</span></code> or <code class="docutils literal notranslate"><span class="pre">pip3</span></code> as below. You will also need an appropriate
<code class="docutils literal notranslate"><span class="pre">geomesa-spark-runtime</span></code> JAR. We assume the use of Accumulo here, but you may alternatively use any of
the providers outlined in <a class="reference internal" href="providers.html#spatial-rdd-providers"><span class="std std-ref">Spatial RDD Providers</span></a>.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>mvn<span class="w"> </span>clean<span class="w"> </span>install<span class="w"> </span>-Ppython
pip3<span class="w"> </span>install<span class="w"> </span>geomesa-spark/geomesa_pyspark/target/geomesa_pyspark-<span class="nv">$VERSION</span>.tar.gz
cp<span class="w">  </span>geomesa-accumulo/geomesa-accumulo-spark-runtime-accumulo2/target/geomesa-accumulo-spark-runtime-accumulo2_<span class="si">${</span><span class="nv">VERSION</span><span class="si">}</span>.jar<span class="w"> </span>/path/to/
</pre></div>
</div>
<p>Alternatively, you can use <code class="docutils literal notranslate"><span class="pre">conda-pack</span></code> to bundle the dependencies for your project. This may be more appropriate if
you have additional dependencies.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">ENV_NAME</span><span class="o">=</span>geomesa-pyspark

conda<span class="w"> </span>create<span class="w"> </span>--name<span class="w"> </span><span class="nv">$ENV_NAME</span><span class="w"> </span>-y<span class="w"> </span><span class="nv">python</span><span class="o">=</span><span class="m">3</span>.7
conda<span class="w"> </span>activate<span class="w"> </span><span class="nv">$ENV_NAME</span>

pip<span class="w"> </span>install<span class="w"> </span>geomesa-spark/geomesa_pyspark/target/geomesa_pyspark-<span class="nv">$VERSION</span>.tar.gz
<span class="c1"># Install additional dependencies using conda or pip here</span>

conda<span class="w"> </span>pack<span class="w"> </span>-o<span class="w"> </span>environment.tar.gz
cp<span class="w"> </span>geomesa-accumulo/geomesa-accumulo-spark-runtime-accumulo2/target/geomesa-accumulo-spark-runtime-accumulo2_<span class="si">${</span><span class="nv">VERSION</span><span class="si">}</span>.jar<span class="w"> </span>/path/to/
</pre></div>
</div>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last"><code class="docutils literal notranslate"><span class="pre">conda-pack</span></code> currently has issues with Python 3.8, and <code class="docutils literal notranslate"><span class="pre">pyspark</span></code> has issues with Python 3.9, hence the explicit
use of Python 3.7</p>
</div>
</div>
<div class="section" id="using-geomesa-pyspark">
<h2>11.7.3. Using GeoMesa PySpark<a class="headerlink" href="#using-geomesa-pyspark" title="Permalink to this headline">¶</a></h2>
<p>You may then access Spark using a Yarn master by default. Importantly, because of the way the <code class="docutils literal notranslate"><span class="pre">geomesa_pyspark</span></code>
library interacts with the underlying Java libraries, you must set up the GeoMesa configuration before referencing
the <code class="docutils literal notranslate"><span class="pre">pyspark</span></code> library.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">geomesa_pyspark</span>
<span class="n">conf</span> <span class="o">=</span> <span class="n">geomesa_pyspark</span><span class="o">.</span><span class="n">configure</span><span class="p">(</span>
    <span class="n">jars</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;/path/to/geomesa-accumulo-spark-runtime-accumulo2_$</span><span class="si">{VERSION}</span><span class="s1">.jar&#39;</span><span class="p">],</span>
    <span class="n">packages</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;geomesa_pyspark&#39;</span><span class="p">,</span><span class="s1">&#39;pytz&#39;</span><span class="p">],</span>
    <span class="n">spark_home</span><span class="o">=</span><span class="s1">&#39;/path/to/spark/&#39;</span><span class="p">)</span><span class="o">.</span>\
    <span class="n">setAppName</span><span class="p">(</span><span class="s1">&#39;MyTestApp&#39;</span><span class="p">)</span>

<span class="n">conf</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;spark.master&#39;</span><span class="p">)</span>
<span class="c1"># u&#39;yarn&#39;</span>

<span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">SparkSession</span>

<span class="n">spark</span> <span class="o">=</span> <span class="p">(</span> <span class="n">SparkSession</span>
    <span class="o">.</span><span class="n">builder</span>
    <span class="o">.</span><span class="n">config</span><span class="p">(</span><span class="n">conf</span><span class="o">=</span><span class="n">conf</span><span class="p">)</span>
    <span class="o">.</span><span class="n">enableHiveSupport</span><span class="p">()</span>
    <span class="o">.</span><span class="n">getOrCreate</span><span class="p">()</span>
<span class="p">)</span>
</pre></div>
</div>
<p>Alternatively, if you used <code class="docutils literal notranslate"><span class="pre">conda-pack</span></code> then you do not need to set up the GeoMesa configuration as above, but you
must start <code class="docutils literal notranslate"><span class="pre">pyspark</span></code> or your application as follows, updating paths as required:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nv">PYSPARK_DRIVER_PYTHON</span><span class="o">=</span>/opt/anaconda3/envs/<span class="nv">$ENV_NAME</span>/bin/python<span class="w"> </span><span class="nv">PYSPARK_PYTHON</span><span class="o">=</span>./environment/bin/python<span class="w"> </span>pyspark<span class="w"> </span><span class="se">\</span>
--jars<span class="w"> </span>/path/to/geomesa-accumulo-spark-runtime_<span class="si">${</span><span class="nv">VERSION</span><span class="si">}</span>.jar<span class="w"> </span><span class="se">\</span>
--conf<span class="w"> </span>spark.yarn.appMasterEnv.PYSPARK_PYTHON<span class="o">=</span>./environment/bin/python<span class="w"> </span><span class="se">\</span>
--master<span class="w"> </span>yarn<span class="w"> </span>--deploy-mode<span class="w"> </span>client<span class="w"> </span>--archives<span class="w"> </span>environment.tar.gz#environment
</pre></div>
</div>
<p>At this point you are ready to create a dict of connection parameters to your Accumulo data store and get a spatial
data frame.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">params</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;accumulo.instance.id&quot;</span><span class="p">:</span> <span class="s2">&quot;myInstance&quot;</span><span class="p">,</span>
    <span class="s2">&quot;accumulo.zookeepers&quot;</span><span class="p">:</span> <span class="s2">&quot;zoo1,zoo2,zoo3&quot;</span><span class="p">,</span>
    <span class="s2">&quot;accumulo.user&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span>
    <span class="s2">&quot;accumulo.password&quot;</span><span class="p">:</span> <span class="s2">&quot;password&quot;</span><span class="p">,</span>
    <span class="s2">&quot;accumulo.catalog&quot;</span><span class="p">:</span> <span class="s2">&quot;myCatalog&quot;</span>
<span class="p">}</span>
<span class="n">feature</span> <span class="o">=</span> <span class="s2">&quot;mySchema&quot;</span>
<span class="n">df</span> <span class="o">=</span> <span class="p">(</span> <span class="n">spark</span>
    <span class="o">.</span><span class="n">read</span>
    <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s2">&quot;geomesa&quot;</span><span class="p">)</span>
    <span class="o">.</span><span class="n">options</span><span class="p">(</span><span class="o">**</span><span class="n">params</span><span class="p">)</span>
    <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&quot;geomesa.feature&quot;</span><span class="p">,</span> <span class="n">feature</span><span class="p">)</span>
    <span class="o">.</span><span class="n">load</span><span class="p">()</span>
<span class="p">)</span>

<span class="n">df</span><span class="o">.</span><span class="n">createOrReplaceTempView</span><span class="p">(</span><span class="s2">&quot;tbl&quot;</span><span class="p">)</span>
<span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s2">&quot;show tables&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># Count features in a bounding box.</span>
<span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">select count(*)</span>
<span class="s2">from tbl</span>
<span class="s2">where st_contains(st_makeBBOX(-72.0, 40.0, -71.0, 41.0), geom)</span>
<span class="s2">&quot;&quot;&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<p>GeoMesa PySpark can also be used in the absence of a GeoMesa data store.  Registering user-defined types and functions
can be done manually by invoking <code class="docutils literal notranslate"><span class="pre">geomesa_pyspark.init_sql()</span></code> on the Spark session object:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">geomesa_pyspark</span><span class="o">.</span><span class="n">init_sql</span><span class="p">(</span><span class="n">spark</span><span class="p">)</span>
</pre></div>
</div>
<p>You can terminate the Spark job on YARN using <code class="docutils literal notranslate"><span class="pre">spark.stop()</span></code>.</p>
</div>
<div class="section" id="using-geomesa-udfs-in-pyspark">
<h2>11.7.4. Using Geomesa UDFs in PySpark<a class="headerlink" href="#using-geomesa-udfs-in-pyspark" title="Permalink to this headline">¶</a></h2>
<p>There are 3 different ways to use the Geomesa UDFs from PySpark: from the SQL API, from the Fluent API via SQL expressions, or from the Fluent API via Python wrappers.
These approaches are equivalent performance-wise, so choosing the best approach for your project comes down to preference.</p>
<div class="section" id="accessing-the-geomesa-udfs-from-the-sql-api">
<h3>11.7.4.1. 1. Accessing the Geomesa UDFs from the SQL API<a class="headerlink" href="#accessing-the-geomesa-udfs-from-the-sql-api" title="Permalink to this headline">¶</a></h3>
<p>We can access the Geomesa UDFs via the SQL API by simply including the functions in our SQL expressions.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">df</span><span class="o">.</span><span class="n">createOrReplaceTempView</span><span class="p">(</span><span class="s2">&quot;tbl&quot;</span><span class="p">)</span>

<span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">select count(*) from tbl</span>
<span class="s2">where st_contains(st_makeBBOX(-72.0, 40.0, -71.0, 41.0), geom)</span>
<span class="s2">&quot;&quot;&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="section" id="accessing-the-geomesa-udfs-from-the-fluent-api-via-sql-expressions">
<h3>11.7.4.2. 2. Accessing the Geomesa UDFs from the Fluent API via SQL Expressions<a class="headerlink" href="#accessing-the-geomesa-udfs-from-the-fluent-api-via-sql-expressions" title="Permalink to this headline">¶</a></h3>
<p>We can also access the Geomesa UDFs from the Fluent API via the <cite>pyspark.sql.functions</cite> module. This module has an <cite>expr</cite> function that we can use to access the Geomesa UDFs.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pyspark.sql.functions</span> <span class="k">as</span> <span class="nn">F</span>

<span class="c1"># add a new column</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s2">&quot;geom_wkt&quot;</span><span class="p">,</span> <span class="n">F</span><span class="o">.</span><span class="n">expr</span><span class="p">(</span><span class="s2">&quot;st_asText(geom)&quot;</span><span class="p">))</span>

<span class="c1"># filter using SQL where expression</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="s2">&quot;*&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="s2">&quot;st_area(geom) &gt; 0.001&quot;</span><span class="p">)</span>

<span class="n">df</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="section" id="accessing-the-geomesa-udfs-from-the-fluent-api-via-python-wrappers">
<h3>11.7.4.3. 3. Accessing the Geomesa UDFs from the Fluent API via Python Wrappers<a class="headerlink" href="#accessing-the-geomesa-udfs-from-the-fluent-api-via-python-wrappers" title="Permalink to this headline">¶</a></h3>
<p>We also support using the Geomesa UDFs as standalone functions through the use of Python wrappers. The Python wrappers for the Geomesa UDFs run on the JVM and are faster than logically equivalent Python UDFs.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">geomesa_pyspark.scala.functions</span> <span class="kn">import</span> <span class="n">st_asText</span><span class="p">,</span> <span class="n">st_area</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s2">&quot;geom_wkt&quot;</span><span class="p">,</span> <span class="n">st_asText</span><span class="p">(</span><span class="s2">&quot;geom&quot;</span><span class="p">))</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s2">&quot;geom_area&quot;</span><span class="p">,</span> <span class="n">st_area</span><span class="p">(</span><span class="s2">&quot;geom&quot;</span><span class="p">))</span>

<span class="n">df</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="using-custom-scala-udfs-from-pyspark">
<h2>11.7.5. Using Custom Scala UDFs from PySpark<a class="headerlink" href="#using-custom-scala-udfs-from-pyspark" title="Permalink to this headline">¶</a></h2>
<p>We provide some utility functions in <cite>geomesa_pyspark</cite> that allow you to use your own Scala UDFs as standalone functions from PySpark. The advantage here is that you can write your UDFs in java or scala (so they run on the JVM), but can be used naturally from PySpark as if it were part of the Fluent API. This gives us the ability to write and use performant UDFs from PySpark without having to rely on Python UDFs, which can often be prohibitively slow for larger datasets.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">functools</span> <span class="kn">import</span> <span class="n">partial</span>
<span class="kn">from</span> <span class="nn">geomesa_pyspark.scala.udf</span> <span class="kn">import</span> <span class="n">build_scala_udf</span><span class="p">,</span> <span class="n">scala_udf</span><span class="p">,</span> <span class="n">ColumnOrName</span>
<span class="kn">from</span> <span class="nn">pyspark</span> <span class="kn">import</span> <span class="n">SparkContext</span>
<span class="kn">from</span> <span class="nn">pyspark.sql.column</span> <span class="kn">import</span> <span class="n">Column</span>

<span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">getOrCreate</span><span class="p">()</span>
<span class="n">custom_udfs</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">to</span><span class="o">.</span><span class="n">your</span><span class="o">.</span><span class="n">CustomUserDefinedFunctions</span>

<span class="c1"># use the helper function for building your udf</span>
<span class="k">def</span> <span class="nf">my_scala_udf</span><span class="p">(</span><span class="n">col</span><span class="p">:</span> <span class="n">ColumnOrName</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Column</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;helpful docstring that explains what col is&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">build_scala_udf</span><span class="p">(</span><span class="n">sc</span><span class="p">,</span> <span class="n">custom_udfs</span><span class="o">.</span><span class="n">my_scala_udf</span><span class="p">)(</span><span class="n">col</span><span class="p">)</span>

<span class="c1"># or alternatively, build it directly by partially applying the scala udf</span>
<span class="n">my_other_udf</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">scala_udf</span><span class="p">,</span> <span class="n">sc</span><span class="p">,</span> <span class="n">custom_udfs</span><span class="o">.</span><span class="n">my_other_udf</span><span class="p">())</span>

<span class="n">df</span><span class="o">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s2">&quot;edited_field_1&quot;</span><span class="p">,</span> <span class="n">my_scala_udf</span><span class="p">(</span><span class="s2">&quot;field_1&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="n">df</span><span class="o">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s2">&quot;edited_field_2&quot;</span><span class="p">,</span> <span class="n">my_other_udf</span><span class="p">(</span><span class="s2">&quot;field_2&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<p>Recall that these UDFs can actually take either a <cite>pyspark.sql.column.Column</cite> or the string name of the column we wish to operate on, so the following are equivalent:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># this is more readable</span>
<span class="n">df</span><span class="o">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s2">&quot;edited_field_1&quot;</span><span class="p">,</span> <span class="n">my_scala_udf</span><span class="p">(</span><span class="s2">&quot;field_1&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># but we can also do this</span>
<span class="n">df</span><span class="o">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s2">&quot;edited_field_1&quot;</span><span class="p">,</span> <span class="n">my_scala_udf</span><span class="p">(</span><span class="n">col</span><span class="p">(</span><span class="s2">&quot;field_1&quot;</span><span class="p">)))</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="section" id="jupyter">
<h2>11.7.6. Jupyter<a class="headerlink" href="#jupyter" title="Permalink to this headline">¶</a></h2>
<p>To use the <code class="docutils literal notranslate"><span class="pre">geomesa_pyspark</span></code> package within Jupyter, you only needs a Python2 or Python3 kernel, which is
provided by default. Substitute the appropriate Spark home and runtime JAR paths in the above code blocks. Be sure
the GeoMesa Accumulo client and server side versions match, as described in <a class="reference internal" href="../accumulo/install.html"><span class="doc">Installing GeoMesa Accumulo</span></a>.</p>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="jupyter.html" class="btn btn-neutral float-right" title="11.8. Deploying GeoMesa Spark with Jupyter Notebook" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="sparksql_functions.html" class="btn btn-neutral" title="11.6. SparkSQL Functions" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>

    </p>
  </div>

<div role="contentinfo">
  <p>
    &copy; Copyright 2013-2021 <a href="https://www.ccri.com/">Commonwealth Computer Research, Inc.</a>
    <br/>
    Licensed under the <a href="http://www.opensource.org/licenses/apache2.0.php">Apache License, Version 2.0</a>
  </p>
</div>

<div role="contentinfo">
  <p>
    Built with <a href="http://sphinx-doc.org/">Sphinx</a>
    using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>
    
    - view <a href="../../_sources/user/spark/pyspark.rst.txt" rel="nofollow">page source</a>
    
  </p>
</div>



</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../../_static/jquery.js"></script>
        <script type="text/javascript" src="../../_static/underscore.js"></script>
        <script type="text/javascript" src="../../_static/doctools.js"></script>
        <script type="text/javascript" src="../../_static/clipboard.min.js"></script>
        <script type="text/javascript" src="../../_static/copybutton.js"></script>
    

  

  <script type="text/javascript" src="../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>